{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HFooladi/GNNs-For-Chemists/blob/main/notebooks/03_GNN_molecular_activity_predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2uU_2_g8x3A"
   },
   "source": [
    "# Graph Convolutional Networks for Molecular Property Prediction: A Comprehensive Tutorial\n",
    "\n",
    "## Table of Contents\n",
    "0. [Setup and Installation](#setup-and-installation)\n",
    "1. [Introduction](#introduction)\n",
    "2. [Theoretical Background: Graph Convolutional Networks](#theoretical-background-graph-convolutions-network)\n",
    "3. [Implementing a GCN from Scratch](#implementing-gcn-scratch)\n",
    "4. [Visualizing the GCN Layer Operations in Detail](#visualization)\n",
    "5. [Building a Complete GCN for Molecular Property Prediction](#gcn-molecular-property-prediction)\n",
    "6. [ Visualizing Learned Molecular Representations](#visualize-learned-representation)\n",
    "7. [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9JNNnBfLSUJ"
   },
   "source": [
    "## 0. Setup and Installation  <a name=\"setup-and-installation\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EROB5rGu77MJ",
    "outputId": "c8db3a42-d962-4c6d-eaab-ae6a7e35da0c"
   },
   "outputs": [],
   "source": [
    "#@title Intstall necessary libraries\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch_geometric\n",
    "!pip install -q rdkit\n",
    "!pip install -q networkx\n",
    "!pip install -q jupyter-black\n",
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "AH6Vh0mA9mrl"
   },
   "outputs": [],
   "source": [
    "# @title import required libraries\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, global_add_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gi0bsRDh8s_Z"
   },
   "source": [
    "## 1. Introduction <a name=\"introduction\"></a>\n",
    "\n",
    "This notebook provides a comprehensive explanation and implementation of Graph Convolutional Networks (GCNs), with a specific focus on their application to molecular property prediction. We'll start with the theoretical foundations, implement a GCN from scratch, and then use PyTorch Geometric to predict molecular properties such as binding affinity.\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand the theory behind Graph Convolutional Networks (GCNs)\n",
    "- Visualize how GCNs process graph-structured data in the context of molecules\n",
    "- Implement a GCN from scratch using PyTorch\n",
    "- Apply a GCN model to predict molecular properties using PyTorch Geometric\n",
    "- Evaluate and visualize the results of molecular property prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLRoSYd5MfUf"
   },
   "source": [
    "## 2. Theoretical Background: Graph Convolutional Networks <a name=\"theoretical-background-graph-convolutions-network\"> </a>\n",
    "\n",
    "### 2.1 Graph Representation and Notation\n",
    "\n",
    "In the context of molecules, graphs provide a natural representation where:\n",
    "- Atoms are represented as nodes\n",
    "- Chemical bonds are represented as edges\n",
    "\n",
    "Mathematically, we define a graph $G = (V, E)$ where:\n",
    "- $V$ is the set of nodes (atoms)\n",
    "- $E$ is the set of edges (chemical bonds)\n",
    "\n",
    "Each node $i$ has a feature vector $\\mathbf{x}_i$ that could represent properties like:\n",
    "- Atom type (C, N, O, etc.)\n",
    "- Formal charge\n",
    "- Hybridization state\n",
    "- Etc.\n",
    "\n",
    "The graph structure can be represented as an adjacency matrix $\\mathbf{A}$, where:\n",
    "- $\\mathbf{A}_{ij} = 1$ if there is an edge between nodes $i$ and $j$\n",
    "- $\\mathbf{A}_{ij} = 0$ otherwise\n",
    "\n",
    "Think of the adjacency matrix as a \"connectivity map\" showing which atoms are connected to each other. This representation allows us to work with molecular structures in a mathematical framework.\n",
    "\n",
    "\n",
    "Let's visualize these concepts using aspirin as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HYDP0Tjh9-XL",
    "outputId": "f2570fe0-e10b-415b-fd92-7db59a19d592"
   },
   "outputs": [],
   "source": [
    "# Define a simple molecule (aspirin)\n",
    "aspirin_smiles = \"CC(=O)OC1=CC=CC=C1C(=O)O\"\n",
    "mol = Chem.MolFromSmiles(aspirin_smiles)\n",
    "AllChem.Compute2DCoords(mol)\n",
    "# Note: We could add hydrogen explicitly with Chem.AddHs(mol) if needed for the analysis\n",
    "\n",
    "# Visualize the molecule using RDKit\n",
    "mol_img = Draw.MolToImage(mol, size=(400, 300))\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(mol_img)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Aspirin Molecule\")\n",
    "plt.show()\n",
    "\n",
    "# Create a NetworkX graph from the molecule to visualize its graph structure\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes (atoms) to the graph\n",
    "atom_symbols = []\n",
    "for atom in mol.GetAtoms():\n",
    "    G.add_node(atom.GetIdx(), symbol=atom.GetSymbol())\n",
    "    atom_symbols.append(atom.GetSymbol())\n",
    "\n",
    "# Add edges (bonds) to the graph\n",
    "for bond in mol.GetBonds():\n",
    "    G.add_edge(bond.GetBeginAtomIdx(), bond.GetEndAtomIdx())\n",
    "\n",
    "# Create a visualization of the graph structure\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Get the 2D coordinates from RDKit\n",
    "pos = {}\n",
    "for i, atom in enumerate(mol.GetAtoms()):\n",
    "    pos[i] = mol.GetConformer().GetAtomPosition(i)\n",
    "    pos[i] = (pos[i].x, pos[i].y)\n",
    "nx.draw(\n",
    "    G,\n",
    "    pos,\n",
    "    with_labels=True,\n",
    "    labels={i: atom_symbols[i] for i in range(len(atom_symbols))},\n",
    "    node_color=\"lightblue\",\n",
    "    node_size=500,\n",
    "    font_weight=\"bold\",\n",
    ")\n",
    "plt.title(\"Graph Representation of Aspirin\")\n",
    "plt.show()\n",
    "\n",
    "# Generate adjacency matrix - the mathematical representation of the graph\n",
    "A = nx.adjacency_matrix(G).toarray()\n",
    "\n",
    "# Visualize the adjacency matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(A, cmap=\"Blues\")\n",
    "plt.colorbar(label=\"Connection\")\n",
    "plt.title(\"Adjacency Matrix\")\n",
    "plt.xlabel(\"Node Index\")\n",
    "plt.ylabel(\"Node Index\")\n",
    "plt.show()\n",
    "\n",
    "# Example node features (one-hot encoding of atom types)\n",
    "# One-hot encoding means each atom type gets its own dimension in the feature vector\n",
    "unique_atoms = sorted(set(atom_symbols))\n",
    "atom_to_idx = {atom: i for i, atom in enumerate(unique_atoms)}\n",
    "n_atoms = len(atom_symbols)\n",
    "n_features = len(unique_atoms)\n",
    "\n",
    "# Create the feature matrix (each row represents an atom, each column an atom type)\n",
    "X = np.zeros((n_atoms, n_features))\n",
    "for i, atom in enumerate(atom_symbols):\n",
    "    X[i, atom_to_idx[atom]] = 1\n",
    "\n",
    "# Visualize the node feature matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(X, cmap=\"viridis\")\n",
    "plt.colorbar(label=\"Feature Value\")\n",
    "plt.title(\"Node Feature Matrix (One-hot encoded atom types)\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Node Index\")\n",
    "plt.yticks(range(n_atoms), atom_symbols)\n",
    "plt.xticks(range(n_features), unique_atoms)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFNNguP7-_jm"
   },
   "source": [
    "### 2.2 Graph Prediction Tasks\n",
    "Before diving deeper into GCNs, let's understand what kinds of problems we can solve with graph-based machine learning.\n",
    "\n",
    "Graph-based problems typically fall into three main categories:\n",
    "\n",
    "1. **Node Classification**: Predicting properties of individual nodes\n",
    "    - *Chemistry example*: Predicting the reactivity of specific atoms in a molecule\n",
    "\n",
    "2. **Link Prediction / Edge Classification**: Predicting connections between nodes\n",
    "    - *Chemistry example*: Predicting if a chemical bond might form between two atoms\n",
    "\n",
    "3. **Graph Classification**: Predicting properties of entire graphs\n",
    "    - *Chemistry example*: Predicting if a molecule will be active against a biological target\n",
    "\n",
    "In this tutorial, we'll focus primarily on graph classification, as we want to predict properties of entire molecules.\n",
    "\n",
    "<image src=\"https://storage.googleapis.com/dm-educational/assets/graph-nets/graph_tasks.png\" width=\"700px\">\n",
    "\n",
    "### 2.3 The Graph Convolutional Network Model (Kipf & Welling, 2017)\n",
    "\n",
    "The key insight of GCNs is to perform convolution operations directly on graphs by aggregating neighboring node features. This allows for learning representations that respect the graph structure.\n",
    "\n",
    "In the seminal paper by Thomas Kipf and Max Welling (2017), the layer-wise propagation rule for GCNs is defined as:\n",
    "\n",
    "$$H^{(l+1)} = \\sigma(\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)})$$\n",
    "\n",
    "Where:\n",
    "- $H^{(l)}$ is the matrix of node features at layer $l$ (with $H^{(0)} = X$)\n",
    "- $\\tilde{A} = A + I$ is the adjacency matrix with self-connections (identity matrix $I$)\n",
    "- $\\tilde{D}$ is the degree matrix of $\\tilde{A}$\n",
    "- $W^{(l)}$ is the weight matrix for layer $l$\n",
    "- $\\sigma$ is a non-linear activation function (e.g., ReLU)\n",
    "\n",
    "The GCN operation can be broken down into two main steps:\n",
    "\n",
    "1. **Compute messages / update node features**: Create a feature vector $\\vec{h}_n$ for each node $n$ (using a neural network). This becomes the message that the node will pass to its neighbors.\n",
    "\n",
    "2. **Message-passing / aggregate node features**: For each node, calculate a new feature vector $\\vec{h}'_n$ based on the messages from its neighborhood. This is how information flows through the molecular graph.\n",
    "\n",
    "The diagram below illustrates this aggregation step:\n",
    "\n",
    "<image src=\"https://storage.googleapis.com/dm-educational/assets/graph-nets/graph_conv.png\" width=\"500px\">\n",
    "\n",
    "*\\\"A generic overview of a graph convolution operation, highlighting the relevant information for deriving the next-level features for every node in the graph.\\\"* Image source: Petar Veličković (https://github.com/PetarV-/TikZ)\n",
    "\n",
    "\n",
    "Let's visualize how this works with a simple graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "FDE4NQ0r_dks",
    "outputId": "036b3c5d-a7c7-4f38-8b77-d553055fe29b"
   },
   "outputs": [],
   "source": [
    "# Create a simpler example for illustration\n",
    "simple_mol_smiles = \"CCO\"  # Ethanol - a simple molecule with just 3 atoms\n",
    "simple_mol = Chem.MolFromSmiles(simple_mol_smiles)\n",
    "\n",
    "# Visualize the molecule\n",
    "simple_mol_img = Draw.MolToImage(simple_mol, size=(300, 200))\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.imshow(simple_mol_img)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Ethanol Molecule\")\n",
    "plt.show()\n",
    "\n",
    "# Create a graph from the molecule\n",
    "G_simple = nx.Graph()\n",
    "\n",
    "# Add nodes (atoms)\n",
    "atom_symbols_simple = []\n",
    "for atom in simple_mol.GetAtoms():\n",
    "    G_simple.add_node(atom.GetIdx(), symbol=atom.GetSymbol())\n",
    "    atom_symbols_simple.append(atom.GetSymbol())\n",
    "\n",
    "# Add edges (bonds)\n",
    "for bond in simple_mol.GetBonds():\n",
    "    G_simple.add_edge(bond.GetBeginAtomIdx(), bond.GetEndAtomIdx())\n",
    "\n",
    "# Visualize the graph\n",
    "plt.figure(figsize=(8, 5))\n",
    "pos_simple = nx.spring_layout(G_simple, seed=42)\n",
    "nx.draw(\n",
    "    G_simple,\n",
    "    pos_simple,\n",
    "    with_labels=True,\n",
    "    labels={i: atom_symbols_simple[i] for i in range(len(atom_symbols_simple))},\n",
    "    node_color=\"lightgreen\",\n",
    "    node_size=700,\n",
    "    font_weight=\"bold\",\n",
    "    font_size=14,\n",
    ")\n",
    "plt.title(\"Graph Representation of Ethanol\")\n",
    "plt.show()\n",
    "\n",
    "# Get adjacency matrix\n",
    "A_simple = nx.adjacency_matrix(G_simple).toarray()\n",
    "\n",
    "# Calculate the normalized adjacency matrix as used in GCN\n",
    "# Step 1: Add self-loops to the adjacency matrix (atoms can pass messages to themselves)\n",
    "A_tilde = A_simple + np.eye(A_simple.shape[0])  # Add self-loops\n",
    "\n",
    "# Step 2: Calculate the degree matrix (diagonal matrix with number of connections per node)\n",
    "D_tilde = np.diag(np.sum(A_tilde, axis=1))  # Degree matrix\n",
    "\n",
    "# Step 3: Calculate the inverse square root of the degree matrix\n",
    "D_tilde_inv_sqrt = np.diag(1 / np.sqrt(np.sum(A_tilde, axis=1)))  # D^(-1/2)\n",
    "\n",
    "# Step 4: Calculate the normalized adjacency matrix used in GCN\n",
    "# This normalization ensures the scale of features doesn't explode during message passing\n",
    "normalized_A = D_tilde_inv_sqrt @ A_tilde @ D_tilde_inv_sqrt  # D^(-1/2) A D^(-1/2)\n",
    "\n",
    "# Visualize the matrices to understand the transformation\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].imshow(A_simple, cmap=\"Blues\")\n",
    "axes[0].set_title(\"Adjacency Matrix (A)\")\n",
    "axes[0].set_xticks(range(len(atom_symbols_simple)))\n",
    "axes[0].set_yticks(range(len(atom_symbols_simple)))\n",
    "axes[0].set_xticklabels(atom_symbols_simple)\n",
    "axes[0].set_yticklabels(atom_symbols_simple)\n",
    "\n",
    "axes[1].imshow(A_tilde, cmap=\"Blues\")\n",
    "axes[1].set_title(\"Adjacency Matrix with Self-Loops (Ã)\")\n",
    "axes[1].set_xticks(range(len(atom_symbols_simple)))\n",
    "axes[1].set_yticks(range(len(atom_symbols_simple)))\n",
    "axes[1].set_xticklabels(atom_symbols_simple)\n",
    "axes[1].set_yticklabels(atom_symbols_simple)\n",
    "\n",
    "axes[2].imshow(normalized_A, cmap=\"Blues\")\n",
    "axes[2].set_title(\"Normalized Adjacency Matrix (D^(-1/2) Ã D^(-1/2))\")\n",
    "axes[2].set_xticks(range(len(atom_symbols_simple)))\n",
    "axes[2].set_yticks(range(len(atom_symbols_simple)))\n",
    "axes[2].set_xticklabels(atom_symbols_simple)\n",
    "axes[2].set_yticklabels(atom_symbols_simple)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create one-hot features for this simple molecule\n",
    "# Each atom type (C, O) gets its own dimension in the feature vector\n",
    "unique_atoms_simple = sorted(set(atom_symbols_simple))\n",
    "atom_to_idx_simple = {atom: i for i, atom in enumerate(unique_atoms_simple)}\n",
    "n_atoms_simple = len(atom_symbols_simple)\n",
    "n_features_simple = len(unique_atoms_simple)\n",
    "\n",
    "X_simple = np.zeros((n_atoms_simple, n_features_simple))\n",
    "for i, atom in enumerate(atom_symbols_simple):\n",
    "    X_simple[i, atom_to_idx_simple[atom]] = 1\n",
    "\n",
    "# Visualize the feature matrix\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.imshow(X_simple, cmap=\"viridis\")\n",
    "plt.colorbar(label=\"Feature Value\")\n",
    "plt.title(\"Node Feature Matrix (One-hot encoded atom types)\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Node Index\")\n",
    "plt.yticks(range(n_atoms_simple), atom_symbols_simple)\n",
    "plt.xticks(range(n_features_simple), unique_atoms_simple)\n",
    "plt.show()\n",
    "\n",
    "# Simulate a GCN layer with random weights\n",
    "np.random.seed(42)\n",
    "hidden_dim = 4\n",
    "W = np.random.randn(n_features_simple, hidden_dim)\n",
    "\n",
    "# Simulating GCN forward pass:\n",
    "# Step 1: Apply the normalized adjacency matrix to aggregate neighborhood information\n",
    "# Step 2: Apply the weight matrix to transform features\n",
    "# Step 3: Apply ReLU activation function\n",
    "H = normalized_A @ X_simple @ W\n",
    "H_activated = np.maximum(0, H)  # ReLU activation\n",
    "\n",
    "# Visualize the transformation\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Original features\n",
    "axes[0].imshow(X_simple, cmap=\"viridis\")\n",
    "axes[0].set_title(\"Input Features (X)\")\n",
    "axes[0].set_xlabel(\"Feature Dimension\")\n",
    "axes[0].set_ylabel(\"Node\")\n",
    "axes[0].set_yticks(range(n_atoms_simple))\n",
    "axes[0].set_yticklabels(atom_symbols_simple)\n",
    "\n",
    "# After message passing (before weight transformation)\n",
    "axes[1].imshow(normalized_A @ X_simple, cmap=\"viridis\")\n",
    "axes[1].set_title(\"After Message Passing (D^(-1/2) Ã D^(-1/2).X)\")\n",
    "axes[1].set_xlabel(\"Feature Dimension\")\n",
    "axes[1].set_ylabel(\"Node\")\n",
    "axes[1].set_yticks(range(n_atoms_simple))\n",
    "axes[1].set_yticklabels(atom_symbols_simple)\n",
    "\n",
    "# After weight transformation and ReLU activation\n",
    "axes[2].imshow(H_activated, cmap=\"viridis\")\n",
    "axes[2].set_title(\"After Weight Transform & ReLU (ReLU(D^(-1/2) Ã D^(-1/2)·X·W))\")\n",
    "axes[2].set_xlabel(\"Hidden Dimension\")\n",
    "axes[2].set_ylabel(\"Node\")\n",
    "axes[2].set_yticks(range(n_atoms_simple))\n",
    "axes[2].set_yticklabels(atom_symbols_simple)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6P5XnxuBZEG"
   },
   "source": [
    "### 2.3 Understanding GCN Intuitively for Molecules\n",
    "\n",
    "At the core, GCNs learn node representations by aggregating information from neighboring nodes. This is particularly well-suited for molecules because:\n",
    "\n",
    "1. **Chemical Context**: Atoms' properties in molecules are highly influenced by their bonding environment\n",
    "2. **Invariance**: The model respects the graph structure, ensuring prediction invariance to node ordering\n",
    "3. **Message Passing**: Information flows along chemical bonds, mimicking how electronic effects propagate through molecules\n",
    "\n",
    "Let's visualize the message passing process in a GCN to understand it more clearly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1RlrAM2ABaqL",
    "outputId": "8a53a69c-ff40-4f25-a3e6-e979a79fc574"
   },
   "outputs": [],
   "source": [
    "def visualize_message_passing(G: nx.Graph, \n",
    "                            pos: dict[int, tuple[float, float]], \n",
    "                            atom_labels: dict[int, str], \n",
    "                            features: np.ndarray,\n",
    "                            title: str) -> None:\n",
    "    \"\"\"Visualize the message passing mechanism.\n",
    "    \n",
    "    Args:\n",
    "        G: NetworkX graph\n",
    "        pos: Dictionary of node positions\n",
    "        atom_labels: Dictionary of atom labels\n",
    "        features: Array of node features\n",
    "        title: Title of the plot\n",
    "    \"\"\"\n",
    "    n_nodes = len(G.nodes())\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Draw the graph structure\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "    nx.draw_networkx_labels(\n",
    "        G, pos, labels=atom_labels, font_size=14, font_weight=\"bold\"\n",
    "    )\n",
    "    \n",
    "    # Create a color map from features\n",
    "    node_colors = [\n",
    "        \"#1f77b4\",\n",
    "        \"#ff7f0e\",\n",
    "        \"#2ca02c\",\n",
    "    ]  # Different colors for different atom types\n",
    "\n",
    "    # Draw nodes with their features as pie charts\n",
    "    for i in range(n_nodes):\n",
    "        feature = features[i]\n",
    "        # Normalize features\n",
    "        if np.sum(feature) > 0:\n",
    "            feature = feature / np.sum(feature)\n",
    "\n",
    "        # Draw a pie chart\n",
    "        plt.pie(\n",
    "            [feature[j] for j in range(len(feature))],\n",
    "            colors=[node_colors[j] for j in range(len(feature))],\n",
    "            center=(pos[i][0], pos[i][1]),\n",
    "            radius=0.1,\n",
    "            normalize=False  # Since we already normalized manually\n",
    "        )\n",
    "\n",
    "    # Draw arrows to represent message passing\n",
    "    for u, v in G.edges():\n",
    "        # Calculate the midpoint of the edge\n",
    "        mid_x = (pos[u][0] + pos[v][0]) / 2\n",
    "        mid_y = (pos[u][1] + pos[v][1]) / 2\n",
    "\n",
    "        # Calculate the vector from u to v\n",
    "        dx = pos[v][0] - pos[u][0]\n",
    "        dy = pos[v][1] - pos[u][1]\n",
    "\n",
    "        # Normalize the vector\n",
    "        length = np.sqrt(dx**2 + dy**2)\n",
    "        if length == 0:  # Add check for zero division\n",
    "            continue\n",
    "            \n",
    "        dx = dx / length\n",
    "        dy = dy / length\n",
    "\n",
    "        # Draw an arrow near the midpoint\n",
    "        plt.arrow(\n",
    "            mid_x - 0.15 * dx,\n",
    "            mid_y - 0.15 * dy,\n",
    "            0.1 * dx,\n",
    "            0.1 * dy,\n",
    "            head_width=0.05,\n",
    "            head_length=0.05,\n",
    "            fc=\"red\",\n",
    "            ec=\"red\",\n",
    "        )\n",
    "        plt.arrow(\n",
    "            mid_x + 0.05 * dx,\n",
    "            mid_y + 0.05 * dy,\n",
    "            -0.1 * dx,\n",
    "            -0.1 * dy,\n",
    "            head_width=0.05,\n",
    "            head_length=0.05,\n",
    "            fc=\"blue\",\n",
    "            ec=\"blue\",\n",
    "        )\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create a fixed position layout for our ethanol graph\n",
    "pos_fixed = {0: (0, 0), 1: (1, 0), 2: (2, 0)}\n",
    "\n",
    "# Visualize original features (before any message passing)\n",
    "visualize_message_passing(\n",
    "    G_simple,\n",
    "    pos_fixed,\n",
    "    {i: atom_symbols_simple[i] for i in range(len(atom_symbols_simple))},\n",
    "    X_simple,\n",
    "    \"Message Passing in GCN: Original Node Features\",\n",
    ")\n",
    "\n",
    "# Visualize after first message passing step\n",
    "# Each atom now contains information from its direct neighbors\n",
    "visualize_message_passing(\n",
    "    G_simple,\n",
    "    pos_fixed,\n",
    "    {i: atom_symbols_simple[i] for i in range(len(atom_symbols_simple))},\n",
    "    normalized_A @ X_simple,\n",
    "    \"Message Passing in GCN: After First Neighborhood Aggregation\",\n",
    ")\n",
    "\n",
    "# Visualize after second message passing step \n",
    "# Now each atom has information from atoms up to 2 bonds away\n",
    "# This is how GCNs propagate information through the molecular structure\n",
    "visualize_message_passing(\n",
    "    G_simple,\n",
    "    pos_fixed,\n",
    "    {i: atom_symbols_simple[i] for i in range(len(atom_symbols_simple))},\n",
    "    normalized_A @ (normalized_A @ X_simple),\n",
    "    \"Message Passing in GCN: After Second Neighborhood Aggregation (2-hop)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEgFxEzHB1xv"
   },
   "source": [
    "### 2.4 Key Advantages of GCNs for Molecular Applications\n",
    "\n",
    "1. **Respects Molecular Structure**: Graph-based approach naturally handles the connectivity of atoms\n",
    "2. **Learns Both Local and Global Features**: Can capture both local chemical environments and overall molecular structure\n",
    "3. **Invariant to Atom Ordering**: Same molecule will give same prediction regardless of how atoms are indexed\n",
    "4. **Handles Variable-Sized Molecules**: Can process molecules with different numbers of atoms\n",
    "5. **Interpretable**: Feature maps can often be related back to chemical substructures\n",
    "\n",
    "In the next section, we'll implement a GCN from scratch to better understand its internal workings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lAupHt6NNwS"
   },
   "source": [
    "## 3. Implementing a GCN from Scratch <a name=\"implementing-gcn-scratch\"></a>\n",
    "\n",
    "To deepen our understanding, let's implement a Graph Convolutional Network from scratch using PyTorch. This will help us understand exactly what happens in each layer of the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N2J_QzCNjGZT",
    "outputId": "9cb81888-cef9-42a1-b601-589bbe44669a"
   },
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the GraphConvolution layer.\n",
    "        \n",
    "        Args:\n",
    "            in_features (int): Number of input features per node\n",
    "            out_features (int): Number of output features per node\n",
    "            bias (bool, optional): If True, add a bias term. Default is True.\n",
    "        \"\"\"\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        \"\"\"\n",
    "        Reset the parameters of the layer.\n",
    "        \"\"\"\n",
    "        nn.init.kaiming_uniform_(self.weight)  # Initialize weights\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)  # Initialize bias to zero\n",
    "\n",
    "    def forward(self, input: torch.Tensor, adj: torch.Tensor) -> torch.Tensor:\n",
    "        support = torch.mm(input, self.weight)  # Linear transformation\n",
    "        output = torch.spmm(adj, support)  # Matrix multiplication with adjacency matrix (should be normalized adjacency matrix)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias  # Add bias if it exists\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            self.__class__.__name__\n",
    "            + \" (\"\n",
    "            + str(self.in_features)\n",
    "            + \" -> \"\n",
    "            + str(self.out_features)\n",
    "            + \")\"\n",
    "        )\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nfeat: int, nhid: int, nout: int, dropout: float) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the GCN model.\n",
    "        \n",
    "        Args:\n",
    "            nfeat (int): Number of input features per node\n",
    "            nhid (int): Number of hidden features\n",
    "            nout (int): Number of output features\n",
    "            dropout (float): Dropout rate\n",
    "        \"\"\"\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)  # First GCN layer\n",
    "        self.gc2 = GraphConvolution(nhid, nout)  # Second GCN layer\n",
    "        self.dropout = dropout  # Dropout rate\n",
    "\n",
    "    def forward(self, x: torch.Tensor, adj: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the GCN model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input features\n",
    "            adj (torch.Tensor): Adjacency matrix (should be normalized adjacency matrix)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output features\n",
    "        \"\"\"\n",
    "        x = F.relu(self.gc1(x, adj))  # Apply first GCN layer\n",
    "        x = F.dropout(x, self.dropout, training=self.training)  # Apply dropout\n",
    "        x = self.gc2(x, adj)  # Apply second GCN layer\n",
    "        return x\n",
    "\n",
    "\n",
    "# Let's test our implementation on the simple ethanol molecule\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "adj_tensor = torch.FloatTensor(normalized_A)\n",
    "features_tensor = torch.FloatTensor(X_simple)\n",
    "\n",
    "# Initialize a GCN model\n",
    "model = GCN(nfeat=n_features_simple, nhid=8, nout=2, dropout=0.5)\n",
    "print(model)\n",
    "\n",
    "# Forward pass\n",
    "output = model(features_tensor, adj_tensor)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Output features:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnMpZBAgB-QL"
   },
   "source": [
    "## 4. Visualizing the GCN Layer Operations in Detail <a name=\"visualization\"></a>\n",
    "\n",
    "Let's break down the GCN operations step by step to understand what's happening inside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "maMpRFgzB6VX",
    "outputId": "bfe547c9-45de-46d9-b9b0-d8575a221d5b"
   },
   "outputs": [],
   "source": [
    "# Let's use a simple artificial example\n",
    "n_nodes = 4\n",
    "n_features = 3\n",
    "\n",
    "# Create a synthetic graph\n",
    "G_demo = nx.cycle_graph(n_nodes)  # A cycle graph with 4 nodes\n",
    "A_demo = nx.adjacency_matrix(G_demo).toarray()\n",
    "\n",
    "# Add self-loops and normalize\n",
    "A_tilde_demo = A_demo + np.eye(n_nodes)  # Add self-loops to the adjacency matrix\n",
    "D_tilde_demo = np.diag(np.sum(A_tilde_demo, axis=1))  # Create degree matrix\n",
    "D_tilde_inv_sqrt_demo = np.linalg.inv(np.sqrt(D_tilde_demo))  # Invert square root of degree matrix\n",
    "normalized_A_demo = D_tilde_inv_sqrt_demo @ A_tilde_demo @ D_tilde_inv_sqrt_demo  # Normalize adjacency matrix\n",
    "\n",
    "# Create random feature matrix\n",
    "np.random.seed(42)\n",
    "X_demo = np.random.rand(n_nodes, n_features)\n",
    "\n",
    "# Create random weight matrix\n",
    "W_demo = np.random.rand(n_features, 2)  # Output dimension is 2\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_demo_tensor = torch.FloatTensor(X_demo)\n",
    "A_demo_tensor = torch.FloatTensor(normalized_A_demo)\n",
    "W_demo_tensor = torch.FloatTensor(W_demo)\n",
    "\n",
    "# Step 1: Linear transformation of features\n",
    "linear_transform = torch.mm(X_demo_tensor, W_demo_tensor)  # Linear transformation\n",
    "\n",
    "# Step 2: Aggregation with normalized adjacency matrix\n",
    "aggregated = torch.mm(A_demo_tensor, linear_transform)  # Aggregation\n",
    "\n",
    "# Step 3: Apply non-linearity (ReLU)\n",
    "activated = F.relu(aggregated)  # Apply ReLU activation\n",
    "\n",
    "# Visualize each step\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(X_demo, cmap=\"viridis\")\n",
    "plt.title(\"Input Node Features (X)\")\n",
    "plt.xlabel(\"Feature Dimension\")\n",
    "plt.ylabel(\"Node ID\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(linear_transform.numpy(), cmap=\"viridis\")\n",
    "plt.title(\"After Linear Transform (XW)\")\n",
    "plt.xlabel(\"Hidden Dimension\")\n",
    "plt.ylabel(\"Node ID\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(aggregated.numpy(), cmap=\"viridis\")\n",
    "plt.title(\"After Aggregation (D^(-1/2) Ã D^(-1/2)XW)\")\n",
    "plt.xlabel(\"Hidden Dimension\")\n",
    "plt.ylabel(\"Node ID\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Alternative view: Following one node through the network\n",
    "node_idx = 0  # Focus on first node\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Original feature vector for this node\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(range(n_features), X_demo[node_idx])\n",
    "plt.title(f\"Original Features of Node {node_idx}\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Feature Value\")\n",
    "\n",
    "# After linear transform\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(range(2), linear_transform[node_idx].numpy())\n",
    "plt.title(f\"Node {node_idx} After Linear Transform\")\n",
    "plt.xlabel(\"Hidden Feature Index\")\n",
    "plt.ylabel(\"Feature Value\")\n",
    "\n",
    "# After aggregation with neighbors\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(range(2), aggregated[node_idx].numpy())\n",
    "plt.title(f\"Node {node_idx} After Neighborhood Aggregation\")\n",
    "plt.xlabel(\"Hidden Feature Index\")\n",
    "plt.ylabel(\"Feature Value\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize the graph with it's embedding\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Draw the graph structure\n",
    "pos_demo = nx.spring_layout(G_demo, seed=42)\n",
    "nx.draw(\n",
    "    G_demo,\n",
    "    pos_demo,\n",
    "    node_color=\"lightblue\",\n",
    "    node_size=500,\n",
    "    with_labels=True,\n",
    "    font_weight=\"bold\",\n",
    ")\n",
    "\n",
    "# Plot the 2D embedding beside each node\n",
    "offset = 0.1\n",
    "for i in range(n_nodes):\n",
    "    plt.text(\n",
    "        pos_demo[i][0] + offset,\n",
    "        pos_demo[i][1] + offset,\n",
    "        f\"Embedding:\\n[{aggregated[i, 0]:.2f}, {aggregated[i, 1]:.2f}]\",\n",
    "        fontsize=10,\n",
    "        bbox=dict(facecolor=\"white\", alpha=0.5),\n",
    "    )\n",
    "\n",
    "plt.title(\"Graph with GCN Embeddings\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjsOlpLlCXF_"
   },
   "source": [
    "## 5. Building a Complete GCN for Molecular Property Prediction <a name=\"gcn-molecular-property-prediction\"></a>\n",
    "\n",
    "Now that we understand the GCN architecture, let's use PyTorch Geometric (PyG) to build a complete model for molecular property prediction. We'll use the ESOL dataset, a collection of data (1128 molecules), used for estimating the aqueous solubility of chemical compounds directly from their molecular structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Sx0uBCwJCEB5",
    "outputId": "b41e1d2b-2b3b-458e-d57d-c838bd78c4b9"
   },
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Download and process the ESOL dataset\n",
    "dataset = MoleculeNet(root=\"data/\", name=\"ESOL\")\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_dataset, val_dataset = train_test_split(\n",
    "    train_dataset, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Check a sample from the dataset\n",
    "sample = next(iter(train_loader))\n",
    "print(\"Sample from ESOL dataset:\")\n",
    "print(f\"Number of graphs in batch: {sample.num_graphs}\")\n",
    "print(f\"Node features shape: {sample.x.shape}\")\n",
    "print(f\"Edge index shape: {sample.edge_index.shape}\")\n",
    "print(f\"Target shape: {sample.y.shape}\")\n",
    "print(f\"Feature dimensions: {sample.num_features}\")\n",
    "\n",
    "\n",
    "# Define our GCN model using PyTorch Geometric\n",
    "class MolecularGCN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Molecular Graph Convolutional Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, node_features: int, hidden_channels: int, out_channels: int) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the MolecularGCN model.\n",
    "        \n",
    "        Args:\n",
    "            node_features (int): Number of input features per node\n",
    "            hidden_channels (int): Number of hidden features\n",
    "            out_channels (int): Number of output features\n",
    "        \"\"\"\n",
    "        super(MolecularGCN, self).__init__()\n",
    "        # GCN layers\n",
    "        self.conv1 = GCNConv(node_features, hidden_channels)  # First GCN layer\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)  # Second GCN layer\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels // 2)  # Third GCN layer\n",
    "\n",
    "        # Readout layers\n",
    "        self.lin1 = torch.nn.Linear(hidden_channels // 2, hidden_channels // 4)  # First readout layer\n",
    "        self.lin2 = torch.nn.Linear(hidden_channels // 4, out_channels)  # Second readout layer\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, batch: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the MolecularGCN model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input features\n",
    "            edge_index (torch.Tensor): Edge index\n",
    "            batch (torch.Tensor): Batch index\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output features\n",
    "        \"\"\"\n",
    "        # Node embeddings\n",
    "        x = self.conv1(x, edge_index)  # First GCN layer\n",
    "        x = F.relu(x)  # Apply ReLU activation\n",
    "        x = F.dropout(x, p=0.2, training=self.training)  # Apply dropout\n",
    "\n",
    "        x = self.conv2(x, edge_index)  # Second GCN layer\n",
    "        x = F.relu(x)  # Apply ReLU activation\n",
    "        x = F.dropout(x, p=0.2, training=self.training)  # Apply dropout\n",
    "\n",
    "        x = self.conv3(x, edge_index)  # Third GCN layer\n",
    "        x = F.relu(x)  # Apply ReLU activation\n",
    "\n",
    "        # Global pooling (graph-level representation)\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # Prediction head\n",
    "        x = self.lin1(x)  # First readout layer\n",
    "        x = F.relu(x)  # Apply ReLU activation\n",
    "        x = F.dropout(x, p=0.2, training=self.training)  # Apply dropout\n",
    "\n",
    "        x = self.lin2(x)  # Second readout layer\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = MolecularGCN(\n",
    "    node_features=sample.num_features, hidden_channels=64, out_channels=1\n",
    ")\n",
    "print(model)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.MSELoss()  # Mean Squared Error loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train() -> float:\n",
    "    \"\"\"\n",
    "    Training loop for the MolecularGCN model.\n",
    "    \n",
    "    Returns:\n",
    "        float: Average training loss\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x.float(), data.edge_index, data.batch)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "# Validation function\n",
    "def validate() -> float:\n",
    "    \"\"\"\n",
    "    Validation loop for the MolecularGCN model.\n",
    "    \n",
    "    Returns:\n",
    "        float: Average validation loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            out = model(data.x.float(), data.edge_index, data.batch)\n",
    "            loss = criterion(out, data.y)\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(val_loader.dataset)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "epochs = 150\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train()\n",
    "    val_loss = validate()\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    if epoch % 5 == 0:\n",
    "        print(\n",
    "            f\"Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs + 1), train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(1, epochs + 1), val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        out = model(data.x.float(), data.edge_index, data.batch)\n",
    "        y_true.extend(data.y.view(-1).tolist())\n",
    "        y_pred.extend(out.view(-1).tolist())\n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Test MAE: {mae:.4f}\")\n",
    "print(f\"Test R²: {r2:.4f}\")\n",
    "\n",
    "# Visualize predictions vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], \"r--\")\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Predicted vs True Values\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Create a residual plot\n",
    "residuals = np.array(y_true) - np.array(y_pred)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color=\"r\", linestyle=\"--\")\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Histogram of residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(residuals, bins=30, alpha=0.7, color=\"skyblue\")\n",
    "plt.xlabel(\"Residual Value\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Histogram of Residuals\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDF6nXLOCthx"
   },
   "source": [
    "## 6. Visualizing Learned Molecular Representations <a name=\"visualize-learned-representation\"></a>\n",
    "\n",
    "One of the advantages of GCNs is that they learn meaningful representations of molecules. Let's visualize these learned representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rfIh6k7mfSq5",
    "outputId": "d65d3927-5da0-4e2b-c588-bcaa6342f1f1"
   },
   "outputs": [],
   "source": [
    "# Get embeddings for test molecules\n",
    "model.eval()\n",
    "embeddings = []\n",
    "targets = []\n",
    "molecules = []\n",
    "\n",
    "\n",
    "# Function to convert a molecule to its SMILES representation\n",
    "def mol_to_smiles(data, index):\n",
    "    # This is a simplified version - in practice you'd need to convert\n",
    "    # from PyG format back to RDKit format\n",
    "    return data[index].smiles\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        # Get intermediate representations (after the last GCN layer, before pooling)\n",
    "        x = model.conv1(data.x.float(), data.edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = model.conv2(x, data.edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = model.conv3(x, data.edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Pooling to get molecule-level representation\n",
    "        x_mol = global_mean_pool(x, data.batch)\n",
    "\n",
    "        # Store embeddings and targets\n",
    "        embeddings.extend(x_mol.cpu().numpy())\n",
    "        targets.extend(data.y.view(-1).cpu().numpy())\n",
    "\n",
    "        # For each molecule in the batch, extract SMILES\n",
    "        batch_size = data.num_graphs\n",
    "        for i in range(batch_size):\n",
    "            # Get indices for this molecule\n",
    "            mask = data.batch == i\n",
    "            x_mol = data.x[mask]\n",
    "            edge_index_mol = data.edge_index[:, mask[data.edge_index[0]]]\n",
    "            # Convert to SMILES (simplified)\n",
    "            smiles = mol_to_smiles(data, i)\n",
    "            molecules.append(smiles)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "embeddings = np.array(embeddings)\n",
    "targets = np.array(targets)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "# Apply t-SNE for non-linear dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_tsne = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Visualize with PCA\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(\n",
    "    embeddings_2d[:, 0], embeddings_2d[:, 1], c=targets, cmap=\"viridis\", alpha=0.7\n",
    ")\n",
    "plt.colorbar(scatter, label=\"Target Property Value\")\n",
    "plt.title(\"PCA of Molecular Embeddings Colored by Target Property\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualize with t-SNE\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(\n",
    "    embeddings_tsne[:, 0], embeddings_tsne[:, 1], c=targets, cmap=\"viridis\", alpha=0.7\n",
    ")\n",
    "plt.colorbar(scatter, label=\"Target Property Value\")\n",
    "plt.title(\"t-SNE of Molecular Embeddings Colored by Target Property\")\n",
    "plt.xlabel(\"t-SNE Component 1\")\n",
    "plt.ylabel(\"t-SNE Component 2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Choose representative molecules\n",
    "selected_indices = np.random.choice(len(embeddings), 5, replace=False)  # Randomly select 5 molecules\n",
    "selected_embeddings = embeddings[selected_indices]  # Select embeddings for the chosen molecules\n",
    "selected_targets = targets[selected_indices]  # Select targets for the chosen molecules\n",
    "selected_molecules = [molecules[i] for i in selected_indices]  # Select SMILES for the chosen molecules\n",
    "\n",
    "# Convert SMILES to RDKit molecules\n",
    "mol_objects = [Chem.MolFromSmiles(smiles) for smiles in selected_molecules]\n",
    "\n",
    "# Create a figure with gridspec for custom layout\n",
    "plt.figure(figsize=(16, 12))\n",
    "gs = gridspec.GridSpec(len(selected_embeddings), 2, width_ratios=[1, 3])\n",
    "\n",
    "for i, (embedding, mol, target) in enumerate(\n",
    "    zip(selected_embeddings, mol_objects, selected_targets)\n",
    "):\n",
    "    # Create molecule image in the first column\n",
    "    ax_mol = plt.subplot(gs[i, 0])\n",
    "    img = Draw.MolToImage(mol, size=(300, 300))\n",
    "    ax_mol.imshow(img)\n",
    "    ax_mol.set_title(f\"Target: {target:.4f}\")\n",
    "    ax_mol.axis(\"off\")\n",
    "\n",
    "    # Create embedding barplot in the second column\n",
    "    ax_plot = plt.subplot(gs[i, 1])\n",
    "    ax_plot.bar(range(len(embedding)), embedding)\n",
    "    ax_plot.set_xlabel(\"Embedding Dimension\")\n",
    "    ax_plot.set_ylabel(\"Value\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Snz-JC0EggJ_"
   },
   "source": [
    "## 7. Conclusion and Future Directions <a name=\"conclusions\"></a>\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Graph Convolutional Networks** provide a powerful framework for molecular property prediction by directly operating on the molecular graph structure.\n",
    "\n",
    "2. **Message Passing Mechanism** in GCNs effectively captures both local chemical environments and global molecular properties.\n",
    "\n",
    "3. **Node Features** represent atom properties, while edges represent bonds, making GNNs a natural fit for molecular representation.\n",
    "\n",
    "4. **Architecture Variations** such as attention mechanisms and edge feature incorporation can further improve predictive performance.\n",
    "\n",
    "5. **Real-world Applications** like molecular solubility prediction demonstrate the practical utility of GCNs in drug discovery and materials science.\n",
    "\n",
    "### Future Directions\n",
    "\n",
    "1. **3D Information Integration**: Extending GCNs to incorporate 3D structural information using geometric graph neural networks.\n",
    "\n",
    "2. **Multi-task Learning**: Training GCNs to predict multiple molecular properties simultaneously, leveraging shared representations.\n",
    "\n",
    "3. **Transfer Learning**: Pre-training GCNs on large molecular databases and fine-tuning for specific prediction tasks.\n",
    "\n",
    "4. **Interpretability**: Developing methods to interpret what GCNs learn about molecular structures and properties.\n",
    "\n",
    "5. **Reaction and Interaction Modeling**: Extending beyond single molecules to model chemical reactions and protein-ligand interactions.\n",
    "\n",
    "### Additional Resources for Further Learning\n",
    "\n",
    "- **Papers**:\n",
    "  - Kipf, T. N., & Welling, M. (2017). Semi-supervised classification with graph convolutional networks.\n",
    "  - Gilmer, J., et al. (2017). Neural message passing for quantum chemistry.\n",
    "  - Yang, K., et al. (2019). Analyzing learned molecular representations for property prediction.\n",
    "\n",
    "- **Libraries**:\n",
    "  - PyTorch Geometric: https://pytorch-geometric.readthedocs.io/\n",
    "  - DeepChem: https://deepchem.io/\n",
    "  - RDKit: https://www.rdkit.org/\n",
    "\n",
    "- **Datasets**:\n",
    "  - MoleculeNet: Benchmark datasets for molecular machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJj4WEAxg6dU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
