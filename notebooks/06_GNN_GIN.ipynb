{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HFooladi/GNNs-For-Chemists/blob/main/notebooks/06_GNN_GIN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2E3jLx_kxPa"
   },
   "source": [
    "# Graph Isomorphism Networks (GIN) Tutorial for Chemists and Pharmacists\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Setup and Installation](#setup-and-installation)\n",
    "3. [Understanding Molecular Graphs and Isomorphism](#understanding-molecular-graphs-and-isomorphism)\n",
    "4. [The Theory Behind Graph Isomorphism Networks (GIN)](#theory-behind-gin)\n",
    "5. [Implementing a GIN Model for Molecules](#implementing-a-gin-model-for-molecules)\n",
    "6. [Training GIN for Molecular Property Prediction](#training-gin-for-molecular-property-prediction)\n",
    "7. [GIN for Molecular Fingerprinting](#gin-for-molecular-fingerprinting)\n",
    "8. [Analyzing Isomorphic vs. Non-Isomorphic Molecule Pairs](#analyzing-isomorphic-vs-non-isomorphic-molecule-pairs)\n",
    "9. [Interpreting GIN Embeddings for Chemical Insights](#interpreting-gin-embeddings-for-chemical-insights)\n",
    "10. [Ablation Studies: Understanding GIN Components](#ablation-studies-understanding-gin-components)\n",
    "11. [Practical Applications in Drug Discovery](#practical-applications-in-drug-discovery)\n",
    "12. [Conclusion and Best Practices](#conclusion-and-best-practices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5o21TjeX47a"
   },
   "source": [
    "## 1. Introduction <a name=\"introduction\"></a>\n",
    "\n",
    "Graph Isomorphism Networks (GINs) are a powerful class of Graph Neural Networks designed to maximize the discriminative power when differentiating graph structures. This makes them particularly valuable for molecular applications, where subtle structural differences between molecules can lead to completely different chemical properties.\n",
    "\n",
    "In this tutorial, we'll explore GINs with a specific focus on:\n",
    "1. Understanding the theoretical foundations of GINs and their discriminative power\n",
    "2. Implementing GINs for molecular property prediction\n",
    "3. Using GINs for molecular fingerprinting and similarity search\n",
    "4. Comparing GIN performance with other GNN architectures (GCN, GAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGRiCNQ6k19K"
   },
   "source": [
    "## 2. Setup and Installation <a name=\"setup-and-installation\"></a>\n",
    "\n",
    "First, let's install the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WMi-JeWZkw7b",
    "outputId": "3069f8e3-d2b5-4b9b-ab35-7ba5ae9a62db"
   },
   "outputs": [],
   "source": [
    "#@title Intstall necessary libraries\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "!pip install -q rdkit\n",
    "!pip install -q networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OEoTHgnjkiC0",
    "outputId": "d2760a79-43e2-4a5e-a3c6-72e7c4aaba24"
   },
   "outputs": [],
   "source": [
    "#@title Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINConv, GCNConv, GATConv\n",
    "from torch_geometric.nn import global_add_pool, global_max_pool, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, AllChem\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import io\n",
    "from PIL import Image\n",
    "import random\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8aZ3-MQ8lBSq"
   },
   "source": [
    "## 3. Understanding Molecular Graphs and Isomorphism <a name=\"understanding-molecular-graphs-and-isomorphism\"></a>\n",
    "\n",
    "Before diving into GINs, let's first understand what graph isomorphism means in the context of molecular graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PinXWvpnlBwp",
    "outputId": "878dd09e-adc7-406b-af77-938ad61ce87c"
   },
   "outputs": [],
   "source": [
    "def atom_features(atom):\n",
    "    \"\"\"\n",
    "    Extract a feature vector for an RDKit atom.\n",
    "\n",
    "    Features included:\n",
    "        - Atomic number\n",
    "        - Chirality tag (encoded as integer)\n",
    "        - Degree (number of directly-bonded atoms)\n",
    "        - Formal charge\n",
    "        - Total number of hydrogens\n",
    "        - Number of radical electrons\n",
    "        - Hybridization (encoded as integer)\n",
    "        - Aromaticity (0 or 1)\n",
    "        - Ring membership (0 or 1)\n",
    "\n",
    "    Args:\n",
    "        atom (rdkit.Chem.rdchem.Atom): An RDKit Atom object.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Feature tensor of shape (9,) with dtype long.\n",
    "    \"\"\"\n",
    "    return torch.tensor([\n",
    "        atom.GetAtomicNum(),                    # Atomic number\n",
    "        int(atom.GetChiralTag()),               # Chirality\n",
    "        atom.GetDegree(),                       # Degree\n",
    "        atom.GetFormalCharge(),                 # Formal charge\n",
    "        atom.GetTotalNumHs(),                   # Number of hydrogens\n",
    "        atom.GetNumRadicalElectrons(),          # Radical electrons\n",
    "        int(atom.GetHybridization()),           # Hybridization\n",
    "        int(atom.GetIsAromatic()),              # Aromaticity\n",
    "        int(atom.IsInRing())                    # Ring membership\n",
    "    ], dtype=torch.long)\n",
    "\n",
    "\n",
    "def bond_features(bond):\n",
    "    \"\"\"\n",
    "    Extract a feature vector for an RDKit bond.\n",
    "\n",
    "    Features included:\n",
    "        - Bond type as double (e.g., 1.0 for single, 2.0 for double)\n",
    "        - Conjugation (0 or 1)\n",
    "        - Ring membership (0 or 1)\n",
    "\n",
    "    Args:\n",
    "        bond (rdkit.Chem.rdchem.Bond): An RDKit Bond object.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Feature tensor of shape (3,) with dtype long.\n",
    "    \"\"\"\n",
    "    return torch.tensor([\n",
    "        int(bond.GetBondTypeAsDouble()),        # Bond type\n",
    "        int(bond.GetIsConjugated()),            # Conjugation\n",
    "        int(bond.IsInRing())                    # Ring membership\n",
    "    ], dtype=torch.long)\n",
    "\n",
    "def mol_to_graph(smiles):\n",
    "    \"\"\"\n",
    "    Converts a SMILES into a PyTorch Geometric graph data object.\n",
    "\n",
    "    Nodes represent atoms with features, and edges represent bonds with features.\n",
    "    The graph is undirected: each bond adds two directed edges (i->j and j->i).\n",
    "\n",
    "    Args:\n",
    "        smiles (str): SMILES representing the molecule.\n",
    "\n",
    "    Returns:\n",
    "        torch_geometric.data.Data: Graph data object containing:\n",
    "            - x: Node feature matrix [num_nodes, 9]\n",
    "            - edge_index: Edge list [2, num_edges]\n",
    "            - edge_attr: Edge feature matrix [num_edges, 3]\n",
    "            - smiles: Original SMILES\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        raise ValueError(f\"Invalid SMILES string: {smiles}\")\n",
    "\n",
    "    # Node features\n",
    "    x = torch.stack([atom_features(atom) for atom in mol.GetAtoms()], dim=0)\n",
    "\n",
    "    # Edge index and edge features\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "\n",
    "        # Add both directions for undirected graph\n",
    "        edge_index.append((i, j))\n",
    "        edge_index.append((j, i))\n",
    "\n",
    "        edge_attr.append(bond_features(bond))\n",
    "        edge_attr.append(bond_features(bond))\n",
    "\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.stack(edge_attr, dim=0) if edge_attr else None\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, smiles=smiles)\n",
    "    return data\n",
    "\n",
    "\n",
    "def visualize_molecule(smiles, title=\"Molecule\"):\n",
    "    \"\"\"Visualize a molecule using RDKit\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    AllChem.Compute2DCoords(mol)\n",
    "\n",
    "    # Draw molecule\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    drawer = rdMolDraw2D.MolDraw2DCairo(500, 500)\n",
    "    drawer.DrawMolecule(mol)\n",
    "    drawer.FinishDrawing()\n",
    "    img = drawer.GetDrawingText()\n",
    "\n",
    "    # Convert the image data to a PIL Image\n",
    "    pil_image = Image.open(io.BytesIO(img))\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(pil_image)\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def visualize_molecular_graph(smiles, title=\"Molecular Graph\"):\n",
    "    \"\"\"\n",
    "    Visualizes the 2D structure of a molecule using RDKit and networkx and displays it.\n",
    "\n",
    "    Args:\n",
    "        smiles (str): SMILES representing the molecule.\n",
    "        title (str): Plot title.\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    AllChem.Compute2DCoords(mol)\n",
    "\n",
    "    data = mol_to_graph(smiles)\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "    # Get the 2D coordinates from RDKit\n",
    "    pos = {}\n",
    "    for i, atom in enumerate(mol.GetAtoms()):\n",
    "        pos[i] = mol.GetConformer().GetAtomPosition(i)\n",
    "        pos[i] = (pos[i].x, -pos[i].y)  # Flip y for better visualization\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "\n",
    "    # Get atom labels\n",
    "    atom_labels = {i: atom.GetSymbol() for i, atom in enumerate(mol.GetAtoms())}\n",
    "\n",
    "    # Get atom features for node coloring\n",
    "    atom_features = [atom.GetAtomicNum() for atom in mol.GetAtoms()]\n",
    "\n",
    "    # Draw the graph\n",
    "    nx.draw(G, pos,\n",
    "            labels=atom_labels,\n",
    "            with_labels=True,\n",
    "            node_color=atom_features,\n",
    "            cmap=plt.cm.viridis,\n",
    "            node_size=500,\n",
    "            font_size=10,\n",
    "            font_color='white',\n",
    "            edge_color='gray')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Let's understand isomorphism with some examples\n",
    "isomorphic_pair = {\n",
    "    \"2-methylbutane\": \"CC(C)CC\",\n",
    "    \"isopentane\": \"CCC(C)C\"\n",
    "}\n",
    "\n",
    "non_isomorphic_pair = {\n",
    "    \"n-pentane\": \"CCCCC\",\n",
    "    \"2-methylbutane\": \"CC(C)CC\"\n",
    "}\n",
    "\n",
    "# Visualize isomorphic molecules\n",
    "print(\"Isomorphic Molecules (Same Structure, Different Representation):\")\n",
    "for name, smiles in isomorphic_pair.items():\n",
    "    print(f\"\\n{name} - {smiles}\")\n",
    "    visualize_molecule(smiles, f\"{name}\")\n",
    "    visualize_molecular_graph(smiles, f\"{name} Graph\")\n",
    "\n",
    "# Visualize non-isomorphic molecules\n",
    "print(\"\\nNon-isomorphic Molecules (Different Structures):\")\n",
    "for name, smiles in non_isomorphic_pair.items():\n",
    "    print(f\"\\n{name} - {smiles}\")\n",
    "    visualize_molecule(smiles, f\"{name}\")\n",
    "    visualize_molecular_graph(smiles, f\"{name} Graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N88RGJ0nlmeo"
   },
   "source": [
    "### 3.1 Understanding Graph Isomorphism and WL Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "L5knE96MlZcB",
    "outputId": "dd5f8dc4-6d8f-4090-dfdd-bc75c1038fdf"
   },
   "outputs": [],
   "source": [
    "def visualize_isomorphism_concept():\n",
    "    \"\"\"Create a visual explanation of graph isomorphism and the WL test\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "    # Create two isomorphic graphs\n",
    "    G1 = nx.Graph()\n",
    "    G1.add_nodes_from(range(1, 7))\n",
    "    G1.add_edges_from([(1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 1)])\n",
    "\n",
    "    G2 = nx.Graph()\n",
    "    G2.add_nodes_from(range(1, 7))\n",
    "    G2.add_edges_from([(1, 6), (6, 5), (5, 4), (4, 3), (3, 2), (2, 1)])\n",
    "\n",
    "    # Different node positioning to make them look different\n",
    "    pos1 = {\n",
    "        1: (0, 1),\n",
    "        2: (0.866, 0.5),\n",
    "        3: (0.866, -0.5),\n",
    "        4: (0, -1),\n",
    "        5: (-0.866, -0.5),\n",
    "        6: (-0.866, 0.5)\n",
    "    }\n",
    "\n",
    "    pos2 = {\n",
    "        1: (0, 1),\n",
    "        2: (1, 0.5),\n",
    "        3: (0.5, -0.5),\n",
    "        4: (-0.5, -0.5),\n",
    "        5: (-1, 0.5),\n",
    "        6: (-0.5, 0.5)\n",
    "    }\n",
    "\n",
    "    # Draw the original graphs\n",
    "    ax = axes[0, 0]\n",
    "    nx.draw(G1, pos1, with_labels=True, node_color='lightblue', node_size=500, font_size=10, ax=ax)\n",
    "    ax.set_title(\"Graph 1\")\n",
    "\n",
    "    ax = axes[0, 1]\n",
    "    nx.draw(G2, pos2, with_labels=True, node_color='lightgreen', node_size=500, font_size=10, ax=ax)\n",
    "    ax.set_title(\"Graph 2\")\n",
    "\n",
    "    # Create empty subplot for text\n",
    "    ax = axes[0, 2]\n",
    "    ax.axis('off')\n",
    "    ax.text(0.1, 0.5, \"Graph Isomorphism:\\nTwo graphs are isomorphic if they have\\nthe same structure regardless of how\\ntheir nodes are labeled or positioned.\\n\\nThese two graphs look different but\\nhave identical connectivity patterns.\",\n",
    "            fontsize=12, ha='left', va='center')\n",
    "\n",
    "    # Now visualize WL test iterations\n",
    "    # Iteration 1: Initialize with degree\n",
    "    wl_colors1 = {n: G1.degree(n) for n in G1.nodes()}\n",
    "    wl_colors2 = {n: G2.degree(n) for n in G2.nodes()}\n",
    "\n",
    "    # Draw WL test step 1\n",
    "    ax = axes[1, 0]\n",
    "    nx.draw(G1, pos1, with_labels=True,\n",
    "            node_color=[wl_colors1[n] for n in G1.nodes()],\n",
    "            cmap=plt.cm.viridis,\n",
    "            node_size=500, font_size=10, ax=ax)\n",
    "    ax.set_title(\"WL Test - Step 1: Node Degrees\")\n",
    "\n",
    "    # Draw inset with color mapping\n",
    "    inset_ax = fig.add_axes([0.3, 0.2, 0.1, 0.1])\n",
    "    inset_ax.bar(wl_colors1.keys(), wl_colors1.values(), color='lightblue')\n",
    "    inset_ax.set_xticks(list(wl_colors1.keys()))\n",
    "    inset_ax.set_title(\"Node Colors\", fontsize=8)\n",
    "\n",
    "    # Iteration 2: Update with neighbor colors\n",
    "    def wl_update(graph, colors):\n",
    "        new_colors = {}\n",
    "        for node in graph.nodes():\n",
    "            # Get colors of neighbors\n",
    "            neighbor_colors = [colors[neigh] for neigh in graph.neighbors(node)]\n",
    "            # Sort for canonical representation\n",
    "            neighbor_colors.sort()\n",
    "            # Create new color from node color and neighbor colors\n",
    "            new_colors[node] = (colors[node], tuple(neighbor_colors))\n",
    "        return new_colors\n",
    "\n",
    "    wl_colors1_step2 = wl_update(G1, wl_colors1)\n",
    "    wl_colors2_step2 = wl_update(G2, wl_colors2)\n",
    "\n",
    "    # Convert to integers for visualization\n",
    "    unique_colors = list(set(wl_colors1_step2.values()))\n",
    "    color_map = {color: i for i, color in enumerate(unique_colors)}\n",
    "\n",
    "    mapped_colors1 = {n: color_map[wl_colors1_step2[n]] for n in G1.nodes()}\n",
    "    mapped_colors2 = {n: color_map[wl_colors2_step2[n]] for n in G2.nodes()}\n",
    "\n",
    "    # Draw WL test step 2\n",
    "    ax = axes[1, 1]\n",
    "    nx.draw(G1, pos1, with_labels=True,\n",
    "            node_color=[mapped_colors1[n] for n in G1.nodes()],\n",
    "            cmap=plt.cm.viridis,\n",
    "            node_size=500, font_size=10, ax=ax)\n",
    "    ax.set_title(\"WL Test - Step 2: Neighbor Aggregation\")\n",
    "\n",
    "    # Create empty subplot for WL explanation\n",
    "    ax = axes[1, 2]\n",
    "    ax.axis('off')\n",
    "    ax.text(0.1, 0.5, \"Weisfeiler-Lehman (WL) Test:\\n\\n1. Initialize node colors (e.g., by degree)\\n2. Update each node's color based on its\\n   neighbors' colors\\n3. Repeat until stable or max iterations\\n4. If final color sets match, graphs may be\\n   isomorphic\\n\\nGIN is designed to match the power of the\\nWL test in distinguishing graph structures.\",\n",
    "            fontsize=12, ha='left', va='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the concept of graph isomorphism and the WL test\n",
    "visualize_isomorphism_concept()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EmV3xg1l8qG"
   },
   "source": [
    "## 4. The Theory Behind Graph Isomorphism Networks (GIN) <a name=\"theory-behind-gin\"></a>\n",
    "\n",
    "GIN was designed to have the same discriminative power as the Weisfeiler-Lehman (WL) graph isomorphism test, which is a powerful approach for distinguishing non-isomorphic graphs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Y1ukTLHklqSg",
    "outputId": "c97a599a-623f-462e-e307-d2a8bf0a2eda"
   },
   "outputs": [],
   "source": [
    "def explain_gin_vs_other_gnns():\n",
    "    \"\"\"Create a visual explanation of how GIN differs from other GNNs\"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    # Define example text\n",
    "    gin_explanation = \"\"\"\n",
    "    # Graph Isomorphism Networks (GIN)\n",
    "\n",
    "    ## Motivation\n",
    "\n",
    "    The key motivation behind GIN is to create a GNN with **maximum discriminative power** - able to\n",
    "    distinguish different graph structures as effectively as the Weisfeiler-Lehman (WL) isomorphism test.\n",
    "\n",
    "    ## Mathematical Foundation\n",
    "\n",
    "    The core GIN update equation:\n",
    "\n",
    "    h_v^{(k)} = MLP^{(k)}( (1 + ε^{(k)}) · h_v^{(k-1)} + ∑_{u ∈ N(v)} h_u^{(k-1)} )\n",
    "\n",
    "    Where:\n",
    "    - h_v^{(k)} is the feature of node v at layer k\n",
    "    - ε is a learnable parameter (or fixed to 0)\n",
    "    - MLP is a multi-layer perceptron\n",
    "\n",
    "    ## Key Differences from Other GNNs\n",
    "\n",
    "    ### 1. Compared to GCN:\n",
    "    - GCN uses normalized mean aggregation\n",
    "    - GIN uses sum aggregation with trainable self-loop weight\n",
    "    - GIN applies MLP after aggregation\n",
    "\n",
    "    ### 2. Compared to GAT:\n",
    "    - GAT uses weighted sum based on attention scores\n",
    "    - GIN treats all neighbors equally in aggregation\n",
    "    - GIN's power comes from expressive MLP after aggregation\n",
    "\n",
    "    ## Why GIN is Powerful for Chemistry\n",
    "\n",
    "    1. **Molecular Fingerprinting**: Can distinguish subtle structural differences\n",
    "    2. **Scaffold Recognition**: Can identify common substructures\n",
    "    3. **Isomer Discrimination**: Can tell apart molecules with the same formula but different structures\n",
    "    4. **Pharmacophore Identification**: Can learn to recognize functional groups\n",
    "    \"\"\"\n",
    "\n",
    "    plt.text(0.05, 0.5, gin_explanation, fontsize=14,\n",
    "            verticalalignment='center', horizontalalignment='left',\n",
    "            family='monospace', transform=plt.gca().transAxes)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "explain_gin_vs_other_gnns()\n",
    "\n",
    "def visualize_gnn_aggregation_comparison():\n",
    "    \"\"\"Compare aggregation mechanisms in different GNNs\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    # Create a simple graph for demonstration\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from([0, 1, 2, 3])\n",
    "    G.add_edges_from([(0, 1), (0, 2), (0, 3)])\n",
    "\n",
    "    # Define positions\n",
    "    pos = {0: (0.5, 0.5), 1: (0, 1), 2: (1, 1), 3: (0.5, 0)}\n",
    "\n",
    "    # GCN - Mean aggregation\n",
    "    ax = axes[0]\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=[0], node_color='red', node_size=700, ax=ax)\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=[1, 2, 3], node_color='lightblue', node_size=700, ax=ax)\n",
    "    nx.draw_networkx_edges(G, pos, width=2, alpha=0.7, ax=ax)\n",
    "    nx.draw_networkx_labels(G, pos, font_size=12, ax=ax)\n",
    "\n",
    "    # Add explanation text\n",
    "    ax.text(0.5, -0.1, r\"$h_v^{(k)} = \\sigma\\left(W \\cdot \\frac{1}{|N(v)|} \\sum_{u \\in N(v)} h_u^{(k-1)}\\right)$\",\n",
    "            ha='center', fontsize=12, transform=ax.transAxes)\n",
    "    ax.text(0.5, -0.2, \"GCN: Normalized Mean Aggregation\", ha='center', fontsize=14, transform=ax.transAxes)\n",
    "\n",
    "    # Add coefficients\n",
    "    ax.text(0.25, 0.75, \"1/3\", ha='center', va='center', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "    ax.text(0.75, 0.75, \"1/3\", ha='center', va='center', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "    ax.text(0.5, 0.25, \"1/3\", ha='center', va='center', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "    ax.set_title(\"GCN Aggregation\", fontsize=16)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # GAT - Attention-weighted aggregation\n",
    "    ax = axes[1]\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=[0], node_color='red', node_size=700, ax=ax)\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=[1, 2, 3], node_color='lightblue', node_size=700, ax=ax)\n",
    "\n",
    "    # Draw edges with different widths to represent attention\n",
    "    edge_widths = [4, 2, 1]\n",
    "    for i, (u, v) in enumerate([(0, 1), (0, 2), (0, 3)]):\n",
    "        nx.draw_networkx_edges(G, pos, edgelist=[(u, v)], width=edge_widths[i], alpha=0.7, ax=ax)\n",
    "\n",
    "    nx.draw_networkx_labels(G, pos, font_size=12, ax=ax)\n",
    "\n",
    "    # Add explanation text\n",
    "    ax.text(0.5, -0.1, r\"$h_v^{(k)} = \\sigma\\left(W \\cdot \\sum_{u \\in N(v)} \\alpha_{vu} \\cdot h_u^{(k-1)}\\right)$\",\n",
    "            ha='center', fontsize=12, transform=ax.transAxes)\n",
    "    ax.text(0.5, -0.2, \"GAT: Attention-weighted Aggregation\", ha='center', fontsize=14, transform=ax.transAxes)\n",
    "\n",
    "    # Add coefficients\n",
    "    ax.text(0.25, 0.75, \"0.6\", ha='center', va='center', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "    ax.text(0.75, 0.75, \"0.3\", ha='center', va='center', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "    ax.text(0.5, 0.25, \"0.1\", ha='center', va='center', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "    ax.set_title(\"GAT Aggregation\", fontsize=16)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # GIN - Sum aggregation with MLP\n",
    "    ax = axes[2]\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=[0], node_color='red', node_size=700, ax=ax)\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=[1, 2, 3], node_color='lightblue', node_size=700, ax=ax)\n",
    "    nx.draw_networkx_edges(G, pos, width=3, alpha=0.7, ax=ax)\n",
    "    nx.draw_networkx_labels(G, pos, font_size=12, ax=ax)\n",
    "\n",
    "    # Add explanation text\n",
    "    ax.text(0.5, -0.1, r\"$h_v^{(k)} = \\text{MLP}^{(k)}\\left((1 + \\epsilon) \\cdot h_v^{(k-1)} + \\sum_{u \\in N(v)} h_u^{(k-1)}\\right)$\",\n",
    "            ha='center', fontsize=12, transform=ax.transAxes)\n",
    "    ax.text(0.5, -0.2, \"GIN: Sum Aggregation with MLP\", ha='center', fontsize=14, transform=ax.transAxes)\n",
    "\n",
    "    # Add coefficients and highlight MLP\n",
    "    ax.text(0.25, 0.75, \"1.0\", ha='center', va='center', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "    ax.text(0.75, 0.75, \"1.0\", ha='center', va='center', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "    ax.text(0.5, 0.25, \"1.0\", ha='center', va='center', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "    # Add self-loop\n",
    "    circle = plt.Circle((0.5, 0.5), 0.15, fill=False, linestyle='--', color='green', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(0.7, 0.5, f\"(1+ε)\", ha='center', va='center', fontsize=12, color='green',\n",
    "            bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "    # Add MLP\n",
    "    rect = plt.Rectangle((0.2, -0.05), 0.6, 0.1, fill=True, color='lightyellow', alpha=0.7, transform=ax.transAxes)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(0.5, 0.0, \"MLP\", ha='center', va='center', fontsize=12, transform=ax.transAxes)\n",
    "\n",
    "    ax.set_title(\"GIN Aggregation\", fontsize=16)\n",
    "    ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.25)\n",
    "    plt.show()\n",
    "\n",
    "# Compare the aggregation mechanisms of different GNNs\n",
    "visualize_gnn_aggregation_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9R3HWwVM1j4M"
   },
   "source": [
    "## 5. Implementing a GIN Model for Molecules <a name=\"implementing-a-gin-model-for-molecules\"></a>\n",
    "\n",
    "Now, let's implement a GIN model for molecular property prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "k3sYKQLgmBUP",
    "outputId": "3d978bd3-5b27-4218-97db-9449a3f894f5"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer Perceptron (MLP) implementation for GIN.\n",
    "    This MLP is used both in the GINConv layers and for final predictions.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input features\n",
    "        hidden_channels (int): Number of hidden features in intermediate layers\n",
    "        out_channels (int): Number of output features\n",
    "        num_layers (int, optional): Number of layers in the MLP. Default: 2\n",
    "\n",
    "    The MLP consists of:\n",
    "    - Linear layers with ReLU activation\n",
    "    - Batch normalization (except for single layer case)\n",
    "    - No activation on the final layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.lins = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "\n",
    "        if num_layers == 1:\n",
    "            # If single layer, don't use batch norm\n",
    "            self.lins.append(nn.Linear(in_channels, out_channels))\n",
    "        else:\n",
    "            self.lins.append(nn.Linear(in_channels, hidden_channels))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "            for _ in range(num_layers - 2):\n",
    "                self.lins.append(nn.Linear(hidden_channels, hidden_channels))\n",
    "                self.batch_norms.append(nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "            self.lins.append(nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, lin in enumerate(self.lins[:-1]):\n",
    "            x = lin(x)\n",
    "            x = self.batch_norms[i](x)\n",
    "            x = F.relu(x)\n",
    "        x = self.lins[-1](x)\n",
    "        return x\n",
    "\n",
    "class GIN(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Isomorphism Network (GIN) implementation.\n",
    "    GIN is a powerful graph neural network architecture that can distinguish between different graph structures.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input node features\n",
    "        hidden_channels (int): Number of hidden features\n",
    "        out_channels (int): Number of output features\n",
    "        num_layers (int, optional): Number of GIN layers. Default: 3\n",
    "        dropout (float, optional): Dropout probability. Default: 0.5\n",
    "        epsilon (float, optional): Initial value for learnable epsilon in GINConv. Default: 0\n",
    "\n",
    "    The architecture consists of:\n",
    "    1. Initial node feature projection\n",
    "    2. Multiple GIN layers with MLPs\n",
    "    3. Global mean pooling\n",
    "    4. Final prediction MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, dropout=0.5, epsilon=0):\n",
    "        super(GIN, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Initial projection of node features\n",
    "        self.node_encoder = nn.Linear(in_channels, hidden_channels)\n",
    "\n",
    "        # GIN convolution layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            mlp = MLP(hidden_channels, hidden_channels, hidden_channels)\n",
    "            # epsilon can be learned or fixed\n",
    "            self.convs.append(GINConv(mlp, train_eps=True, eps=epsilon))\n",
    "\n",
    "        # Batch normalization layers\n",
    "        self.batch_norms = nn.ModuleList([nn.BatchNorm1d(hidden_channels) for _ in range(num_layers)])\n",
    "\n",
    "        # Prediction MLP\n",
    "        self.mlp = MLP(hidden_channels, hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        \"\"\"\n",
    "        Forward pass of the GIN model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input node features\n",
    "            edge_index (torch.Tensor): Edge index tensor\n",
    "            batch (torch.Tensor): Batch tensor indicating the graph structure\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, out_channels)\n",
    "        \"\"\"\n",
    "        # Initial embedding\n",
    "        x = self.node_encoder(x.float())\n",
    "\n",
    "        # Store representations from each layer for readout\n",
    "        xs = []\n",
    "\n",
    "        # GIN layers\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.batch_norms[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            xs.append(x)\n",
    "\n",
    "        # Global pooling (mean)\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # Final prediction\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_embeddings(self, x, edge_index, batch):\n",
    "        \"\"\"\n",
    "        Get node embeddings from all layers of the GIN model.\n",
    "        This is useful for visualization or analysis of learned representations.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Node features of shape [num_nodes, in_channels]\n",
    "            edge_index (Tensor): Graph connectivity in COO format of shape [2, num_edges]\n",
    "            batch (Tensor): Batch assignment vector of shape [num_nodes]\n",
    "\n",
    "        Returns:\n",
    "            List[Tensor]: List of node embeddings from each layer\n",
    "        \"\"\"\n",
    "        # Initial embedding\n",
    "        x = self.node_encoder(x.float())\n",
    "\n",
    "        # Store representations from each layer\n",
    "        xs = []\n",
    "\n",
    "        # GIN layers (without dropout for inference)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.batch_norms[i](x)\n",
    "            x = F.relu(x)\n",
    "            xs.append(x)\n",
    "\n",
    "        return xs  # Return embeddings from all layers\n",
    "\n",
    "# Visualize GIN architecture\n",
    "def visualize_gin_architecture():\n",
    "    \"\"\"\n",
    "    Create a visual representation of the GIN architecture.\n",
    "    This visualization shows:\n",
    "    - The overall flow of data through the network\n",
    "    - The structure of each GIN layer\n",
    "    - The mathematical formulation of the GIN convolution operation\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "    # Define layer positions\n",
    "    layer_x = [1, 3, 5, 7, 9, 11]\n",
    "    layer_width = 1.5\n",
    "    layer_height = 3\n",
    "\n",
    "    # Define labels\n",
    "    layer_labels = [\"Input\", \"Node\\nEncoder\", \"GIN Layer 1\", \"GIN Layer 2\", \"GIN Layer 3\", \"Output MLP\"]\n",
    "\n",
    "    # Draw layers\n",
    "    for i, (x, label) in enumerate(zip(layer_x, layer_labels)):\n",
    "        if i == 0:  # Input\n",
    "            color = 'lightblue'\n",
    "        elif i == len(layer_x) - 1:  # Output\n",
    "            color = 'lightgreen'\n",
    "        else:  # Hidden layers\n",
    "            color = 'lightsalmon'\n",
    "\n",
    "        rect = plt.Rectangle((x - layer_width/2, 0), layer_width, layer_height,\n",
    "                           facecolor=color, edgecolor='black', alpha=0.7)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x, layer_height/2, label, ha='center', va='center', fontsize=12)\n",
    "\n",
    "    # Draw arrows connecting layers\n",
    "    for i in range(len(layer_x) - 1):\n",
    "        ax.arrow(layer_x[i] + layer_width/2, layer_height/2,\n",
    "                layer_x[i+1] - layer_x[i] - layer_width, 0,\n",
    "                head_width=0.2, head_length=0.2, fc='black', ec='black')\n",
    "\n",
    "    # Add details for GIN layers\n",
    "    for i in range(2, 5):\n",
    "        # Add detail box below\n",
    "        detail_box = plt.Rectangle((layer_x[i] - layer_width/2, -2.5), layer_width, 2,\n",
    "                                 facecolor='lightyellow', edgecolor='black', alpha=0.9)\n",
    "        ax.add_patch(detail_box)\n",
    "\n",
    "        # Add details text\n",
    "        details = \"GINConv Operation:\\n\\n\" + \\\n",
    "                 r\"$h_v^{(k)} = \\text{MLP}^{(k)}((1 + \\epsilon) \\cdot h_v^{(k-1)} + \\sum_{u \\in N(v)} h_u^{(k-1)})$\"\n",
    "        ax.text(layer_x[i], -1.5, details, ha='center', va='center', fontsize=10)\n",
    "\n",
    "        # Add connecting line\n",
    "        ax.plot([layer_x[i], layer_x[i]], [0, -0.5], 'k--', alpha=0.5)\n",
    "\n",
    "    # Set axis limits\n",
    "    ax.set_xlim(0, 12)\n",
    "    ax.set_ylim(-3, 4)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Add title\n",
    "    ax.set_title(\"Graph Isomorphism Network (GIN) Architecture\", fontsize=16)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_gin_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_E3XZQE2Hvb"
   },
   "source": [
    "## 6. Training GIN for Molecular Property Prediction <a name=\"training-gin-for-molecular-property-prediction\"></a>\n",
    "\n",
    "Let's train a GIN model and compare its performance with GCN and GAT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SoGKS5-51pTk",
    "outputId": "3486124f-2fd6-4826-d5b8-3ef9cff1fd17"
   },
   "outputs": [],
   "source": [
    "# Load a dataset from MoleculeNet (binary classification task)\n",
    "print(\"Loading BBBP dataset (Blood-Brain Barrier Penetration)...\")\n",
    "dataset = MoleculeNet(root='data', name='BBBP')\n",
    "print(f\"Dataset loaded: {len(dataset)} molecules\")\n",
    "\n",
    "# Split the dataset\n",
    "torch.manual_seed(42)\n",
    "indices = torch.randperm(len(dataset))\n",
    "train_idx = indices[:int(0.8 * len(dataset))]\n",
    "val_idx = indices[int(0.8 * len(dataset)):int(0.9 * len(dataset))]\n",
    "test_idx = indices[int(0.9 * len(dataset)):]\n",
    "\n",
    "train_dataset = dataset[train_idx]\n",
    "val_dataset = dataset[val_idx]\n",
    "test_dataset = dataset[test_idx]\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Validation: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "# Create data loaders\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# For comparison, let's implement GCN and GAT models as well\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, dropout=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Initial projection of node features\n",
    "        self.node_encoder = nn.Linear(in_channels, hidden_channels)\n",
    "\n",
    "        # GCN convolution layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "\n",
    "        # Batch normalization layers\n",
    "        self.batch_norms = nn.ModuleList([nn.BatchNorm1d(hidden_channels) for _ in range(num_layers)])\n",
    "\n",
    "        # Prediction MLP\n",
    "        self.mlp = MLP(hidden_channels, hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Initial embedding\n",
    "        x = self.node_encoder(x.float())\n",
    "\n",
    "        # GCN layers\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.batch_norms[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Global pooling (mean)\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # Final prediction\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_embeddings(self, x, edge_index, batch):\n",
    "        \"\"\"Get node embeddings from the GCN model\"\"\"\n",
    "        # Initial embedding\n",
    "        x = self.node_encoder(x.float())\n",
    "\n",
    "        # Store representations from each layer\n",
    "        xs = []\n",
    "\n",
    "        # GCN layers (without dropout for inference)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.batch_norms[i](x)\n",
    "            x = F.relu(x)\n",
    "            xs.append(x)\n",
    "\n",
    "        return xs  # Return embeddings from all layers\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, heads=4, dropout=0.5):\n",
    "        super(GAT, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Initial projection of node features\n",
    "        self.node_encoder = nn.Linear(in_channels, hidden_channels)\n",
    "\n",
    "        # GAT convolution layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            # Last layer has 1 head, others have 'heads' number of heads\n",
    "            heads_in_layer = 1 if i == num_layers - 1 else heads\n",
    "            heads_out_channels = hidden_channels // heads if i < num_layers - 1 else hidden_channels\n",
    "            self.convs.append(GATConv(hidden_channels, heads_out_channels, heads=heads_in_layer, dropout=dropout))\n",
    "\n",
    "        # Batch normalization layers\n",
    "        self.batch_norms = nn.ModuleList([nn.BatchNorm1d(hidden_channels) for _ in range(num_layers)])\n",
    "\n",
    "        # Prediction MLP\n",
    "        self.mlp = MLP(hidden_channels, hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Initial embedding\n",
    "        x = self.node_encoder(x.float())\n",
    "\n",
    "        # GAT layers\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.batch_norms[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Global pooling (mean)\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # Final prediction\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_embeddings(self, x, edge_index, batch):\n",
    "        \"\"\"Get node embeddings from the GAT model\"\"\"\n",
    "        # Initial embedding\n",
    "        x = self.node_encoder(x.float())\n",
    "\n",
    "        # Store representations from each layer\n",
    "        xs = []\n",
    "\n",
    "        # GAT layers (without dropout for inference)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.batch_norms[i](x)\n",
    "            x = F.relu(x)\n",
    "            xs.append(x)\n",
    "\n",
    "        return xs  # Return embeddings from all layers\n",
    "\n",
    "# Train and evaluate model function\n",
    "def train_and_evaluate(model, optimizer, train_loader, val_loader, test_loader, device, epochs=100):\n",
    "    \"\"\"Train and evaluate a model for molecular property prediction\"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_aucs = []\n",
    "    best_val_auc = 0\n",
    "    best_model = None\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for data in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            data = data.to(device)\n",
    "\n",
    "            out = model(data.x.float(), data.edge_index, data.batch)\n",
    "\n",
    "            # Ensure output and target have the same batch size\n",
    "            if out.size(0) != data.y.size(0):\n",
    "                # If sizes don't match, use the minimum size\n",
    "                min_size = min(out.size(0), data.y.size(0))\n",
    "                out = out[:min_size]\n",
    "                data.y = data.y[:min_size]\n",
    "\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "            total_samples += data.num_graphs\n",
    "\n",
    "        avg_loss = total_loss / total_samples\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_samples = 0\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                data = data.to(device)\n",
    "                out = model(data.x.float(), data.edge_index, data.batch)\n",
    "\n",
    "                # Ensure output and target have the same batch size\n",
    "                if out.size(0) != data.y.size(0):\n",
    "                    min_size = min(out.size(0), data.y.size(0))\n",
    "                    out = out[:min_size]\n",
    "                    data.y = data.y[:min_size]\n",
    "\n",
    "                loss = criterion(out, data.y)\n",
    "                val_loss += loss.item() * data.num_graphs\n",
    "                val_samples += data.num_graphs\n",
    "\n",
    "                y_true.append(data.y.cpu().numpy())\n",
    "                y_pred.append(torch.sigmoid(out).cpu().numpy())\n",
    "\n",
    "        avg_val_loss = val_loss / val_samples\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Calculate validation AUC\n",
    "        y_true = np.concatenate(y_true)\n",
    "        y_pred = np.concatenate(y_pred)\n",
    "        val_auc = roc_auc_score(y_true, y_pred)\n",
    "        val_aucs.append(val_auc)\n",
    "\n",
    "        # Save best model\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            best_model = model.state_dict().copy()\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch:03d}: Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "    # Load best model for evaluation\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    # Test evaluation\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x.float(), data.edge_index, data.batch)\n",
    "\n",
    "            # Ensure output and target have the same batch size\n",
    "            if out.size(0) != data.y.size(0):\n",
    "                min_size = min(out.size(0), data.y.size(0))\n",
    "                out = out[:min_size]\n",
    "                data.y = data.y[:min_size]\n",
    "\n",
    "            y_true.append(data.y.cpu().numpy())\n",
    "            y_pred.append(torch.sigmoid(out).cpu().numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    test_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_aucs': val_aucs,\n",
    "        'best_val_auc': best_val_auc,\n",
    "        'test_auc': test_auc,\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred\n",
    "    }\n",
    "\n",
    "# Compare GCN, GAT, and GIN\n",
    "import copy\n",
    "\n",
    "def compare_gnn_architectures():\n",
    "    \"\"\"Train and compare different GNN architectures\"\"\"\n",
    "    # Get input dimension\n",
    "    sample = dataset[0]\n",
    "    in_channels = sample.x.shape[1]\n",
    "\n",
    "    # Define hyperparameters\n",
    "    hidden_channels = 64\n",
    "    out_channels = 1  # Binary classification\n",
    "    num_layers = 3\n",
    "    dropout = 0.5\n",
    "    lr = 0.001\n",
    "    weight_decay = 1e-4\n",
    "    epochs = 100\n",
    "\n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'GCN': GCN(in_channels, hidden_channels, out_channels, num_layers, dropout).to(device),\n",
    "        'GAT': GAT(in_channels, hidden_channels, out_channels, num_layers, heads=4, dropout=dropout).to(device),\n",
    "        'GIN': GIN(in_channels, hidden_channels, out_channels, num_layers, dropout=dropout, epsilon=0).to(device)\n",
    "    }\n",
    "\n",
    "    # Train and evaluate models\n",
    "    results = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name} model...\")\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        results[name] = train_and_evaluate(model, optimizer, train_loader, val_loader, test_loader, device, epochs)\n",
    "        print(f\"{name} Test AUC: {results[name]['test_auc']:.4f}\")\n",
    "\n",
    "    return results, models\n",
    "\n",
    "# Run comparison\n",
    "results, models = compare_gnn_architectures()\n",
    "\n",
    "# Visualize training curves\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(1, 3, 1)\n",
    "for name, result in results.items():\n",
    "    plt.plot(result['train_losses'], label=name)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot validation loss\n",
    "plt.subplot(1, 3, 2)\n",
    "for name, result in results.items():\n",
    "    plt.plot(result['val_losses'], label=name)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot validation AUC\n",
    "plt.subplot(1, 3, 3)\n",
    "for name, result in results.items():\n",
    "    plt.plot(result['val_aucs'], label=name)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation AUC')\n",
    "plt.title('Validation AUC Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot test AUC comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "names = list(results.keys())\n",
    "aucs = [result['test_auc'] for result in results.values()]\n",
    "\n",
    "bars = plt.bar(names, aucs, color=['steelblue', 'darkorange', 'forestgreen'])\n",
    "plt.xlabel('Model Architecture')\n",
    "plt.ylabel('Test AUC')\n",
    "plt.title('Test Performance (AUC) Comparison')\n",
    "plt.ylim(0.5, 1.0)  # AUC ranges from 0.5 to 1.0\n",
    "\n",
    "# Add value labels\n",
    "for bar, auc in zip(bars, aucs):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, f'{auc:.4f}',\n",
    "            ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "plt.grid(True, axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58qNEFT23BvH"
   },
   "source": [
    "## 7. GIN for Molecular Fingerprinting <a name=\"gin-for-molecular-fingerprinting\"></a>\n",
    "\n",
    "One of the key strengths of GIN is its ability to generate effective molecular fingerprints - numerical representations of molecules that capture their structural features. Let's explore this application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9p5ab25-2SPi",
    "outputId": "65e46b80-cda2-4791-f22b-294b5a891d21"
   },
   "outputs": [],
   "source": [
    "def generate_molecule_embeddings(model, dataset, device, batch_size=32):\n",
    "    \"\"\"Generate molecular embeddings using the trained GIN model\"\"\"\n",
    "    model.eval()\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    smiles_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            try:\n",
    "                data = data.to(device)\n",
    "                node_embeddings = model.get_embeddings(data.x.float(), data.edge_index, data.batch)\n",
    "                last_layer_embedding = global_mean_pool(node_embeddings[-1], data.batch)\n",
    "\n",
    "                embeddings.append(last_layer_embedding.cpu().numpy())\n",
    "                labels.append(data.y.cpu().numpy())\n",
    "\n",
    "                if hasattr(data, 'smiles'):\n",
    "                    smiles_list.extend(data.smiles)\n",
    "\n",
    "                del data\n",
    "                torch.cuda.empty_cache()  # If using CUDA\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    if not embeddings:\n",
    "        raise ValueError(\"No embeddings were generated\")\n",
    "\n",
    "    embeddings = np.concatenate(embeddings, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "    return embeddings, labels, smiles_list\n",
    "\n",
    "# Generate embeddings using the trained GIN model\n",
    "gin_model = models['GIN']\n",
    "gin_embeddings, gin_labels, _ = generate_molecule_embeddings(gin_model, test_dataset, device)\n",
    "\n",
    "# Visualize the embeddings using t-SNE\n",
    "def visualize_molecular_embeddings(embeddings, labels):\n",
    "    \"\"\"Visualize molecular embeddings using t-SNE\"\"\"\n",
    "    # Reduce embedding dimensionality for visualization\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings)-1))\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "    # Create scatter plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],\n",
    "                        c=labels.squeeze(), cmap='coolwarm', alpha=0.7, s=100)\n",
    "\n",
    "    plt.colorbar(scatter, label='Property Value (0/1)')\n",
    "    plt.title('t-SNE Visualization of GIN Molecular Embeddings', fontsize=16)\n",
    "    plt.xlabel('t-SNE Component 1', fontsize=14)\n",
    "    plt.ylabel('t-SNE Component 2', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize the GIN embeddings\n",
    "visualize_molecular_embeddings(gin_embeddings, gin_labels)\n",
    "\n",
    "# Compare with embeddings from other models\n",
    "gcn_model = models['GCN']\n",
    "gat_model = models['GAT']\n",
    "\n",
    "gcn_embeddings, gcn_labels, _ = generate_molecule_embeddings(gcn_model, test_dataset, device)\n",
    "gat_embeddings, gat_labels, _ = generate_molecule_embeddings(gat_model, test_dataset, device)\n",
    "\n",
    "\n",
    "# Calculate embedding similarity\n",
    "def calculate_similarity_matrix(embeddings, normalize=True):\n",
    "    \"\"\"Calculate cosine similarity matrix between embeddings\"\"\"\n",
    "    # Normalize if requested\n",
    "    if normalize:\n",
    "        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        embeddings = embeddings / norms\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity_matrix = np.dot(embeddings, embeddings.T)\n",
    "    return similarity_matrix\n",
    "\n",
    "# Compare similarity distributions\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Process each model's embeddings\n",
    "models_data = [\n",
    "    ('GCN', gcn_embeddings),\n",
    "    ('GAT', gat_embeddings),\n",
    "    ('GIN', gin_embeddings)\n",
    "]\n",
    "\n",
    "for i, (name, embeddings) in enumerate(models_data):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "\n",
    "    # Split by class\n",
    "    class_0_idx = np.where(gin_labels.squeeze() < 0.5)[0]\n",
    "    class_1_idx = np.where(gin_labels.squeeze() >= 0.5)[0]\n",
    "\n",
    "    # Calculate similarity matrices\n",
    "    sim_matrix = calculate_similarity_matrix(embeddings)\n",
    "\n",
    "    # Extract similarities between same class and different class\n",
    "    same_class_sim = []\n",
    "    diff_class_sim = []\n",
    "\n",
    "    for j in range(len(embeddings)):\n",
    "        for k in range(j+1, len(embeddings)):\n",
    "            if (j in class_0_idx and k in class_0_idx) or (j in class_1_idx and k in class_1_idx):\n",
    "                same_class_sim.append(sim_matrix[j, k])\n",
    "            else:\n",
    "                diff_class_sim.append(sim_matrix[j, k])\n",
    "\n",
    "    # Plot distributions\n",
    "    plt.hist(same_class_sim, bins=20, alpha=0.5, label='Same Class', density=True)\n",
    "    plt.hist(diff_class_sim, bins=20, alpha=0.5, label='Different Class', density=True)\n",
    "\n",
    "    plt.xlabel('Cosine Similarity')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f'{name} Embedding Similarities')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def visualize_molecular_similarity(test_dataset, model, device, num_molecules=5, top_k=3):\n",
    "    \"\"\"Visualize molecular similarities based on GIN embeddings\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    try:\n",
    "        subset_indices = np.random.choice(len(test_dataset), num_molecules, replace=False)\n",
    "        query_molecules = [test_dataset[i] for i in subset_indices]\n",
    "\n",
    "        all_embeddings, _, all_smiles = generate_molecule_embeddings(model, test_dataset, device)\n",
    "\n",
    "        fig, axes = plt.subplots(num_molecules, top_k + 1, figsize=(4 * (top_k + 1), 4 * num_molecules))\n",
    "\n",
    "        for i, query_mol in enumerate(query_molecules):\n",
    "            try:\n",
    "                query_data = query_mol.to(device)\n",
    "                with torch.no_grad():\n",
    "                    # Remove the unsqueeze operations that were adding extra dimensions\n",
    "                    node_embeddings = model.get_embeddings(\n",
    "                        query_data.x.float(),  # No need for unsqueeze\n",
    "                        query_data.edge_index,  # No need for unsqueeze\n",
    "                        torch.zeros(query_data.x.size(0), dtype=torch.long, device=device)\n",
    "                    )\n",
    "                    query_embedding = global_mean_pool(\n",
    "                        node_embeddings[-1],\n",
    "                        torch.zeros(query_data.x.size(0), dtype=torch.long, device=device)\n",
    "                    )\n",
    "                    query_embedding = query_embedding.cpu().numpy()\n",
    "\n",
    "                similarities = np.dot(all_embeddings, query_embedding.T).squeeze()\n",
    "                sorted_indices = np.argsort(similarities)[::-1]\n",
    "\n",
    "                if hasattr(query_mol, 'smiles'):\n",
    "                    query_idx = all_smiles.index(query_mol.smiles) if query_mol.smiles in all_smiles else -1\n",
    "                    if query_idx >= 0 and query_idx in sorted_indices:\n",
    "                        sorted_indices = np.delete(sorted_indices, np.where(sorted_indices == query_idx))\n",
    "\n",
    "                top_k_indices = sorted_indices[:top_k]\n",
    "\n",
    "                if hasattr(query_mol, 'smiles'):\n",
    "                    try:\n",
    "                        mol = Chem.MolFromSmiles(query_mol.smiles)\n",
    "                        if mol is not None:\n",
    "                            drawer = rdMolDraw2D.MolDraw2DCairo(300, 300)\n",
    "                            drawer.DrawMolecule(mol)\n",
    "                            drawer.FinishDrawing()\n",
    "                            img = drawer.GetDrawingText()\n",
    "                            query_img = Image.open(io.BytesIO(img))\n",
    "                            axes[i, 0].imshow(query_img)\n",
    "                            axes[i, 0].set_title(f\"Query: {query_mol.smiles[:20]}...\")\n",
    "                            axes[i, 0].axis('off')\n",
    "\n",
    "                            for j, idx in enumerate(top_k_indices):\n",
    "                                if idx < len(test_dataset) and hasattr(test_dataset[idx], 'smiles'):\n",
    "                                    similar_smiles = test_dataset[idx].smiles\n",
    "                                    similar_mol = Chem.MolFromSmiles(similar_smiles)\n",
    "                                    if similar_mol:\n",
    "                                        drawer = rdMolDraw2D.MolDraw2DCairo(300, 300)\n",
    "                                        drawer.DrawMolecule(similar_mol)\n",
    "                                        drawer.FinishDrawing()\n",
    "                                        img = drawer.GetDrawingText()\n",
    "                                        similar_img = Image.open(io.BytesIO(img))\n",
    "                                        axes[i, j+1].imshow(similar_img)\n",
    "                                        axes[i, j+1].set_title(f\"Similar #{j+1}\\nSim: {similarities[idx]:.3f}\")\n",
    "                                        axes[i, j+1].axis('off')\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing molecule {i}: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing query molecule {i}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in visualization: {str(e)}\")\n",
    "\n",
    "# Visualize molecular similarities for the GIN model\n",
    "visualize_molecular_similarity(test_dataset, models[\"GIN\"], device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfPhy_6g4NLs"
   },
   "source": [
    "## 8. Analyzing Isomorphic vs. Non-Isomorphic Molecule Pairs <a name=\"analyzing-isomorphic-vs-non-isomorphic-molecule-pairs\"></a>\n",
    "\n",
    "Let's demonstrate the discriminative power of GIN on isomorphic and non-isomorphic molecule pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4zUwFJQT4Nss",
    "outputId": "5347634b-223e-47f2-a4e7-f108dd64bc78"
   },
   "outputs": [],
   "source": [
    "def analyze_isomorphism_discrimination(model, device):\n",
    "    \"\"\"Analyze how well the model distinguishes between isomorphic and non-isomorphic molecules\"\"\"\n",
    "    # Isomorphic SMILES pairs (same molecule, different representations)\n",
    "    isomorphic_pairs = [\n",
    "        # Ethanol representations\n",
    "        (\"CCO\", \"OCC\"),\n",
    "        # Toluene representations\n",
    "        (\"Cc1ccccc1\", \"C1=CC=C(C)C=C1\"),\n",
    "        # Benzene representations\n",
    "        (\"c1ccccc1\", \"C1=CC=CC=C1\"),\n",
    "        # Acetone representations\n",
    "        (\"CC(=O)C\", \"CC(C)=O\"),\n",
    "        # Methoxymethane representations\n",
    "        (\"COC\", \"O(C)C\")\n",
    "    ]\n",
    "\n",
    "    # Non-isomorphic but similar SMILES pairs\n",
    "    non_isomorphic_pairs = [\n",
    "        # Ethanol vs. Dimethyl ether (same formula, different structure)\n",
    "        (\"CCO\", \"COC\"),\n",
    "        # Propane vs. Cyclopropane\n",
    "        (\"CCC\", \"C1CC1\"),\n",
    "        # Benzene vs. Cyclohexane\n",
    "        (\"c1ccccc1\", \"C1CCCCC1\"),\n",
    "        # Acetone vs. Propanal\n",
    "        (\"CC(=O)C\", \"CCC=O\"),\n",
    "        # Butane vs. Isobutane\n",
    "        (\"CCCC\", \"CC(C)C\")\n",
    "    ]\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    results = {\n",
    "        'isomorphic': [],\n",
    "        'non_isomorphic': []\n",
    "    }\n",
    "\n",
    "    def get_embedding(smiles):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None\n",
    "\n",
    "        data = mol_to_graph(smiles)\n",
    "        data = data.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            node_embeddings = model.get_embeddings(data.x, data.edge_index, torch.zeros(data.x.size(0), dtype=torch.long, device=device))\n",
    "            graph_embedding = global_mean_pool(node_embeddings[-1], torch.zeros(data.x.size(0), dtype=torch.long, device=device))\n",
    "\n",
    "        return graph_embedding.cpu().numpy()\n",
    "\n",
    "    # Process isomorphic pairs\n",
    "    for smiles1, smiles2 in isomorphic_pairs:\n",
    "        emb1 = get_embedding(smiles1)\n",
    "        emb2 = get_embedding(smiles2)\n",
    "\n",
    "        if emb1 is not None and emb2 is not None:\n",
    "            # Calculate cosine similarity\n",
    "            similarity = np.dot(emb1, emb2.T) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "            results['isomorphic'].append({\n",
    "                'smiles1': smiles1,\n",
    "                'smiles2': smiles2,\n",
    "                'similarity': float(similarity)\n",
    "            })\n",
    "\n",
    "    # Process non-isomorphic pairs\n",
    "    for smiles1, smiles2 in non_isomorphic_pairs:\n",
    "        emb1 = get_embedding(smiles1)\n",
    "        emb2 = get_embedding(smiles2)\n",
    "\n",
    "        if emb1 is not None and emb2 is not None:\n",
    "            # Calculate cosine similarity\n",
    "            similarity = np.dot(emb1, emb2.T) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "            results['non_isomorphic'].append({\n",
    "                'smiles1': smiles1,\n",
    "                'smiles2': smiles2,\n",
    "                'similarity': float(similarity)\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Analyze discrimination for all models\n",
    "gin_results = analyze_isomorphism_discrimination(gin_model, device)\n",
    "gcn_results = analyze_isomorphism_discrimination(gcn_model, device)\n",
    "gat_results = analyze_isomorphism_discrimination(gat_model, device)\n",
    "\n",
    "# Function to visualize the results\n",
    "def visualize_isomorphism_results(gin_results, gcn_results, gat_results):\n",
    "    \"\"\"Visualize the isomorphism test results for different models\"\"\"\n",
    "    models = ['GIN', 'GCN', 'GAT']\n",
    "    iso_similarities = [\n",
    "        [r['similarity'] for r in gin_results['isomorphic']],\n",
    "        [r['similarity'] for r in gcn_results['isomorphic']],\n",
    "        [r['similarity'] for r in gat_results['isomorphic']]\n",
    "    ]\n",
    "\n",
    "    non_iso_similarities = [\n",
    "        [r['similarity'] for r in gin_results['non_isomorphic']],\n",
    "        [r['similarity'] for r in gcn_results['non_isomorphic']],\n",
    "        [r['similarity'] for r in gat_results['non_isomorphic']]\n",
    "    ]\n",
    "\n",
    "    # Calculate mean similarities\n",
    "    iso_means = [np.mean(sims) for sims in iso_similarities]\n",
    "    non_iso_means = [np.mean(sims) for sims in non_iso_similarities]\n",
    "\n",
    "    # Calculate the discrimination gap (difference between isomorphic and non-isomorphic similarities)\n",
    "    discrimination_gaps = [iso - non for iso, non in zip(iso_means, non_iso_means)]\n",
    "\n",
    "    # Plot mean similarities\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.bar(x - width/2, iso_means, width, label='Isomorphic Pairs', color='blue', alpha=0.7)\n",
    "    plt.bar(x + width/2, non_iso_means, width, label='Non-isomorphic Pairs', color='orange', alpha=0.7)\n",
    "\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Mean Cosine Similarity')\n",
    "    plt.title('Embedding Similarity for Molecule Pairs')\n",
    "    plt.xticks(x, models)\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot discrimination gap\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(models, discrimination_gaps, color='green', alpha=0.7)\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Discrimination Gap')\n",
    "    plt.title('Isomorphism Discrimination Power')\n",
    "    plt.ylim(0, 1.0)\n",
    "\n",
    "    # Add value labels\n",
    "    for i, v in enumerate(discrimination_gaps):\n",
    "        plt.text(i, v + 0.02, f'{v:.3f}', ha='center')\n",
    "\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot detailed similarity matrix for specific pairs\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    for i, model_name in enumerate(models):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "\n",
    "        # Combine results\n",
    "        all_results = []\n",
    "        model_results = [gin_results, gcn_results, gat_results][i]\n",
    "\n",
    "        for j, pair in enumerate(model_results['isomorphic']):\n",
    "            all_results.append({\n",
    "                'x': j,\n",
    "                'y': 0,\n",
    "                'similarity': pair['similarity'],\n",
    "                'smiles1': pair['smiles1'],\n",
    "                'smiles2': pair['smiles2'],\n",
    "                'type': 'Isomorphic'\n",
    "            })\n",
    "\n",
    "        for j, pair in enumerate(model_results['non_isomorphic']):\n",
    "            all_results.append({\n",
    "                'x': j,\n",
    "                'y': 1,\n",
    "                'similarity': pair['similarity'],\n",
    "                'smiles1': pair['smiles1'],\n",
    "                'smiles2': pair['smiles2'],\n",
    "                'type': 'Non-isomorphic'\n",
    "            })\n",
    "\n",
    "        # Create similarity matrix\n",
    "        matrix = np.zeros((2, 5))\n",
    "        for r in all_results:\n",
    "            matrix[r['y'], r['x']] = r['similarity']\n",
    "\n",
    "        # Plot matrix\n",
    "        im = plt.imshow(matrix, cmap='viridis', vmin=0, vmax=1)\n",
    "        plt.colorbar(im, label='Cosine Similarity')\n",
    "\n",
    "        plt.title(f\"{model_name} Molecule Pair Similarities\")\n",
    "        plt.xlabel('Pair Index')\n",
    "        plt.yticks([0, 1], ['Isomorphic', 'Non-isomorphic'])\n",
    "\n",
    "        # Add similarity values\n",
    "        for r in all_results:\n",
    "            plt.text(r['x'], r['y'], f\"{r['similarity']:.3f}\", ha='center', va='center',\n",
    "                   color='white' if r['similarity'] > 0.5 else 'black')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the results\n",
    "visualize_isomorphism_results(gin_results, gcn_results, gat_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sy0b5l_z4bqK"
   },
   "source": [
    "## 9. Interpreting GIN Embeddings for Chemical Insights <a name=\"interpreting-gin-embeddings-for-chemical-insights\"></a>\n",
    "\n",
    "Let's analyze what chemical information GIN embeddings are capturing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qE3EGGlT4cD7",
    "outputId": "ba74998c-e28e-49ff-e0a3-4e8da612b077"
   },
   "outputs": [],
   "source": [
    "def analyze_chemical_patterns(model, device):\n",
    "    \"\"\"Analyze how GIN embeddings capture chemical patterns\"\"\"\n",
    "    # Define molecule sets with specific patterns\n",
    "    pattern_sets = {\n",
    "        'Alcohols': ['CCO', 'CCCO', 'CCCCO', 'CC(C)O', 'OCC(O)CO'],\n",
    "        'Amines': ['CN', 'CCN', 'CCCN', 'CC(C)N', 'NCCN'],\n",
    "        'Aromatics': ['c1ccccc1', 'c1ccccc1C', 'c1ccccc1O', 'c1ccccc1N', 'c1ccc2ccccc2c1'],\n",
    "        'Carbonyls': ['CC=O', 'CCC=O', 'CC(=O)C', 'CC(=O)O', 'CC(=O)N'],\n",
    "        'Cyclic': ['C1CCCCC1', 'C1CCC1', 'C1CCCC1', 'C1CCCCCCC1', 'C1CC2CCC1CC2']\n",
    "    }\n",
    "\n",
    "    # Process each set and get embeddings\n",
    "    embeddings_by_pattern = {}\n",
    "    smiles_by_pattern = {}\n",
    "\n",
    "    for pattern, smiles_list in pattern_sets.items():\n",
    "        embeddings = []\n",
    "        valid_smiles = []\n",
    "\n",
    "        for smiles in smiles_list:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                continue\n",
    "\n",
    "            data = mol_to_graph(smiles)\n",
    "            data = data.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                node_embeddings = model.get_embeddings(data.x, data.edge_index,\n",
    "                                                   torch.zeros(data.x.size(0), dtype=torch.long, device=device))\n",
    "                graph_embedding = global_mean_pool(node_embeddings[-1],\n",
    "                                               torch.zeros(data.x.size(0), dtype=torch.long, device=device))\n",
    "\n",
    "            embeddings.append(graph_embedding.cpu().numpy())\n",
    "            valid_smiles.append(smiles)\n",
    "\n",
    "        if embeddings:\n",
    "            embeddings_by_pattern[pattern] = np.vstack(embeddings)\n",
    "            smiles_by_pattern[pattern] = valid_smiles\n",
    "\n",
    "    return embeddings_by_pattern, smiles_by_pattern\n",
    "\n",
    "# Get embeddings by chemical pattern for each model\n",
    "gin_pattern_embeddings, gin_pattern_smiles = analyze_chemical_patterns(gin_model, device)\n",
    "gcn_pattern_embeddings, gcn_pattern_smiles = analyze_chemical_patterns(gcn_model, device)\n",
    "gat_pattern_embeddings, gat_pattern_smiles = analyze_chemical_patterns(gat_model, device)\n",
    "\n",
    "# Function to visualize the chemical patterns\n",
    "def visualize_chemical_patterns(model_embeddings, model_smiles, model_name):\n",
    "    \"\"\"Visualize how the embeddings cluster by chemical pattern\"\"\"\n",
    "    # Combine all embeddings\n",
    "    all_embeddings = []\n",
    "    all_patterns = []\n",
    "\n",
    "    for pattern, embeddings in model_embeddings.items():\n",
    "        all_embeddings.append(embeddings)\n",
    "        all_patterns.extend([pattern] * len(embeddings))\n",
    "\n",
    "    if all_embeddings:\n",
    "        all_embeddings = np.vstack(all_embeddings)\n",
    "\n",
    "        # Map patterns to colors\n",
    "        pattern_types = list(model_embeddings.keys())\n",
    "        pattern_to_color = {pattern: i for i, pattern in enumerate(pattern_types)}\n",
    "\n",
    "        # Reduce dimensionality for visualization\n",
    "        if len(all_embeddings) > 2:  # Need at least 3 points for t-SNE\n",
    "            tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(all_embeddings)-1))\n",
    "            embeddings_2d = tsne.fit_transform(all_embeddings)\n",
    "\n",
    "            # Create scatter plot\n",
    "            plt.figure(figsize=(10, 8))\n",
    "\n",
    "            for pattern in pattern_types:\n",
    "                pattern_indices = [i for i, p in enumerate(all_patterns) if p == pattern]\n",
    "                if pattern_indices:\n",
    "                    plt.scatter(embeddings_2d[pattern_indices, 0], embeddings_2d[pattern_indices, 1],\n",
    "                               label=pattern, s=100, alpha=0.7)\n",
    "\n",
    "            plt.title(f'{model_name}: Chemical Pattern Clustering in Embedding Space', fontsize=16)\n",
    "            plt.xlabel('t-SNE Component 1', fontsize=14)\n",
    "            plt.ylabel('t-SNE Component 2', fontsize=14)\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Calculate within-pattern and between-pattern distances\n",
    "            pattern_stats = {}\n",
    "\n",
    "            for pattern in pattern_types:\n",
    "                pattern_indices = np.array([i for i, p in enumerate(all_patterns) if p == pattern])\n",
    "                if len(pattern_indices) >= 2:  # Need at least 2 molecules to calculate within-pattern distances\n",
    "                    pattern_embeddings = all_embeddings[pattern_indices]\n",
    "\n",
    "                    # Calculate pairwise distances within the pattern\n",
    "                    within_distances = []\n",
    "                    for i in range(len(pattern_embeddings)):\n",
    "                        for j in range(i + 1, len(pattern_embeddings)):\n",
    "                            dist = np.linalg.norm(pattern_embeddings[i] - pattern_embeddings[j])\n",
    "                            within_distances.append(dist)\n",
    "\n",
    "                    # Calculate distances to other patterns\n",
    "                    between_distances = []\n",
    "                    for other_pattern in pattern_types:\n",
    "                        if other_pattern != pattern:\n",
    "                            other_indices = np.array([i for i, p in enumerate(all_patterns) if p == other_pattern])\n",
    "                            if len(other_indices) > 0:\n",
    "                                other_embeddings = all_embeddings[other_indices]\n",
    "                                for emb1 in pattern_embeddings:\n",
    "                                    for emb2 in other_embeddings:\n",
    "                                        dist = np.linalg.norm(emb1 - emb2)\n",
    "                                        between_distances.append(dist)\n",
    "\n",
    "                    pattern_stats[pattern] = {\n",
    "                        'within_mean': np.mean(within_distances) if within_distances else 0,\n",
    "                        'between_mean': np.mean(between_distances) if between_distances else 0,\n",
    "                        'separation_ratio': np.mean(between_distances) / np.mean(within_distances) if within_distances and between_distances else 0\n",
    "                    }\n",
    "\n",
    "            # Plot separation statistics\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            patterns = list(pattern_stats.keys())\n",
    "            separation_ratios = [pattern_stats[p]['separation_ratio'] for p in patterns]\n",
    "\n",
    "            plt.bar(patterns, separation_ratios, alpha=0.7)\n",
    "            plt.xlabel('Chemical Pattern')\n",
    "            plt.ylabel('Separation Ratio (Between/Within Distance)')\n",
    "            plt.title(f'{model_name}: Pattern Separation in Embedding Space', fontsize=16)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "            # Add value labels\n",
    "            for i, v in enumerate(separation_ratios):\n",
    "                plt.text(i, v + 0.05, f'{v:.2f}', ha='center')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Visualize chemical patterns for each model\n",
    "visualize_chemical_patterns(gin_pattern_embeddings, gin_pattern_smiles, 'GIN')\n",
    "visualize_chemical_patterns(gcn_pattern_embeddings, gcn_pattern_smiles, 'GCN')\n",
    "visualize_chemical_patterns(gat_pattern_embeddings, gat_pattern_smiles, 'GAT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UG79rsD5Cfx"
   },
   "source": [
    "## 10. Ablation Studies: Understanding GIN Components <a name=\"ablation-studies-understanding-gin-components\"></a>\n",
    "\n",
    "Let's examine how different components of GIN affect its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wnmhmOjN5D3p",
    "outputId": "a3a22ad8-29cc-440f-ae41-6ded88a79e74"
   },
   "outputs": [],
   "source": [
    "class GIN_Ablation(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, dropout=0.5,\n",
    "                 epsilon=0, use_mlp=True, aggregation='sum'):\n",
    "        super(GIN_Ablation, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.use_mlp = use_mlp\n",
    "        self.aggregation = aggregation\n",
    "\n",
    "        # Initial projection of node features\n",
    "        self.node_encoder = nn.Linear(in_channels, hidden_channels)\n",
    "\n",
    "        # GIN convolution layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            if use_mlp:\n",
    "                mlp = MLP(hidden_channels, hidden_channels, hidden_channels)\n",
    "            else:\n",
    "                mlp = nn.Linear(hidden_channels, hidden_channels)\n",
    "\n",
    "            # Choose the appropriate layer based on aggregation method\n",
    "            if aggregation == 'sum':\n",
    "                self.convs.append(GINConv(mlp, train_eps=True, eps=epsilon))\n",
    "            elif aggregation == 'mean':\n",
    "                # For mean aggregation, we'll use a custom layer\n",
    "                self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown aggregation method: {aggregation}\")\n",
    "\n",
    "        # Batch normalization layers\n",
    "        self.batch_norms = nn.ModuleList([nn.BatchNorm1d(hidden_channels) for _ in range(num_layers)])\n",
    "\n",
    "        # Prediction MLP\n",
    "        self.mlp = MLP(hidden_channels, hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Initial embedding\n",
    "        x = self.node_encoder(x)\n",
    "\n",
    "        # Store representations from each layer for readout\n",
    "        xs = []\n",
    "\n",
    "        # GIN layers\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.batch_norms[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            xs.append(x)\n",
    "\n",
    "        # Global pooling (mean)\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # Final prediction\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def ablation_study():\n",
    "    \"\"\"Perform ablation study on GIN components\"\"\"\n",
    "    # Get input dimension\n",
    "    sample = dataset[0]\n",
    "    in_channels = sample.x.shape[1]\n",
    "\n",
    "    # Define hyperparameters\n",
    "    hidden_channels = 64\n",
    "    out_channels = 1  # Binary classification\n",
    "    num_layers = 3\n",
    "    dropout = 0.5\n",
    "    lr = 0.001\n",
    "    weight_decay = 1e-4\n",
    "    epochs = 50  # Shorter training for ablation\n",
    "\n",
    "    # Define ablation configurations\n",
    "    ablation_configs = {\n",
    "        'Full GIN': {\n",
    "            'epsilon': 0,\n",
    "            'use_mlp': True,\n",
    "            'aggregation': 'sum'\n",
    "        },\n",
    "        'No MLP': {\n",
    "            'epsilon': 0,\n",
    "            'use_mlp': False,\n",
    "            'aggregation': 'sum'\n",
    "        },\n",
    "        'Mean Aggregation': {\n",
    "            'epsilon': 0,\n",
    "            'use_mlp': True,\n",
    "            'aggregation': 'mean'\n",
    "        },\n",
    "        'Fixed Epsilon=0': {\n",
    "            'epsilon': 0,\n",
    "            'use_mlp': True,\n",
    "            'aggregation': 'sum'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Initialize and train models\n",
    "    results = {}\n",
    "\n",
    "    for name, config in ablation_configs.items():\n",
    "        print(f\"\\nTraining {name} model...\")\n",
    "        model = GIN_Ablation(\n",
    "            in_channels,\n",
    "            hidden_channels,\n",
    "            out_channels,\n",
    "            num_layers,\n",
    "            dropout,\n",
    "            config['epsilon'],\n",
    "            config['use_mlp'],\n",
    "            config['aggregation']\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        results[name] = train_and_evaluate(model, optimizer, train_loader, val_loader, test_loader, device, epochs)\n",
    "        print(f\"{name} Test AUC: {results[name]['test_auc']:.4f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run ablation study\n",
    "ablation_results = ablation_study()\n",
    "\n",
    "# Visualize ablation results\n",
    "plt.figure(figsize=(10, 6))\n",
    "names = list(ablation_results.keys())\n",
    "aucs = [result['test_auc'] for result in ablation_results.values()]\n",
    "\n",
    "bars = plt.bar(names, aucs, color='lightgreen')\n",
    "plt.xlabel('Model Configuration')\n",
    "plt.ylabel('Test AUC')\n",
    "plt.title('GIN Ablation Study Results')\n",
    "plt.ylim(0.5, 1.0)  # AUC ranges from 0.5 to 1.0\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add value labels\n",
    "for bar, auc in zip(bars, aucs):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, f'{auc:.4f}',\n",
    "            ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "plt.grid(True, axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot training curves for ablation models\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(1, 3, 1)\n",
    "for name, result in ablation_results.items():\n",
    "    plt.plot(result['train_losses'], label=name)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot validation loss\n",
    "plt.subplot(1, 3, 2)\n",
    "for name, result in ablation_results.items():\n",
    "    plt.plot(result['val_losses'], label=name)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot validation AUC\n",
    "plt.subplot(1, 3, 3)\n",
    "for name, result in ablation_results.items():\n",
    "    plt.plot(result['val_aucs'], label=name)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation AUC')\n",
    "plt.title('Validation AUC Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDiT63f64-Ua"
   },
   "source": [
    "## 11. Practical Applications in Drug Discovery <a name=\"practical-applications-in-drug-discovery\"></a>\n",
    "\n",
    "Let's explore how GIN can be used in practical drug discovery applications, such as activity prediction and virtual screening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UUfnMMFw5JeS",
    "outputId": "5f511f7c-c1c5-45a9-f1ee-e03f75a0e151"
   },
   "outputs": [],
   "source": [
    "def virtual_screening_demo():\n",
    "    \"\"\"Demonstrate a simple virtual screening workflow using GIN\"\"\"\n",
    "    # Load a small library of drug-like molecules\n",
    "    drug_smiles = [\n",
    "        \"CC(C)CC1=CC=C(C=C1)C(C)C(=O)O\",  # Ibuprofen\n",
    "        \"CC(=O)OC1=CC=CC=C1C(=O)O\",      # Aspirin\n",
    "        \"CC(=O)NC1=CC=C(C=C1)O\",         # Acetaminophen\n",
    "        \"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\",  # Caffeine\n",
    "        \"CN1C(=O)CN=C(C2=CC=CC=C2)C1=O\", # Diazepam\n",
    "        \"COC1=CC2=C(C=C1OC)C(=O)C(CC2)CC3=CC(=C(C=C3)OC)OC\", # Podophyllotoxin\n",
    "        \"CC(C)NCC(O)COC1=CC=C(C=C1)CC(=O)N\", # Atenolol\n",
    "        \"CCOC(=O)N1CCN(CC1)C2=NC3=CC=CC=C3NC2=O\", # Zopiclone\n",
    "        \"CC1=C(C(=O)N(N1C)C2=CC=CC=C2)C3=CC=CC=C3\", # Antipyrine\n",
    "        \"CN1C2=C(C(=O)N(C1=O)C)NC=N2\"   # Theophylline\n",
    "    ]\n",
    "\n",
    "    drug_names = [\n",
    "        \"Ibuprofen\", \"Aspirin\", \"Acetaminophen\", \"Caffeine\", \"Diazepam\",\n",
    "        \"Podophyllotoxin\", \"Atenolol\", \"Zopiclone\", \"Antipyrine\", \"Theophylline\"\n",
    "    ]\n",
    "\n",
    "    # Generate embeddings for each drug\n",
    "    gin_model.eval()\n",
    "    embeddings = []\n",
    "    valid_indices = []\n",
    "\n",
    "    for i, smiles in enumerate(drug_smiles):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            continue\n",
    "\n",
    "        data = mol_to_graph(smiles)\n",
    "        data = data.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            node_embeddings = gin_model.get_embeddings(data.x, data.edge_index,\n",
    "                                                   torch.zeros(data.x.size(0), dtype=torch.long, device=device))\n",
    "            graph_embedding = global_mean_pool(node_embeddings[-1],\n",
    "                                           torch.zeros(data.x.size(0), dtype=torch.long, device=device))\n",
    "\n",
    "        embeddings.append(graph_embedding.cpu().numpy())\n",
    "        valid_indices.append(i)\n",
    "\n",
    "    if embeddings:\n",
    "        embeddings = np.vstack(embeddings)\n",
    "        valid_drug_names = [drug_names[i] for i in valid_indices]\n",
    "        valid_drug_smiles = [drug_smiles[i] for i in valid_indices]\n",
    "\n",
    "        # Predict BBB permeability for each drug\n",
    "        predictions = []\n",
    "\n",
    "        for i, smiles in enumerate(valid_drug_smiles):\n",
    "            data = mol_to_graph(smiles).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = gin_model(data.x, data.edge_index, torch.zeros(data.x.size(0), dtype=torch.long, device=device))\n",
    "                prob = torch.sigmoid(out).item()\n",
    "                predictions.append(prob)\n",
    "\n",
    "        # Visualize results\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # Sort by predicted probability\n",
    "        sorted_indices = np.argsort(predictions)[::-1]\n",
    "        sorted_names = [valid_drug_names[i] for i in sorted_indices]\n",
    "        sorted_probs = [predictions[i] for i in sorted_indices]\n",
    "\n",
    "        # Plot predictions\n",
    "        plt.subplot(1, 2, 1)\n",
    "        bars = plt.barh(sorted_names, sorted_probs, color=['green' if p > 0.5 else 'red' for p in sorted_probs])\n",
    "        plt.xlabel('Predicted Probability of BBB Penetration')\n",
    "        plt.title('Virtual Screening Results')\n",
    "        plt.xlim(0, 1)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # Add values\n",
    "        for i, bar in enumerate(bars):\n",
    "            plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "                    f'{sorted_probs[i]:.3f}', va='center')\n",
    "\n",
    "        # Calculate similarity matrix\n",
    "        similarity_matrix = np.zeros((len(embeddings), len(embeddings)))\n",
    "        for i in range(len(embeddings)):\n",
    "            for j in range(len(embeddings)):\n",
    "                similarity_matrix[i, j] = np.dot(embeddings[i], embeddings[j]) / (\n",
    "                    np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[j]))\n",
    "\n",
    "        # Plot similarity matrix\n",
    "        plt.subplot(1, 2, 2)\n",
    "        im = plt.imshow(similarity_matrix, cmap='viridis')\n",
    "        plt.colorbar(im, label='Cosine Similarity')\n",
    "        plt.title('Drug Similarity Matrix Based on GIN Embeddings')\n",
    "        plt.xticks(range(len(valid_drug_names)), valid_drug_names, rotation=90)\n",
    "        plt.yticks(range(len(valid_drug_names)), valid_drug_names)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Visualize molecules clustered by similarity\n",
    "        # Reduce dimensionality for visualization\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(embeddings)-1))\n",
    "        embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "        plt.figure(figsize=(12, 10))\n",
    "\n",
    "        # Plot embeddings\n",
    "        scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],\n",
    "                           c=predictions, cmap='coolwarm', s=100, alpha=0.7)\n",
    "        plt.colorbar(scatter, label='Predicted BBB Penetration')\n",
    "\n",
    "        # Add drug names as labels\n",
    "        for i, name in enumerate(valid_drug_names):\n",
    "            plt.text(embeddings_2d[i, 0], embeddings_2d[i, 1], name, fontsize=12,\n",
    "                   ha='center', va='bottom')\n",
    "\n",
    "            # Add molecule images\n",
    "            mol = Chem.MolFromSmiles(valid_drug_smiles[i])\n",
    "\n",
    "            if mol:\n",
    "                img = Draw.MolToImage(mol, size=(100, 100))\n",
    "                imagebox = OffsetImage(img, zoom=0.5)\n",
    "                ab = AnnotationBbox(imagebox, (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                                  xycoords='data', boxcoords=\"offset points\",\n",
    "                                  box_alignment=(0.5, 0.5), pad=0.5,\n",
    "                                  bboxprops=dict(facecolor='white', alpha=0.8))\n",
    "                plt.gca().add_artist(ab)\n",
    "\n",
    "        plt.title('Drug Similarity Space (GIN Embeddings)', fontsize=16)\n",
    "        plt.xlabel('t-SNE Component 1', fontsize=14)\n",
    "        plt.ylabel('t-SNE Component 2', fontsize=14)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Demonstrate virtual screening application\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "virtual_screening_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jx8pwm5-5M_x"
   },
   "source": [
    "## 12. Conclusion and Best Practices <a name=\"conclusion-and-best-practices\"></a>\n",
    "\n",
    "Let's summarize what we've learned about GIN and provide best practices for using it in molecular modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1Mx2oyB-5OyR",
    "outputId": "b72a2b08-356d-4078-9ac8-fb71a1198334"
   },
   "outputs": [],
   "source": [
    "def create_summary_visualization():\n",
    "    \"\"\"Create a visual summary of GIN for molecular modeling\"\"\"\n",
    "    plt.figure(figsize=(14, 10))\n",
    "\n",
    "    summary_text = \"\"\"\n",
    "    # Graph Isomorphism Networks (GIN) for Molecular Modeling\n",
    "\n",
    "    ## Key Strengths of GIN\n",
    "\n",
    "    1. **Discriminative Power**: Maximum ability to distinguish different molecular structures\n",
    "    2. **Isomorphism Awareness**: Can effectively differentiate non-isomorphic molecules\n",
    "    3. **Chemical Pattern Recognition**: Captures chemical substructures and functional groups\n",
    "    4. **Effective Fingerprinting**: Creates meaningful molecular embeddings for similarity search\n",
    "\n",
    "    ## When to Use GIN vs. Other GNNs\n",
    "\n",
    "    - **Use GIN when**:\n",
    "      * Subtle structural differences are important\n",
    "      * You need optimal discrimination between similar molecules\n",
    "      * Creating molecular fingerprints for similarity search\n",
    "      * Identifying specific scaffolds or substructures\n",
    "\n",
    "    - **Consider GCN when**:\n",
    "      * Computational efficiency is important\n",
    "      * Working with very large molecules or datasets\n",
    "      * The task depends more on global properties than precise structure\n",
    "\n",
    "    - **Consider GAT when**:\n",
    "      * You need interpretability of atom-atom interactions\n",
    "      * Specific interactions between atoms are important\n",
    "      * Working with 3D structures where spatial attention matters\n",
    "\n",
    "    ## Best Practices for GIN in Drug Discovery\n",
    "\n",
    "    1. **Architecture**:\n",
    "       * Use sufficient layers (3-4) to capture complex substructures\n",
    "       * Apply skip connections for very deep networks\n",
    "       * Use batch normalization between layers\n",
    "\n",
    "    2. **Training**:\n",
    "       * Regularize with dropout (0.3-0.5) to prevent overfitting\n",
    "       * Use learning rate ~0.001 with Adam optimizer\n",
    "       * Consider weight decay (1e-4 to 1e-5)\n",
    "\n",
    "    3. **Features**:\n",
    "       * Include atom and bond features relevant to your task\n",
    "       * Consider including 3D information for conformation-dependent properties\n",
    "       * For fingerprinting, use embeddings from later layers\n",
    "\n",
    "    ## Limitations\n",
    "\n",
    "    1. Computationally more expensive than simpler GNNs\n",
    "    2. May overfit on small datasets due to high expressive power\n",
    "    3. Less interpretable than attention-based models (GAT)\n",
    "    4. Standard implementation does not directly incorporate 3D information\n",
    "    \"\"\"\n",
    "\n",
    "    plt.text(0.05, 0.5, summary_text, fontsize=14,\n",
    "            verticalalignment='center', horizontalalignment='left',\n",
    "            family='monospace', transform=plt.gca().transAxes)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "create_summary_visualization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFNdWS9-5WYg"
   },
   "source": [
    "This concludes our comprehensive tutorial on Graph Isomorphism Networks (GIN) for chemists and pharmacists. We've covered the theoretical foundations, implementation details, and practical applications of GIN, particularly in the context of molecular modeling and drug discovery.\n",
    "\n",
    "Key takeaways include:\n",
    "1. The unique theoretical power of GIN in distinguishing molecular structures\n",
    "2. How to implement and train GIN models for molecular property prediction\n",
    "3. Using GIN for molecular fingerprinting and similarity search\n",
    "4. The ability of GIN to capture chemical patterns and substructures\n",
    "5. Practical applications in drug discovery workflows\n",
    "\n",
    "We hope this tutorial helps you understand and apply GIN in your chemical and pharmaceutical research! '# Graph Isomorphism Networks (GIN) Tutorial for Chemists and Pharmacists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XyoRUEma5RxS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
