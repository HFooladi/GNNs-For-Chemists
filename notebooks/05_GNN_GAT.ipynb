{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HFooladi/GNNs-For-Chemists/blob/main/notebooks/05_GNN_GAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pi8JlOwjeQjw"
   },
   "source": [
    "# Graph Attention Networks (GAT) Tutorial for Chemists and Pharmacists\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Setup and Installation](#setup-and-installation)\n",
    "3. [Understanding Molecular Graphs](#understand-graph)\n",
    "4. [Graph Neural Networks: Basic Concepts](#gnn-basics)\n",
    "5. [The Attention Mechanism: Why It Matters](#attention-mechanism)\n",
    "6. [Implementing a Graph Attention Network (GAT)](#implement-gat)\n",
    "7. [Understanding Multi-Head Attention](#understand-multi-head)\n",
    "8. [Implementing a Complete GAT Model for Molecular Property Prediction](#implement-gat-property-prediction)\n",
    "9. [Visualizing Attention Weights in Molecules](#visualize-attention)\n",
    "10. [Comparing Single-Head vs Multi-Head Attention Performance](#compare-single-multi-head)\n",
    "11. [Visualizing Feature Transformation Through the Network](#visualize-feature-transformation)\n",
    "12. [Interactive Visualization of Attention Mechanism](#interactive-visualiztion)\n",
    "13. [Conclusion and Further Research Directions](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7yeqnMLQyVX"
   },
   "source": [
    "## 1. Introduction to Graph Neural Networks and Attention Mechanisms  <a name=\"introduction\"></a>\n",
    "\n",
    "In this notebook, we'll explore Graph Attention Networks (GAT), a powerful graph neural network architecture particularly useful for molecular data. This tutorial is specifically designed for chemists and pharmacists who want to understand how these models work for molecular property prediction and drug discovery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFe5wKCJQ5Co"
   },
   "source": [
    "## 2. Setup and Installation <a name=\"setup-and-installation\"></a>\n",
    "\n",
    "First, let's install the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8UJWZUJWeP6f",
    "outputId": "dab67029-23c0-4f8d-defc-353d67305370"
   },
   "outputs": [],
   "source": [
    "#@title Intstall necessary libraries\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "!pip install -q rdkit\n",
    "!pip install -q networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Ej6ZhFBdbPz",
    "outputId": "62350c69-7318-4457-81ea-7041ceb26561"
   },
   "outputs": [],
   "source": [
    "#@title Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, GCNConv, MessagePassing\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.utils import to_networkx, softmax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, AllChem\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import io\n",
    "from PIL import Image\n",
    "import random\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSaeOEFIe6GJ"
   },
   "source": [
    "## 3. Understanding Molecular Graphs <a name=\"understand-graph\"></a>\n",
    "\n",
    "Molecules are naturally represented as graphs where atoms are nodes and bonds are edges. Let's visualize a simple molecule as a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "887XhJ1Be6jl",
    "outputId": "51376bd7-9303-4677-8439-496c434f9cdb"
   },
   "outputs": [],
   "source": [
    "def atom_features(atom):\n",
    "    \"\"\"\n",
    "    Extract a feature vector for an RDKit atom.\n",
    "\n",
    "    Features included:\n",
    "        - Atomic number\n",
    "        - Chirality tag (encoded as integer)\n",
    "        - Degree (number of directly-bonded atoms)\n",
    "        - Formal charge\n",
    "        - Total number of hydrogens\n",
    "        - Number of radical electrons\n",
    "        - Hybridization (encoded as integer)\n",
    "        - Aromaticity (0 or 1)\n",
    "        - Ring membership (0 or 1)\n",
    "\n",
    "    Args:\n",
    "        atom (rdkit.Chem.rdchem.Atom): An RDKit Atom object.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Feature tensor of shape (9,) with dtype long.\n",
    "    \"\"\"\n",
    "    return torch.tensor([\n",
    "        atom.GetAtomicNum(),                    # Atomic number\n",
    "        int(atom.GetChiralTag()),               # Chirality\n",
    "        atom.GetDegree(),                       # Degree\n",
    "        atom.GetFormalCharge(),                 # Formal charge\n",
    "        atom.GetTotalNumHs(),                   # Number of hydrogens\n",
    "        atom.GetNumRadicalElectrons(),          # Radical electrons\n",
    "        int(atom.GetHybridization()),           # Hybridization\n",
    "        int(atom.GetIsAromatic()),              # Aromaticity\n",
    "        int(atom.IsInRing())                    # Ring membership\n",
    "    ], dtype=torch.long)\n",
    "\n",
    "\n",
    "def bond_features(bond):\n",
    "    \"\"\"\n",
    "    Extract a feature vector for an RDKit bond.\n",
    "\n",
    "    Features included:\n",
    "        - Bond type as double (e.g., 1.0 for single, 2.0 for double)\n",
    "        - Conjugation (0 or 1)\n",
    "        - Ring membership (0 or 1)\n",
    "\n",
    "    Args:\n",
    "        bond (rdkit.Chem.rdchem.Bond): An RDKit Bond object.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Feature tensor of shape (3,) with dtype long.\n",
    "    \"\"\"\n",
    "    return torch.tensor([\n",
    "        int(bond.GetBondTypeAsDouble()),        # Bond type\n",
    "        int(bond.GetIsConjugated()),            # Conjugation\n",
    "        int(bond.IsInRing())                    # Ring membership\n",
    "    ], dtype=torch.long)\n",
    "\n",
    "def mol_to_graph(smiles):\n",
    "    \"\"\"\n",
    "    Converts a SMILES into a PyTorch Geometric graph data object.\n",
    "\n",
    "    Nodes represent atoms with features, and edges represent bonds with features.\n",
    "    The graph is undirected: each bond adds two directed edges (i->j and j->i).\n",
    "\n",
    "    Args:\n",
    "        smiles (str): SMILES representing the molecule.\n",
    "\n",
    "    Returns:\n",
    "        torch_geometric.data.Data: Graph data object containing:\n",
    "            - x: Node feature matrix [num_nodes, 9]\n",
    "            - edge_index: Edge list [2, num_edges]\n",
    "            - edge_attr: Edge feature matrix [num_edges, 3]\n",
    "            - smiles: Original SMILES\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        raise ValueError(f\"Invalid SMILES string: {smiles}\")\n",
    "\n",
    "    # Node features\n",
    "    x = torch.stack([atom_features(atom) for atom in mol.GetAtoms()], dim=0)\n",
    "\n",
    "    # Edge index and edge features\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "\n",
    "        # Add both directions for undirected graph\n",
    "        edge_index.append((i, j))\n",
    "        edge_index.append((j, i))\n",
    "\n",
    "        edge_attr.append(bond_features(bond))\n",
    "        edge_attr.append(bond_features(bond))\n",
    "\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.stack(edge_attr, dim=0) if edge_attr else None\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, smiles=smiles)\n",
    "    return data\n",
    "\n",
    "\n",
    "def visualize_molecule(smiles, title=\"Molecule\"):\n",
    "    \"\"\"Visualize a molecule using RDKit\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    AllChem.Compute2DCoords(mol)\n",
    "\n",
    "    # Draw molecule\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    drawer = rdMolDraw2D.MolDraw2DCairo(500, 500)\n",
    "    drawer.DrawMolecule(mol)\n",
    "    drawer.FinishDrawing()\n",
    "    img = drawer.GetDrawingText()\n",
    "\n",
    "    # Convert the image data to a PIL Image\n",
    "    pil_image = Image.open(io.BytesIO(img))\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(pil_image)\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def visualize_molecular_graph(smiles, title=\"Molecular Graph\"):\n",
    "    \"\"\"\n",
    "    Visualizes the 2D structure of a molecule using RDKit and networkx and displays it.\n",
    "\n",
    "    Args:\n",
    "        smiles (str): SMILES representing the molecule.\n",
    "        title (str): Plot title.\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    AllChem.Compute2DCoords(mol)\n",
    "\n",
    "    data = mol_to_graph(smiles)\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "    # Get the 2D coordinates from RDKit\n",
    "    pos = {}\n",
    "    for i, atom in enumerate(mol.GetAtoms()):\n",
    "        pos[i] = mol.GetConformer().GetAtomPosition(i)\n",
    "        pos[i] = (pos[i].x, -pos[i].y)  # Flip y for better visualization\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "\n",
    "    # Get atom labels\n",
    "    atom_labels = {i: atom.GetSymbol() for i, atom in enumerate(mol.GetAtoms())}\n",
    "\n",
    "    # Get atom features for node coloring\n",
    "    atom_features = [atom.GetAtomicNum() for atom in mol.GetAtoms()]\n",
    "\n",
    "    # Draw the graph\n",
    "    nx.draw(G, pos,\n",
    "            labels=atom_labels,\n",
    "            with_labels=True,\n",
    "            node_color=atom_features,\n",
    "            cmap=plt.cm.viridis,\n",
    "            node_size=500,\n",
    "            font_size=10,\n",
    "            font_color='white',\n",
    "            edge_color='gray')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example: Aspirin\n",
    "aspirin_smiles = \"CC(=O)OC1=CC=CC=C1C(=O)O\"\n",
    "visualize_molecule(aspirin_smiles, \"Aspirin\")\n",
    "visualize_molecular_graph(aspirin_smiles, \"Aspirin as a Graph\")\n",
    "\n",
    "# Example: Paracetamol (Acetaminophen)\n",
    "paracetamol_smiles = \"CC(=O)NC1=CC=C(C=C1)O\"\n",
    "visualize_molecule(paracetamol_smiles, \"Paracetamol\")\n",
    "visualize_molecular_graph(paracetamol_smiles, \"Paracetamol as a Graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEDQA8RlfE8u"
   },
   "source": [
    "## 4. Graph Neural Networks: Basic Concepts <a name=\"gnn-basics\"></a>\n",
    "\n",
    "Before diving into GATs, let's understand the basic concept of message passing in graph neural networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 720
    },
    "id": "YOvVEVhifGDb",
    "outputId": "4213af31-734b-4857-cc54-85878698af33"
   },
   "outputs": [],
   "source": [
    "class BasicMessagePassing(MessagePassing):\n",
    "    \"\"\"\n",
    "    A simple custom message passing layer using PyG's MessagePassing framework.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input features per node.\n",
    "        out_channels (int): Number of output features per node.\n",
    "\n",
    "    Notes: \n",
    "        - This is a simplified example. Receiving messages from all neighbors.\n",
    "        - The aggregation function is just the sum of the messages.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(BasicMessagePassing, self).__init__(aggr='add')  # \"add\" aggregation\n",
    "        self.lin = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Forward pass of the message passing layer.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Node feature matrix [num_nodes, in_channels].\n",
    "            edge_index (Tensor): Edge index [2, num_edges].\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Updated node features [num_nodes, out_channels].\n",
    "        \"\"\"\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        # Transform node features\n",
    "        x = self.lin(x.float())\n",
    "\n",
    "        # Start propagating messages\n",
    "        return self.propagate(edge_index, x=x)\n",
    "\n",
    "    def message(self, x_j):\n",
    "        \"\"\"\n",
    "        Constructs messages from source nodes.\n",
    "\n",
    "        Args:\n",
    "            x_j (Tensor): Features of source nodes [num_edges, out_channels].\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Messages to be aggregated.\n",
    "        \"\"\"\n",
    "        # x_j has shape [E, out_channels]\n",
    "        # Simple message function that just returns node features\n",
    "        return x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        \"\"\"\n",
    "        Updates node embeddings after aggregation.\n",
    "\n",
    "        Args:\n",
    "            aggr_out (Tensor): Aggregated messages for each node [num_nodes, out_channels].\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Updated node features.\n",
    "        \"\"\"\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "        # No update function, just return the aggregated messages\n",
    "        return aggr_out\n",
    "\n",
    "def simulate_basic_message_passing(data, input_dim=9, output_dim=9):\n",
    "    \"\"\"\n",
    "    Apply the basic message passing layer to a molecular graph.\n",
    "\n",
    "    Args:\n",
    "        data (Data): PyG Data object for a molecule.\n",
    "        input_dim (int): Input feature dimension.\n",
    "        output_dim (int): Output feature dimension.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor]: Original and updated node feature matrices.\n",
    "    \"\"\"\n",
    "    # Initialize a simple message passing layer\n",
    "    mp_layer = BasicMessagePassing(input_dim, output_dim)\n",
    "\n",
    "    # Original node features\n",
    "    original_features = data.x\n",
    "\n",
    "    # Apply message passing\n",
    "    updated_features = mp_layer(data.x, data.edge_index)\n",
    "\n",
    "    return original_features, updated_features\n",
    "\n",
    "# Create a visualization of message passing on aspirin\n",
    "aspirin_data = mol_to_graph(aspirin_smiles)\n",
    "orig_feat, updated_feat = simulate_basic_message_passing(aspirin_data)\n",
    "\n",
    "print(\"Original node features (first 3 nodes):\")\n",
    "print(orig_feat[:3])\n",
    "print(\"\\nUpdated node features after message passing (first 3 nodes):\")\n",
    "print(updated_feat[:3])\n",
    "\n",
    "# Visualize the difference using a heatmap\n",
    "def plot_feature_comparison(original, updated, title=\"Feature Comparison\"):\n",
    "    \"\"\"Plot a comparison of original and updated features\"\"\"\n",
    "    # Compute the difference\n",
    "    diff = (updated - original).abs().mean(dim=1)\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # Plot original features\n",
    "    im1 = ax1.imshow(original.detach().numpy(), cmap='viridis')\n",
    "    ax1.set_title(\"Original Features\")\n",
    "    ax1.set_xlabel(\"Feature Dimension\")\n",
    "    ax1.set_ylabel(\"Node ID\")\n",
    "    plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "    # Plot updated features\n",
    "    im2 = ax2.imshow(updated.detach().numpy(), cmap='viridis')\n",
    "    ax2.set_title(\"Updated Features After Message Passing\")\n",
    "    ax2.set_xlabel(\"Feature Dimension\")\n",
    "    ax2.set_ylabel(\"Node ID\")\n",
    "    plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "    # Plot the difference in a separate visualization\n",
    "    ax3.bar(range(len(diff)), diff.detach().numpy())\n",
    "    ax3.set_title(\"Average Absolute Difference Per Node\")\n",
    "    ax3.set_xlabel(\"Node ID\")\n",
    "    ax3.set_ylabel(\"Average Absolute Difference\")\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_comparison(orig_feat, updated_feat, \"Basic Message Passing on Aspirin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0EsbspNfOXF"
   },
   "source": [
    "## 5. The Attention Mechanism: Why It Matters <a name=\"attention-mechanism\"></a>\n",
    "\n",
    "Let's compare traditional averaging-based message passing with attention-based message passing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1gVD_TCsfLLr",
    "outputId": "e741e26d-01bf-41c0-9ce5-5d7451e1f95e"
   },
   "outputs": [],
   "source": [
    "class SimpleAveragingLayer(MessagePassing):\n",
    "    \"\"\"\n",
    "    A layer that simply averages neighbor features\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input features per node.\n",
    "        out_channels (int): Number of output features per node.\n",
    "\n",
    "    Notes: \n",
    "        - This is a simplified example. Receiving messages from all neighbors.\n",
    "        - The aggregation function is just the average of the messages.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SimpleAveragingLayer, self).__init__(aggr='mean')  # \"mean\" aggregation\n",
    "        self.lin = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Transform node features\n",
    "        x = self.lin(x.float())\n",
    "\n",
    "        # Start propagating messages\n",
    "        return self.propagate(edge_index, x=x)\n",
    "\n",
    "    def message(self, x_j):\n",
    "        # Simple message function\n",
    "        return x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # No update function\n",
    "        return aggr_out\n",
    "\n",
    "class SimpleAttentionLayer(MessagePassing):\n",
    "    \"\"\"\n",
    "    A simplified attention layer with single head for demonstration\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input features per node.\n",
    "        out_channels (int): Number of output features per node.\n",
    "\n",
    "    Notes: \n",
    "        - This is a simplified example. Receiving messages from all neighbors.\n",
    "        - The aggregation function is just the sum of the messages with a learned attention mechanism.\n",
    "        - The attention mechanism is a simple linear layer that takes the concatenation of the target and source node features and outputs a single attention coefficient.\n",
    "        - The attention coefficient is then normalized to sum to 1.\n",
    "        - We consider one head for simplicity.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SimpleAttentionLayer, self).__init__(aggr='add')\n",
    "        self.lin = nn.Linear(in_channels, out_channels)\n",
    "        self.att = nn.Linear(2 * out_channels, 1)\n",
    "        self.alpha = None  # Store attention weights\n",
    "\n",
    "    def forward(self, x, edge_index, return_attention_weights=False):\n",
    "        # Transform node features\n",
    "        x = self.lin(x.float())\n",
    "        \n",
    "        # Start propagating messages with attention\n",
    "        out = self.propagate(edge_index, x=x)\n",
    "        \n",
    "        if return_attention_weights:\n",
    "            return out, (edge_index, self.alpha)\n",
    "        return out\n",
    "\n",
    "    def message(self, x_i, x_j, index):\n",
    "        # Concatenate features of target and source nodes\n",
    "        x = torch.cat([x_i, x_j], dim=-1)\n",
    "        \n",
    "        # Compute attention coefficient\n",
    "        alpha = self.att(x)\n",
    "        alpha = F.leaky_relu(alpha, negative_slope=0.2)\n",
    "        \n",
    "        # Normalize attention coefficients\n",
    "        alpha = softmax(alpha, index)\n",
    "        \n",
    "        # Store attention weights\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Apply attention weights to source features\n",
    "        return x_j * alpha\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n",
    "\n",
    "def compare_averaging_vs_attention(data, input_dim=9, output_dim=9):\n",
    "    \"\"\"Compare averaging vs attention on a molecular graph\"\"\"\n",
    "    # Initialize layers\n",
    "    avg_layer = SimpleAveragingLayer(input_dim, output_dim)\n",
    "    att_layer = SimpleAttentionLayer(input_dim, output_dim)\n",
    "\n",
    "    # Apply both methods\n",
    "    avg_features = avg_layer(data.x, data.edge_index)\n",
    "    att_features, (edge_index, att_weights) = att_layer(data.x, data.edge_index, return_attention_weights=True)\n",
    "\n",
    "    return data.x, avg_features, att_features, att_weights\n",
    "\n",
    "# Compare on aspirin\n",
    "original, avg_feat, att_feat, att_weights = compare_averaging_vs_attention(aspirin_data)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot original features\n",
    "axes[0].imshow(original.detach().numpy(), cmap='viridis')\n",
    "axes[0].set_title(\"Original Features\")\n",
    "axes[0].set_xlabel(\"Feature Dimension\")\n",
    "axes[0].set_ylabel(\"Node ID\")\n",
    "\n",
    "# Plot averaging features\n",
    "axes[1].imshow(avg_feat.detach().numpy(), cmap='viridis')\n",
    "axes[1].set_title(\"After Averaging Aggregation\")\n",
    "axes[1].set_xlabel(\"Feature Dimension\")\n",
    "\n",
    "# Plot attention features\n",
    "im = axes[2].imshow(att_feat.detach().numpy(), cmap='viridis')\n",
    "axes[2].set_title(\"After Attention Aggregation\")\n",
    "axes[2].set_xlabel(\"Feature Dimension\")\n",
    "\n",
    "plt.colorbar(im, ax=axes[2])\n",
    "plt.suptitle(\"Comparison: Original vs. Averaging vs. Attention\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize the attention weights on the molecular graph\n",
    "def visualize_attention_on_graph(smiles, attention_weights, title=\"Attention Weights\"):\n",
    "    \"\"\"Visualize attention weights on a molecular graph\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    AllChem.Compute2DCoords(mol)\n",
    "\n",
    "    data = mol_to_graph(smiles)\n",
    "    G = to_networkx(data, to_undirected=False)  # Directed graph for attention\n",
    "\n",
    "    # Get the 2D coordinates from RDKit\n",
    "    pos = {}\n",
    "    for i, atom in enumerate(mol.GetAtoms()):\n",
    "        pos[i] = mol.GetConformer().GetAtomPosition(i)\n",
    "        pos[i] = (pos[i].x, -pos[i].y)  # Flip y for better visualization\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "\n",
    "    # Get atom labels\n",
    "    atom_labels = {i: atom.GetSymbol() for i, atom in enumerate(mol.GetAtoms())}\n",
    "\n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G, pos,\n",
    "                          node_color='lightblue',\n",
    "                          node_size=500)\n",
    "\n",
    "    # Draw labels\n",
    "    nx.draw_networkx_labels(G, pos, labels=atom_labels, font_size=12)\n",
    "\n",
    "    # Normalize attention weights for visualization\n",
    "    if attention_weights is not None:\n",
    "        att_weights = attention_weights.detach().numpy().flatten()\n",
    "        # Create a mapping from edge indices to attention weights\n",
    "        edge_att = {}\n",
    "        for i, (src, dst) in enumerate(data.edge_index.t().tolist()):\n",
    "            edge_att[(src, dst)] = att_weights[i]\n",
    "\n",
    "        # Create edge list with weights\n",
    "        edges, weights = zip(*edge_att.items())\n",
    "\n",
    "        # Normalize weights for visualization\n",
    "        min_width = 1\n",
    "        max_width = 5\n",
    "        norm_weights = [min_width + (w - min(weights)) * (max_width - min_width) / (max(weights) - min(weights) + 1e-6) for w in weights]\n",
    "\n",
    "        # Draw edges with varying width based on attention\n",
    "        nx.draw_networkx_edges(G, pos,\n",
    "                              edgelist=edges,\n",
    "                              width=norm_weights,\n",
    "                              edge_color='gray',\n",
    "                              alpha=0.7,\n",
    "                              arrowsize=15,\n",
    "                              node_size=500,\n",
    "                              connectionstyle='arc3,rad=0.1')\n",
    "    else:\n",
    "        # Draw edges without attention weights\n",
    "        nx.draw_networkx_edges(G, pos, edge_color='gray', alpha=0.7)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Reshape attention weights to match edges\n",
    "edge_att_weights = torch.zeros(aspirin_data.edge_index.size(1))\n",
    "for i in range(len(att_weights)):\n",
    "    edge_att_weights[i] = att_weights[i]\n",
    "\n",
    "visualize_attention_on_graph(aspirin_smiles, edge_att_weights, \"Attention Weights on Aspirin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jzM0Un9g28R"
   },
   "source": [
    "### 5.1 Key Differences Between Attention and Simple Averaging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "id": "BDFUDS7Wftd2",
    "outputId": "cc249c78-db22-4da3-ed7f-79d9ddb42e53"
   },
   "outputs": [],
   "source": [
    "def create_difference_visualization():\n",
    "    \"\"\"\n",
    "    Create a visual explanation of the difference between\n",
    "    averaging aggregation and attention-based aggregation in GNNs.\n",
    "\n",
    "    This function generates a simple 4-node graph centered on node 1,\n",
    "    showing how it aggregates messages from its neighbors:\n",
    "    - Left subplot: simple averaging (equal weights).\n",
    "    - Right subplot: attention (varying weights based on learned importance).\n",
    "    \"\"\"\n",
    "    # Create a simple synthetic graph for demonstration\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Example graph layout\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from([1, 2, 3, 4])\n",
    "    G.add_edges_from([(1, 2), (1, 3), (1, 4)])\n",
    "\n",
    "    pos = {1: (0.5, 0.5), 2: (0.2, 0.8), 3: (0.8, 0.8), 4: (0.5, 0.2)}\n",
    "\n",
    "    # Draw averaging aggregation\n",
    "    ax1.set_title(\"Simple Averaging Aggregation\", fontsize=14)\n",
    "    nx.draw_networkx_nodes(G, pos, ax=ax1, node_size=500, node_color='lightblue')\n",
    "    nx.draw_networkx_labels(G, pos, ax=ax1, font_size=12)\n",
    "\n",
    "    # Draw edges with equal importance\n",
    "    nx.draw_networkx_edges(G, pos, ax=ax1, width=2.0, alpha=0.7,\n",
    "                         arrowsize=15, edge_color='gray')\n",
    "\n",
    "    # Add equal weight labels\n",
    "    edge_labels = {(1, 2): '1/3', (1, 3): '1/3', (1, 4): '1/3'}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels,\n",
    "                                font_size=12, ax=ax1)\n",
    "\n",
    "    # Draw attention-based aggregation\n",
    "    ax2.set_title(\"Attention-Based Aggregation\", fontsize=14)\n",
    "    nx.draw_networkx_nodes(G, pos, ax=ax2, node_size=500, node_color='lightblue')\n",
    "    nx.draw_networkx_labels(G, pos, ax=ax2, font_size=12)\n",
    "\n",
    "    # Draw edges with different widths to represent attention\n",
    "    nx.draw_networkx_edges(G, pos, ax=ax2,\n",
    "                         width=[4.0, 2.0, 1.0],\n",
    "                         edge_color=['red', 'blue', 'gray'],\n",
    "                         alpha=0.7, arrowsize=15)\n",
    "\n",
    "    # Add attention weight labels\n",
    "    edge_labels = {(1, 2): '0.6', (1, 3): '0.3', (1, 4): '0.1'}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels,\n",
    "                                font_size=12, ax=ax2)\n",
    "\n",
    "    # Add explanation text\n",
    "    ax1.text(0.5, -0.1, \"All neighbors contribute equally\\nto the central node's update\",\n",
    "             ha='center', va='center', transform=ax1.transAxes, fontsize=12)\n",
    "\n",
    "    ax2.text(0.5, -0.1, \"Neighbors contribute based on\\nlearned importance (attention weights)\",\n",
    "             ha='center', va='center', transform=ax2.transAxes, fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "create_difference_visualization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SRy69Zek1Gg"
   },
   "source": [
    "## 6. Implementing a Graph Attention Network (GAT) <a name=\"implement-gat\"></a>\n",
    "\n",
    "Now let's implement a full GAT model using PyTorch Geometric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nwgVOrl8g6oQ",
    "outputId": "88af14e9-9de7-48ef-cf99-3d05878f9986"
   },
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom GAT layer for demonstration and visualization\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input features per node.\n",
    "        out_channels (int): Number of output features per node.\n",
    "        heads (int): Number of attention heads.\n",
    "        dropout (float): Dropout rate.\n",
    "        concat (bool): Whether to concatenate the attention heads.\n",
    "        add_self_loops (bool): Whether to add self-loops to the edge index.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, heads=1, dropout=0.0, concat=True, add_self_loops=True):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.gat = GATConv(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            heads=heads,\n",
    "            dropout=dropout,\n",
    "            concat=concat,\n",
    "            add_self_loops=add_self_loops\n",
    "        )\n",
    "        self.heads = heads\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def forward(self, x, edge_index, return_attention=False):\n",
    "        # For visualization purposes, we'll use the GATConv's return_attention_weights\n",
    "        if return_attention:\n",
    "            out, attention_weights = self.gat(x.float(), edge_index, return_attention_weights=True)\n",
    "            return out, attention_weights\n",
    "        else:\n",
    "            return self.gat(x.float(), edge_index)\n",
    "\n",
    "class SimpleGAT(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple GAT model with the ability to return attention weights\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input features per node.\n",
    "        hidden_channels (int): Number of hidden features per node.\n",
    "        out_channels (int): Number of output features per node.\n",
    "        heads (int): Number of attention heads.\n",
    "        dropout (float): Dropout rate.\n",
    "        concat (bool): Whether to concatenate the attention heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=1, dropout=0.0, concat=True):\n",
    "            super(SimpleGAT, self).__init__()\n",
    "            self.dropout = dropout\n",
    "            \n",
    "            # First GAT layer\n",
    "            self.conv1 = GATLayer(in_channels, hidden_channels, heads=heads, dropout=dropout, concat=concat)\n",
    "            \n",
    "            # Second GAT layer - adjust input channels based on concatenation\n",
    "            conv2_in_channels = hidden_channels * heads if concat else hidden_channels\n",
    "            self.conv2 = GATLayer(conv2_in_channels, out_channels, heads=1, dropout=dropout, concat=concat)\n",
    "\n",
    "    def forward(self, x, edge_index, return_attention=False):\n",
    "        # For the first layer\n",
    "        if return_attention:\n",
    "            x, attention_weights_1 = self.conv1(x, edge_index, return_attention=True)\n",
    "        else:\n",
    "            x = self.conv1(x, edge_index)\n",
    "\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # For the second layer\n",
    "        if return_attention:\n",
    "            x, attention_weights_2 = self.conv2(x, edge_index, return_attention=True)\n",
    "            return x, (attention_weights_1, attention_weights_2)\n",
    "        else:\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return x\n",
    "\n",
    "# For molecular data, we'll use a more appropriate model\n",
    "class MolecularGAT(nn.Module):\n",
    "    \"\"\"GAT model designed for molecular property prediction\n",
    "    \n",
    "    Args:\n",
    "        in_channels (int): Number of input features per node.\n",
    "        hidden_channels (int): Number of hidden features per node.\n",
    "        out_channels (int): Number of output features per node.\n",
    "        heads (int): Number of attention heads.\n",
    "        dropout (float): Dropout rate.\n",
    "        concat (bool): Whether to concatenate the attention heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=1, dropout=0.1, concat=True):\n",
    "        super(MolecularGAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # First GAT layer with multi-head attention\n",
    "        self.gat1 = GATLayer(in_channels, hidden_channels, heads=heads, dropout=dropout, concat=concat)\n",
    "        # Second GAT layer\n",
    "        conv2_in_channels = hidden_channels * heads if concat else hidden_channels\n",
    "        self.gat2 = GATLayer(conv2_in_channels, hidden_channels, heads=1, dropout=dropout, concat=concat)\n",
    "        # Output layer\n",
    "        self.lin = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # First GAT layer\n",
    "        x = F.elu(self.gat1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Second GAT layer\n",
    "        x = F.elu(self.gat2(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Global pooling\n",
    "        x = torch.mean(x, dim=0)\n",
    "\n",
    "        # Final linear layer\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Initialize a simple model for our aspirin molecule\n",
    "in_channels = aspirin_data.x.size(1)  # Number of input features\n",
    "hidden_channels = 8\n",
    "out_channels = 8\n",
    "\n",
    "# Try with single head attention\n",
    "single_head_model = SimpleGAT(in_channels, hidden_channels, out_channels, heads=1)\n",
    "# Try with multi-head attention\n",
    "multi_head_model = SimpleGAT(in_channels, hidden_channels, out_channels, heads=4)\n",
    "\n",
    "# Run both models\n",
    "single_head_out = single_head_model(aspirin_data.x, aspirin_data.edge_index)\n",
    "multi_head_out = multi_head_model(aspirin_data.x, aspirin_data.edge_index)\n",
    "\n",
    "print(\"Output with single head attention:\", single_head_out.shape)\n",
    "print(\"Output with multi-head attention:\", multi_head_out.shape)\n",
    "\n",
    "\n",
    "# Get outputs and attention weights from the multi_head model\n",
    "x, attention_weights = multi_head_model(aspirin_data.x, aspirin_data.edge_index, return_attention=True)\n",
    "\n",
    "# Unpack attention weights for each layer\n",
    "attention_weights_layer1, attention_weights_layer2 = attention_weights\n",
    "\n",
    "# Each attention weight is a tuple with (edge_index, attention_values)\n",
    "edge_index_layer1, attn_values_layer1 = attention_weights_layer1\n",
    "edge_index_layer2, attn_values_layer2 = attention_weights_layer2\n",
    "\n",
    "print(\"Layer 1 attention shape:\", attn_values_layer1.shape)\n",
    "print(\"Layer 2 attention shape:\", attn_values_layer2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TLqDt3dk73U"
   },
   "source": [
    "## 7. Understanding Multi-Head Attention <a name=\"understand-multi-head\"></a>\n",
    "\n",
    "Let's visualize and understand the importance of multi-head attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Qp7t60-4k5uA",
    "outputId": "b848d553-254a-44ac-b628-23e83ff7607a"
   },
   "outputs": [],
   "source": [
    "def visualize_multi_head_attention(data, heads=4, hidden_dim=8, add_self_loops=True):\n",
    "    \"\"\"\n",
    "    Visualize multi-head attention on a molecular graph\n",
    "\n",
    "    Args:\n",
    "        data (torch_geometric.data.Data): The input data object containing the molecular graph.\n",
    "        heads (int): The number of attention heads to visualize.\n",
    "        hidden_dim (int): The dimension of the hidden features.\n",
    "        add_self_loops (bool): Whether to add self-loops to the edge index. Default is True.\n",
    "    \"\"\"\n",
    "    # Create a custom GAT layer that will return attention weights\n",
    "    gat_layer = GATLayer(data.x.size(1), hidden_dim, heads=heads, add_self_loops=add_self_loops)\n",
    "\n",
    "    # Forward pass with attention weights\n",
    "    _, attention_weights = gat_layer(data.x, data.edge_index, return_attention=True)\n",
    "\n",
    "    # Extract source, target, and attention weights\n",
    "    edge_index, att_weights = attention_weights\n",
    "\n",
    "    # Reshape attention weights for visualization\n",
    "    # att_weights is of shape [num_edges, num_heads]\n",
    "    num_edges = edge_index.size(1)\n",
    "\n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(1, heads, figsize=(heads * 5, 5))\n",
    "    if heads == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # Get molecule for visualization\n",
    "    mol = Chem.MolFromSmiles(data.smiles)\n",
    "    AllChem.Compute2DCoords(mol)\n",
    "\n",
    "    # Get the 2D coordinates from RDKit\n",
    "    pos = {}\n",
    "    for i, atom in enumerate(mol.GetAtoms()):\n",
    "        pos[i] = mol.GetConformer().GetAtomPosition(i)\n",
    "        pos[i] = (pos[i].x, -pos[i].y)\n",
    "\n",
    "    # Get atom labels\n",
    "    atom_labels = {i: atom.GetSymbol() for i, atom in enumerate(mol.GetAtoms())}\n",
    "\n",
    "    # Create NetworkX graph\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(range(data.x.size(0)))\n",
    "\n",
    "    # Visualize each attention head\n",
    "    for h in range(heads):\n",
    "        ax = axes[h]\n",
    "\n",
    "        # Get attention weights for this head\n",
    "        head_weights = att_weights[:, h].detach().numpy()\n",
    "\n",
    "        # Create edge list with attention weights for this head\n",
    "        edges = []\n",
    "        weights = []\n",
    "        for i in range(num_edges):\n",
    "            src, dst = edge_index[0, i].item(), edge_index[1, i].item()\n",
    "            weight = head_weights[i]\n",
    "            edges.append((src, dst))\n",
    "            weights.append(weight)\n",
    "\n",
    "        # Normalize weights for visualization\n",
    "        min_width = 0.5\n",
    "        max_width = 4.0\n",
    "        if len(weights) > 0:  # Ensure there are edges\n",
    "            norm_weights = [min_width + (w - min(weights)) * (max_width - min_width) /\n",
    "                            (max(weights) - min(weights) + 1e-6) for w in weights]\n",
    "        else:\n",
    "            norm_weights = []\n",
    "\n",
    "        # Draw nodes\n",
    "        nx.draw_networkx_nodes(G, pos,\n",
    "                              node_color='lightblue',\n",
    "                              node_size=500,\n",
    "                              ax=ax)\n",
    "\n",
    "        # Draw labels\n",
    "        nx.draw_networkx_labels(G, pos, labels=atom_labels, font_size=10, ax=ax)\n",
    "\n",
    "        # Draw edges with attention weights\n",
    "        if edges:  # Ensure there are edges\n",
    "            nx.draw_networkx_edges(G, pos,\n",
    "                                 edgelist=edges,\n",
    "                                 width=norm_weights,\n",
    "                                 edge_color='gray',\n",
    "                                 alpha=0.7,\n",
    "                                 arrowsize=10,\n",
    "                                 ax=ax,\n",
    "                                 node_size=500,\n",
    "                                 connectionstyle='arc3,rad=0.1')\n",
    "\n",
    "        ax.set_title(f\"Attention Head {h+1}\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.suptitle(f\"Multi-Head Attention ({heads} heads) on Molecular Graph\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize multi-head attention on aspirin\n",
    "visualize_multi_head_attention(aspirin_data, heads=4, add_self_loops=True)\n",
    "\n",
    "# Create a visual explanation of why multi-head attention is important\n",
    "def explain_multi_head_advantages():\n",
    "    \"\"\"Create a visual explanation of why multi-head attention is beneficial\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "    # Create a simple molecule for demonstration\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(1, 7))  # 6 nodes representing atoms\n",
    "\n",
    "    # Add edges\n",
    "    edges = [(1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 1)]\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    # Define positions for drawing (hexagon layout)\n",
    "    pos = {\n",
    "        1: (0, 1),\n",
    "        2: (0.866, 0.5),\n",
    "        3: (0.866, -0.5),\n",
    "        4: (0, -1),\n",
    "        5: (-0.866, -0.5),\n",
    "        6: (-0.866, 0.5)\n",
    "    }\n",
    "\n",
    "    # Single head attention\n",
    "    ax1.set_title(\"Single Head Attention\", fontsize=14)\n",
    "\n",
    "    # Draw nodes and labels\n",
    "    nx.draw_networkx_nodes(G, pos, ax=ax1, node_size=500, node_color='lightblue')\n",
    "    nx.draw_networkx_labels(G, pos, ax=ax1, font_size=12)\n",
    "\n",
    "    # Draw edges with varying widths to represent attention\n",
    "    edge_widths = [4, 3, 2, 1, 2, 3]  # Attention weights for visualization\n",
    "    nx.draw_networkx_edges(G, pos, ax=ax1,\n",
    "                         width=edge_widths,\n",
    "                         alpha=0.7,\n",
    "                         edge_color='gray')\n",
    "\n",
    "    # Add annotation to explain the limitation\n",
    "    ax1.text(0, -1.5, \"Single attention mechanism must divide\\nits focus across all relationships\",\n",
    "             ha='center', fontsize=12)\n",
    "\n",
    "    # Multi-head attention\n",
    "    ax2.set_title(\"Multi-Head Attention\", fontsize=14)\n",
    "\n",
    "    # Draw nodes and labels\n",
    "    nx.draw_networkx_nodes(G, pos, ax=ax2, node_size=500, node_color='lightblue')\n",
    "    nx.draw_networkx_labels(G, pos, ax=ax2, font_size=12)\n",
    "\n",
    "    # Create distinct edge styles for different attention heads\n",
    "    # Head 1: Focus on bonds 1-2, 4-5\n",
    "    head1_edges = [(1, 2), (4, 5)]\n",
    "    nx.draw_networkx_edges(G, pos, ax=ax2,\n",
    "                         edgelist=head1_edges,\n",
    "                         width=3.0,\n",
    "                         alpha=0.8,\n",
    "                         edge_color='red',\n",
    "                         style='solid',\n",
    "                         label='Head 1')\n",
    "\n",
    "    # Head 2: Focus on bonds 2-3, 5-6\n",
    "    head2_edges = [(2, 3), (5, 6)]\n",
    "    nx.draw_networkx_edges(G, pos, ax=ax2,\n",
    "                         edgelist=head2_edges,\n",
    "                         width=3.0,\n",
    "                         alpha=0.8,\n",
    "                         edge_color='blue',\n",
    "                         style='dashed',\n",
    "                         label='Head 2')\n",
    "\n",
    "    # Head 3: Focus on bonds 3-4, 6-1\n",
    "    head3_edges = [(3, 4), (6, 1)]\n",
    "    nx.draw_networkx_edges(G, pos, ax=ax2,\n",
    "                         edgelist=head3_edges,\n",
    "                         width=3.0,\n",
    "                         alpha=0.8,\n",
    "                         edge_color='green',\n",
    "                         style='dotted',\n",
    "                         label='Head 3')\n",
    "\n",
    "    ax2.legend(fontsize=10)\n",
    "\n",
    "    # Add annotation to explain the advantage\n",
    "    ax2.text(0, -1.5, \"Multiple attention heads can specialize\\nin different chemical relationships simultaneously\",\n",
    "             ha='center', fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "explain_multi_head_advantages()\n",
    "\n",
    "# Create a more technical explanation with chemical context\n",
    "def explain_multi_head_chemistry_context():\n",
    "    \"\"\"Explain the importance of multi-head attention in molecular contexts\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Create a text explanation with diagram\n",
    "    explanation = \"\"\"\n",
    "    # Why Multi-Head Attention is Critical for Molecular Graphs\n",
    "\n",
    "    ## Different Attention Heads Can Capture:\n",
    "\n",
    "    1. **Functional Group Interactions**\n",
    "       - Head 1: Focus on hydrogen bonding patterns\n",
    "       - Head 2: Focus on π-π stacking interactions\n",
    "       - Head 3: Focus on hydrophobic interactions\n",
    "\n",
    "    2. **Multi-Scale Chemical Properties**\n",
    "       - Head 1: Local atomic environment (1-2 bonds)\n",
    "       - Head 2: Medium-range effects (3-4 bonds)\n",
    "       - Head 3: Global molecular shape and electron distribution\n",
    "\n",
    "    3. **Different Chemical Contexts**\n",
    "       - Head 1: Identify aromatic ring systems\n",
    "       - Head 2: Detect hydrogen bond donors/acceptors\n",
    "       - Head 3: Recognize electronegativity patterns\n",
    "\n",
    "    ## Benefits in Drug Discovery Applications:\n",
    "\n",
    "    - More comprehensive feature detection\n",
    "    - Better handling of complex structure-activity relationships\n",
    "    - Improved generalization to new molecules\n",
    "    - Enhanced interpretability of important substructures\n",
    "    \"\"\"\n",
    "\n",
    "    plt.text(0.1, 0.1, explanation, fontsize=14,\n",
    "             verticalalignment='bottom', horizontalalignment='left',\n",
    "             family='monospace')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "explain_multi_head_chemistry_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FX2RxJQlOd2"
   },
   "source": [
    "## 8. Implementing a Complete GAT Model for Molecular Property Prediction <a name=\"implement-gat-property-prediction\"></a>\n",
    "\n",
    "Now let's implement a complete GAT model for molecular property prediction and train it on a real dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "og6QrfkZlCwl",
    "outputId": "37505937-0dad-419e-801d-133a12ac6a77"
   },
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# 1. Load and Preprocess the Dataset\n",
    "# ===================================================\n",
    "# Load a dataset from MoleculeNet for training\n",
    "print(\"Loading ESOL dataset (water solubility data)...\")\n",
    "dataset = MoleculeNet(root='data', name='ESOL')\n",
    "print(f\"Dataset loaded: {len(dataset)} molecules\")\n",
    "\n",
    "# Split the dataset\n",
    "torch.manual_seed(42)\n",
    "indices = torch.randperm(len(dataset))\n",
    "train_idx = indices[:int(0.8 * len(dataset))]\n",
    "val_idx = indices[int(0.8 * len(dataset)):int(0.9 * len(dataset))]\n",
    "test_idx = indices[int(0.9 * len(dataset)):]\n",
    "\n",
    "train_dataset = dataset[train_idx]\n",
    "val_dataset = dataset[val_idx]\n",
    "test_dataset = dataset[test_idx]\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Validation: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "# Create data loaders\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# ===================================================\n",
    "# 2. Define GAT Model for Regression\n",
    "# ===================================================\n",
    "\n",
    "# Define our GAT model for regression\n",
    "class MolecularGATForRegression(nn.Module):\n",
    "    \"\"\"GAT model designed for molecular property prediction\n",
    "    \n",
    "    Args:\n",
    "        in_channels (int): Number of input features per node.\n",
    "        hidden_channels (int): Number of hidden features per node.\n",
    "        out_channels (int): Number of output features per node.\n",
    "        heads (int): Number of attention heads.\n",
    "        dropout (float): Dropout rate.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels=64, out_channels=1, heads=4, dropout=0.2):\n",
    "        super(MolecularGATForRegression, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # First GAT layer\n",
    "        self.gat1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout)\n",
    "        \n",
    "        # Second GAT layer\n",
    "        self.gat2 = GATConv(hidden_channels * heads, hidden_channels * 2, heads=2, dropout=dropout)\n",
    "        \n",
    "        # Calculate the input dimension for lin1 based on pooling operations\n",
    "        pool_out_dim = hidden_channels * 4 * 3  # 3 pooling operations\n",
    "        self.lin1 = nn.Linear(pool_out_dim, hidden_channels)\n",
    "        self.lin2 = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # First GAT layer\n",
    "        x = F.elu(self.gat1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Second GAT layer\n",
    "        x = F.elu(self.gat2(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Global pooling\n",
    "        x = torch.cat([\n",
    "            global_add_pool(x, batch),\n",
    "            global_mean_pool(x, batch),\n",
    "            global_max_pool(x, batch)\n",
    "        ], dim=1)\n",
    "\n",
    "        # Output MLP\n",
    "        x = F.elu(self.lin1(x))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Need to import pooling functions\n",
    "from torch_geometric.nn import global_add_pool, global_mean_pool, global_max_pool\n",
    "\n",
    "# ===================================================\n",
    "# 3. Initialize Model and Optimizer\n",
    "# ===================================================\n",
    "\n",
    "# Check the input feature dimensions from the dataset\n",
    "sample_data = dataset[0]\n",
    "print(f\"Node features: {sample_data.x.shape}\")\n",
    "\n",
    "# Initialize model\n",
    "in_channels = sample_data.x.shape[1]\n",
    "model = MolecularGATForRegression(in_channels, hidden_channels=32, heads=4)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x.float(), data.edge_index, data.batch)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(loader):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset loader.\n",
    "\n",
    "    Returns:\n",
    "        - average MSE loss\n",
    "        - predicted values\n",
    "        - ground truth values\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    pred_list, true_list = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x.float(), data.edge_index, data.batch)\n",
    "            loss = criterion(out, data.y)\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "\n",
    "            pred_list.append(out.cpu())\n",
    "            true_list.append(data.y.cpu())\n",
    "\n",
    "    preds = torch.cat(pred_list, dim=0)\n",
    "    targets = torch.cat(true_list, dim=0)\n",
    "\n",
    "    return total_loss / len(loader.dataset), preds, targets\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the GAT model...\")\n",
    "train_losses, val_losses = [], []\n",
    "best_val_loss = float('inf')\n",
    "patience, patience_counter = 10, 0\n",
    "\n",
    "for epoch in range(1, 101):  # 100 epochs\n",
    "    train_loss = train()\n",
    "    val_loss, _, _ = evaluate(val_loader)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_gat_model.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "    print(f\"Epoch: {epoch:02d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Load the best model and evaluate on test set\n",
    "model.load_state_dict(torch.load('best_gat_model.pt'))\n",
    "test_loss, test_preds, test_targets = evaluate(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Visualize the training process\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot predictions vs actual values\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(test_targets.numpy(), test_preds.numpy(), alpha=0.5)\n",
    "plt.plot([-10, 2], [-10, 2], 'r--')  # Perfect prediction line\n",
    "plt.xlabel('Actual log(Solubility)')\n",
    "plt.ylabel('Predicted log(Solubility)')\n",
    "plt.title('GAT Model Predictions on Test Set')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6M4fXWIsvQm"
   },
   "source": [
    "## 9. Visualizing Attention Weights in Molecules <a name=\"visualize-attention\"></a>\n",
    "\n",
    "Now let's visualize the attention weights on some test molecules to understand what the model has learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ud-jbtGmlUAK",
    "outputId": "ccc3f752-97fe-4592-b832-1992695378d2"
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def visualize_model_attention(model, data, smiles, num_heads=4, use_gpu=False):\n",
    "    \"\"\"\n",
    "    Visualize what the model is attending to\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to visualize.\n",
    "        data (torch_geometric.data.Data): The input data object containing the molecular graph.\n",
    "        smiles (str): The SMILES string of the molecule to visualize.\n",
    "        num_heads (int): The number of attention heads to visualize.\n",
    "        use_gpu (bool): Whether to use GPU for visualization.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Set the device\n",
    "    viz_device = torch.device('cuda' if torch.cuda.is_available() and use_gpu else 'cpu')\n",
    "    model = model.to(viz_device)\n",
    "\n",
    "    # Move data to the correct device\n",
    "    x = data.x.to(viz_device)\n",
    "    edge_index = data.edge_index.to(viz_device)\n",
    "\n",
    "    # Create a fake batch index (all zeros since we have only one graph)\n",
    "    batch = torch.zeros(x.size(0), dtype=torch.long, device=viz_device)\n",
    "\n",
    "    # Forward pass with attention weights\n",
    "    with torch.no_grad():\n",
    "        # Check if the model has the expected return_attention parameter\n",
    "        try:\n",
    "            forward_params = list(inspect.getfullargspec(model.forward).args)\n",
    "            has_return_attention = 'return_attention' in forward_params\n",
    "        except:\n",
    "            has_return_attention = False\n",
    "\n",
    "        if has_return_attention:\n",
    "            # This works for our SimpleGAT model\n",
    "            _, attention_weights = model(x, edge_index, return_attention=True)\n",
    "            layer1_attn, layer2_attn = attention_weights\n",
    "        else:\n",
    "            # Fall back to hook-based method for other models\n",
    "            attention_weights = []\n",
    "\n",
    "            def hook_fn(module, input, output):\n",
    "                # The attention weights are the second element of the tuple\n",
    "                # if return_attention_weights=True\n",
    "                if isinstance(output, tuple) and len(output) == 2:\n",
    "                    attention_weights.append(output[1])\n",
    "\n",
    "            # Register the hook on GAT layers\n",
    "            hooks = []\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, GATConv) or isinstance(module, GATLayer):\n",
    "                    hooks.append(module.register_forward_hook(hook_fn))\n",
    "\n",
    "            # Forward pass\n",
    "            _ = model(x, edge_index, batch)\n",
    "\n",
    "            # Remove the hooks\n",
    "            for hook in hooks:\n",
    "                hook.remove()\n",
    "\n",
    "            if not attention_weights:\n",
    "                print(\"No attention weights were captured. Check the model architecture.\")\n",
    "                return\n",
    "\n",
    "            # Use the first layer's attention\n",
    "            layer1_attn = attention_weights[0]\n",
    "            layer2_attn = attention_weights[1] if len(attention_weights) > 1 else None\n",
    "\n",
    "    # Get the attention weights from the first layer\n",
    "    edge_index, att_weights = layer1_attn\n",
    "\n",
    "    # Move tensors to CPU for visualization\n",
    "    edge_index = edge_index.cpu()\n",
    "    att_weights = att_weights.cpu()\n",
    "\n",
    "    # Determine number of heads to visualize (could be 1 for single-head attention)\n",
    "    if len(att_weights.shape) > 1:\n",
    "        num_heads = min(att_weights.size(1), num_heads)\n",
    "    else:\n",
    "        num_heads = 1\n",
    "        # Reshape for consistent processing\n",
    "        att_weights = att_weights.unsqueeze(1)\n",
    "\n",
    "    # Create molecule for visualization\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "    # Calculate 2D coordinates if they don't exist\n",
    "    if mol.GetNumConformers() == 0:\n",
    "        AllChem.Compute2DCoords(mol)\n",
    "\n",
    "    # Get positions for drawing\n",
    "    pos = {}\n",
    "    for i, atom in enumerate(mol.GetAtoms()):\n",
    "        position = mol.GetConformer().GetAtomPosition(i)\n",
    "        pos[i] = (position.x, -position.y)  # Flip y for better visualization\n",
    "\n",
    "    # Create NetworkX graph\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(range(data.x.size(0)))\n",
    "\n",
    "    # Get atom labels\n",
    "    atom_labels = {i: atom.GetSymbol() for i, atom in enumerate(mol.GetAtoms())}\n",
    "\n",
    "    # Visualize each attention head\n",
    "    fig, axes = plt.subplots(1, num_heads, figsize=(num_heads * 5, 5))\n",
    "    if num_heads == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # Visualize each attention head\n",
    "    for h in range(num_heads):\n",
    "        ax = axes[h]\n",
    "\n",
    "        # Get attention weights for this head\n",
    "        if num_heads > 1:\n",
    "            head_weights = att_weights[:, h].detach().numpy()\n",
    "        else:\n",
    "            head_weights = att_weights.squeeze().detach().numpy()\n",
    "\n",
    "        # Create edge list with attention weights\n",
    "        edges = []\n",
    "        weights = []\n",
    "        for i in range(edge_index.size(1)):\n",
    "            src, dst = edge_index[0, i].item(), edge_index[1, i].item()\n",
    "            weight = head_weights[i] if i < len(head_weights) else 0.0\n",
    "            edges.append((src, dst))\n",
    "            weights.append(weight)\n",
    "\n",
    "        # Normalize weights for visualization\n",
    "        if weights:\n",
    "            min_width = 0.5\n",
    "            max_width = 4.0\n",
    "            min_weight = min(weights)\n",
    "            max_weight = max(weights)\n",
    "            range_weight = max_weight - min_weight\n",
    "            if range_weight > 1e-6:\n",
    "                norm_weights = [(w - min_weight) * (max_width - min_width) / range_weight + min_width for w in weights]\n",
    "            else:\n",
    "                norm_weights = [min_width for _ in weights]\n",
    "        else:\n",
    "            norm_weights = []\n",
    "\n",
    "        # Draw nodes\n",
    "        nx.draw_networkx_nodes(G, pos,\n",
    "                              node_color='lightblue',\n",
    "                              node_size=500,\n",
    "                              ax=ax)\n",
    "\n",
    "        # Draw labels\n",
    "        nx.draw_networkx_labels(G, pos, labels=atom_labels, font_size=10, ax=ax)\n",
    "\n",
    "        # Draw edges with attention weights\n",
    "        if edges:\n",
    "            cmap = plt.cm.viridis\n",
    "            edges_collection = nx.draw_networkx_edges(G, pos,\n",
    "                                                    edgelist=edges,\n",
    "                                                    width=norm_weights,\n",
    "                                                    edge_color=weights,\n",
    "                                                    edge_cmap=cmap,\n",
    "                                                    alpha=0.7,\n",
    "                                                    arrowsize=10,\n",
    "                                                    ax=ax,\n",
    "                                                    node_size=500,\n",
    "                                                    connectionstyle='arc3,rad=0.1')  # Curved edges\n",
    "\n",
    "            # Add a colorbar\n",
    "            sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=min(weights), vmax=max(weights)))\n",
    "            sm.set_array([])\n",
    "            cbar = plt.colorbar(sm, ax=ax, shrink=0.7)\n",
    "            cbar.set_label('Attention Weight')\n",
    "\n",
    "        ax.set_title(f\"Attention Head {h+1}\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Add molecule image in an inset\n",
    "    if num_heads > 0:\n",
    "        img_ax = fig.add_axes([0.01, 0.65, 0.2, 0.3])\n",
    "        img = Draw.MolToImage(mol, size=(300, 300))\n",
    "        img_ax.imshow(img)\n",
    "        img_ax.axis('off')\n",
    "\n",
    "    model_name = model.__class__.__name__\n",
    "    plt.suptitle(f\"{model_name}'s Attention Weights on {Chem.MolToSmiles(mol, isomericSmiles=False)}\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Also visualize second layer if available\n",
    "    if layer2_attn is not None:\n",
    "        edge_index2, att_weights2 = layer2_attn\n",
    "        edge_index2 = edge_index2.cpu()\n",
    "        att_weights2 = att_weights2.cpu()\n",
    "\n",
    "        # Determine number of heads for second layer\n",
    "        if len(att_weights2.shape) > 1:\n",
    "            num_heads2 = min(att_weights2.size(1), num_heads)\n",
    "        else:\n",
    "            num_heads2 = 1\n",
    "            att_weights2 = att_weights2.unsqueeze(1)\n",
    "\n",
    "        # Create second layer visualization\n",
    "        fig2, axes2 = plt.subplots(1, num_heads2, figsize=(num_heads2 * 5, 5))\n",
    "        if num_heads2 == 1:\n",
    "            axes2 = [axes2]\n",
    "\n",
    "        # Similar visualization code for second layer...\n",
    "        for h in range(num_heads2):\n",
    "            ax = axes2[h]\n",
    "\n",
    "            # Get attention weights for this head in layer 2\n",
    "            if num_heads2 > 1:\n",
    "                head_weights = att_weights2[:, h].detach().numpy()\n",
    "            else:\n",
    "                head_weights = att_weights2.squeeze().detach().numpy()\n",
    "\n",
    "            # Create edge list with attention weights\n",
    "            edges = []\n",
    "            weights = []\n",
    "            for i in range(edge_index2.size(1)):\n",
    "                src, dst = edge_index2[0, i].item(), edge_index2[1, i].item()\n",
    "                weight = head_weights[i] if i < len(head_weights) else 0.0\n",
    "                edges.append((src, dst))\n",
    "                weights.append(weight)\n",
    "\n",
    "            # Normalize weights for visualization\n",
    "            if weights:\n",
    "                min_width = 0.5\n",
    "                max_width = 4.0\n",
    "                min_weight = min(weights)\n",
    "                max_weight = max(weights)\n",
    "                range_weight = max_weight - min_weight\n",
    "                if range_weight > 1e-6:\n",
    "                    norm_weights = [(w - min_weight) * (max_width - min_width) / range_weight + min_width for w in weights]\n",
    "                else:\n",
    "                    norm_weights = [min_width for _ in weights]\n",
    "            else:\n",
    "                norm_weights = []\n",
    "\n",
    "            # Draw nodes\n",
    "            nx.draw_networkx_nodes(G, pos,\n",
    "                                  node_color='lightpink',  # Different color for layer 2\n",
    "                                  node_size=500,\n",
    "                                  ax=ax)\n",
    "\n",
    "            # Draw labels\n",
    "            nx.draw_networkx_labels(G, pos, labels=atom_labels, font_size=10, ax=ax)\n",
    "\n",
    "            # Draw edges with attention weights\n",
    "            if edges:\n",
    "                cmap = plt.cm.plasma  # Different colormap for layer 2\n",
    "                edges_collection = nx.draw_networkx_edges(G, pos,\n",
    "                                                        edgelist=edges,\n",
    "                                                        width=norm_weights,\n",
    "                                                        edge_color=weights,\n",
    "                                                        edge_cmap=cmap,\n",
    "                                                        alpha=0.7,\n",
    "                                                        arrowsize=10,\n",
    "                                                        ax=ax,\n",
    "                                                        node_size=500,\n",
    "                                                        connectionstyle='arc3,rad=0.1')\n",
    "\n",
    "                # Add a colorbar\n",
    "                sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=min(weights), vmax=max(weights)))\n",
    "                sm.set_array([])\n",
    "                cbar = plt.colorbar(sm, ax=ax, shrink=0.7)\n",
    "                cbar.set_label('Attention Weight')\n",
    "\n",
    "            ax.set_title(f\"Layer 2 - Attention Head {h+1}\")\n",
    "            ax.axis('off')\n",
    "\n",
    "        # Add molecule image in an inset for layer 2 visualization\n",
    "        if num_heads2 > 0:\n",
    "            img_ax = fig2.add_axes([0.01, 0.65, 0.2, 0.3])\n",
    "            img = Draw.MolToImage(mol, size=(300, 300))\n",
    "            img_ax.imshow(img)\n",
    "            img_ax.axis('off')\n",
    "\n",
    "        plt.suptitle(f\"{model_name}'s Layer 2 Attention Weights\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Usage for a specific molecule\n",
    "def visualize_gat_on_molecule(smiles=\"CC(=O)Oc1ccccc1C(=O)O\", use_multi_head=True):\n",
    "    \"\"\"\n",
    "    Visualize attention weights for a specific molecule with single or multi-head attention\n",
    "    \n",
    "    Args:\n",
    "        smiles (str): The SMILES string of the molecule to visualize.\n",
    "        use_multi_head (bool): Whether to use multi-head attention.\n",
    "    \"\"\"\n",
    "    # Create molecule graph\n",
    "    import inspect\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import AllChem, Draw\n",
    "    import networkx as nx\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "    # Get molecule graph data\n",
    "    data = mol_to_graph(smiles)\n",
    "\n",
    "    # Get number of features\n",
    "    in_channels = data.x.size(1)\n",
    "    hidden_channels = 8\n",
    "    out_channels = 8\n",
    "\n",
    "    # Create GAT model - single or multi-head \n",
    "    if use_multi_head:\n",
    "        model = SimpleGAT(in_channels, hidden_channels, out_channels, heads=4)\n",
    "        print(\"Using multi-head attention (4 heads)\")\n",
    "    else:\n",
    "        model = SimpleGAT(in_channels, hidden_channels, out_channels, heads=1)\n",
    "        print(\"Using single-head attention\")\n",
    "\n",
    "    # Visualize attention weights\n",
    "    visualize_model_attention(model, data, smiles,\n",
    "                             num_heads=4 if use_multi_head else 1,\n",
    "                             use_gpu=False)\n",
    "\n",
    "    return model, data\n",
    "\n",
    "# Example usage - visualize for aspirin with both single and multi-head attention\n",
    "print(\"Visualizing attention for Aspirin:\")\n",
    "Aspirin = \"CC(=O)Oc1ccccc1C(=O)O\"\n",
    "aspirin_single_head_model, _= visualize_gat_on_molecule(Aspirin, use_multi_head=False)\n",
    "aspirin_multi_head_model, _ = visualize_gat_on_molecule(Aspirin, use_multi_head=True)\n",
    "\n",
    "# Try with other molecules\n",
    "paracetamol = \"CC(=O)Nc1ccc(O)cc1\"\n",
    "ibuprofen = \"CC(C)Cc1ccc(C(C)C(=O)O)cc1\"\n",
    "\n",
    "print(\"\\nVisualizing attention for Paracetamol:\")\n",
    "visualize_gat_on_molecule(paracetamol, use_multi_head=True)\n",
    "\n",
    "print(\"\\nVisualizing attention for Ibuprofen:\")\n",
    "visualize_gat_on_molecule(ibuprofen, use_multi_head=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teyREwBWs3Aj"
   },
   "source": [
    "## 10. Comparing Single-Head vs Multi-Head Attention Performance <a name=\"compare-single-multi-head\"></a>\n",
    "\n",
    "Let's train models with different numbers of attention heads and compare their performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "45qiYeVXs3QG",
    "outputId": "928a6d20-fdaf-42b8-cdc8-82923805e1aa"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, optimizer, criterion, train_loader, val_loader, test_loader, epochs, heads):\n",
    "    \"\"\"\n",
    "    Train a GAT model for molecular regression and evaluate its performance.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The GAT model instance.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "        criterion (nn.Module): Loss function (e.g., MSELoss).\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        val_loader (DataLoader): DataLoader for validation data.\n",
    "        test_loader (DataLoader): DataLoader for test data.\n",
    "        epochs (int): Number of training epochs.\n",
    "        heads (int): Number of attention heads in GAT layers.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing training/validation losses, final performance, and model.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Train\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x.float(), data.edge_index, data.batch)\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "\n",
    "        train_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                data = data.to(device)\n",
    "                out = model(data.x.float(), data.edge_index, data.batch)\n",
    "                loss = criterion(out, data.y)\n",
    "                val_loss += loss.item() * data.num_graphs\n",
    "\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x.float(), data.edge_index, data.batch)\n",
    "            loss = criterion(out, data.y)\n",
    "            test_loss += loss.item() * data.num_graphs\n",
    "\n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "\n",
    "    return {\n",
    "        'heads': heads,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'final_train_loss': train_losses[-1],\n",
    "        'final_val_loss': val_losses[-1],\n",
    "        'test_loss': test_loss,\n",
    "        'model': model,\n",
    "    }\n",
    "\n",
    "# Train models with different numbers of attention heads\n",
    "head_configs = [1, 2, 4, 8]\n",
    "model_results = []\n",
    "\n",
    "for heads in head_configs:\n",
    "    epochs = 30\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    # Initialize model\n",
    "    model = MolecularGATForRegression(in_channels, hidden_channels=32, heads=heads)\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "    print(f\"Training model with {heads} attention heads...\")\n",
    "    results = train_and_evaluate_model(model, optimizer, criterion, train_loader, val_loader, test_loader, epochs, heads)\n",
    "    model_results.append(results)\n",
    "    print(f\"Heads: {heads}, Test Loss: {results['test_loss']:.4f}\")\n",
    "\n",
    "# Visualize the comparison\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot training curves\n",
    "plt.subplot(2, 1, 1)\n",
    "for result in model_results:\n",
    "    plt.plot(result['train_losses'], label=f\"{result['heads']} Heads (Train)\")\n",
    "    plt.plot(result['val_losses'], linestyle='--', label=f\"{result['heads']} Heads (Val)\")\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training and Validation Loss for Different Numbers of Attention Heads')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot final test performance\n",
    "plt.subplot(2, 1, 2)\n",
    "heads = [result['heads'] for result in model_results]\n",
    "test_losses = [result['test_loss'] for result in model_results]\n",
    "\n",
    "bars = plt.bar(heads, test_losses)\n",
    "plt.xlabel('Number of Attention Heads')\n",
    "plt.ylabel('Test Loss (MSE)')\n",
    "plt.title('Test Performance vs. Number of Attention Heads')\n",
    "plt.xticks(heads)\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, test_losses):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0,\n",
    "             bar.get_height() + 0.005,\n",
    "             f'{val:.4f}',\n",
    "             ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-E7RduAZs8tV"
   },
   "source": [
    "## 11. Visualizing Feature Transformation Through the Network <a name=\"visualize-feature-transformation\"></a>\n",
    "\n",
    "Let's see how node features evolve through the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "c5FIa26os6Su",
    "lines_to_next_cell": 2,
    "outputId": "7d855cc1-eb1f-426a-879c-7eba34732edf"
   },
   "outputs": [],
   "source": [
    "def visualize_feature_transformation(model, data, smiles, use_gpu=False):\n",
    "    \"\"\"\n",
    "    Visualize how node features are transformed through the GAT layers\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to visualize.\n",
    "        data (torch_geometric.data.Data): The input data object containing the molecular graph.\n",
    "        smiles (str): The SMILES string of the molecule to visualize.\n",
    "        use_gpu (bool): Whether to use GPU for computation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model.eval()\n",
    "\n",
    "        # Set the device with error handling\n",
    "        try:\n",
    "            viz_device = torch.device('cuda' if torch.cuda.is_available() and use_gpu else 'cpu')\n",
    "            model = model.to(viz_device)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error setting device: {e}\")\n",
    "            viz_device = torch.device('cpu')\n",
    "            model = model.to(viz_device)\n",
    "\n",
    "        # Create hooks to get intermediate activations\n",
    "        activations = {}\n",
    "\n",
    "        def get_activation(name):\n",
    "            def hook(module, input, output):\n",
    "                if isinstance(output, tuple):\n",
    "                    activations[name] = output[0].detach()\n",
    "                else:\n",
    "                    activations[name] = output.detach()\n",
    "            return hook\n",
    "\n",
    "        # Find the GAT layers in the model\n",
    "        gat_layers = []\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, GATConv):\n",
    "                gat_layers.append((name, module))\n",
    "\n",
    "        if len(gat_layers) < 2:\n",
    "            print(\"Model needs at least 2 GAT layers for feature transformation visualization\")\n",
    "            return None\n",
    "\n",
    "        # Register hooks\n",
    "        hooks = []\n",
    "        for name, module in gat_layers[:2]:\n",
    "            hooks.append(module.register_forward_hook(get_activation(name)))\n",
    "\n",
    "        # Forward pass with error handling\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                x = data.x.to(viz_device)\n",
    "                edge_index = data.edge_index.to(viz_device)\n",
    "                batch = torch.zeros(x.size(0), dtype=torch.long, device=viz_device)\n",
    "                _ = model(x, edge_index, batch)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during forward pass: {e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            # Always remove hooks\n",
    "            for hook in hooks:\n",
    "                hook.remove()\n",
    "\n",
    "        # Get the input features and activations\n",
    "        input_features = data.x.cpu().numpy()\n",
    "        print(\"Input features: \", input_features.shape)\n",
    "\n",
    "        if len(activations) < 2:\n",
    "            print(\"Could not capture activations from both GAT layers\")\n",
    "            return None\n",
    "\n",
    "        # Get the activations\n",
    "        gat_names = list(activations.keys())\n",
    "        gat1_features = activations[gat_names[0]].cpu().numpy()\n",
    "        gat2_features = activations[gat_names[1]].cpu().numpy()\n",
    "\n",
    "        print(\"GAT1 features: \", gat1_features.shape)\n",
    "        print(\"GAT2 features: \", gat2_features.shape)\n",
    "\n",
    "        # Get the molecule and atom symbols\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            print(f\"Invalid SMILES string: {smiles}\")\n",
    "            return None\n",
    "\n",
    "        atom_symbols = [atom.GetSymbol() for atom in mol.GetAtoms()]\n",
    "        \n",
    "        # Create color map for different atom types\n",
    "        unique_symbols = list(set(atom_symbols))\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(unique_symbols)))\n",
    "        symbol_to_color = {sym: colors[i] for i, sym in enumerate(unique_symbols)}\n",
    "        node_colors = [symbol_to_color[sym] for sym in atom_symbols]\n",
    "\n",
    "        # Reduce dimensionality with error handling\n",
    "        try:\n",
    "            if input_features.shape[1] > 2:\n",
    "                pca = PCA(n_components=2)\n",
    "                input_pca = pca.fit_transform(input_features)\n",
    "            else:\n",
    "                input_pca = input_features\n",
    "\n",
    "            # Use t-SNE with error handling\n",
    "            perplexity = min(10, input_features.shape[0]-1)\n",
    "            if perplexity < 1:\n",
    "                print(\"Not enough samples for t-SNE\")\n",
    "                return None\n",
    "\n",
    "            tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
    "            gat1_tsne = tsne.fit_transform(gat1_features)\n",
    "            gat2_tsne = tsne.fit_transform(gat2_features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during dimensionality reduction: {e}\")\n",
    "            return None\n",
    "\n",
    "        # Plot the transformations\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "        # Plot input features\n",
    "        for i, (x, y) in enumerate(input_pca):\n",
    "            axes[0].scatter(x, y, s=100, alpha=0.8, color=node_colors[i])\n",
    "            axes[0].text(x, y, atom_symbols[i], ha='center', va='center')\n",
    "        axes[0].set_title('Input Features (PCA)')\n",
    "        axes[0].set_xlabel('PC1')\n",
    "        axes[0].set_ylabel('PC2')\n",
    "\n",
    "        # Plot features after first GAT layer\n",
    "        for i, (x, y) in enumerate(gat1_tsne):\n",
    "            axes[1].scatter(x, y, s=100, alpha=0.8, color=node_colors[i])\n",
    "            axes[1].text(x, y, atom_symbols[i], ha='center', va='center')\n",
    "        axes[1].set_title('Features After First GAT Layer (t-SNE)')\n",
    "        axes[1].set_xlabel('Dimension 1')\n",
    "        axes[1].set_ylabel('Dimension 2')\n",
    "\n",
    "        # Plot features after second GAT layer\n",
    "        for i, (x, y) in enumerate(gat2_tsne):\n",
    "            axes[2].scatter(x, y, s=100, alpha=0.8, color=node_colors[i])\n",
    "            axes[2].text(x, y, atom_symbols[i], ha='center', va='center')\n",
    "        axes[2].set_title('Features After Second GAT Layer (t-SNE)')\n",
    "        axes[2].set_xlabel('Dimension 1')\n",
    "        axes[2].set_ylabel('Dimension 2')\n",
    "\n",
    "        # Add legend for atom types\n",
    "        legend_elements = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                    markerfacecolor=symbol_to_color[sym], \n",
    "                                    label=sym, markersize=10) \n",
    "                         for sym in unique_symbols]\n",
    "        fig.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "        plt.suptitle(f'Feature Transformation Through GAT Layers for {smiles}', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in visualization: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create and train two small models for visualization\n",
    "def train_small_model_for_vis(heads=4):\n",
    "    \"\"\"\n",
    "    Create a small model and visualize feature transformations on a test molecule\n",
    "    \n",
    "    Args:\n",
    "        heads (int): The number of attention heads to use in the model.\n",
    "    \"\"\"\n",
    "    # Create a graph from the SMILES\n",
    "    sample = dataset[0]\n",
    "\n",
    "    # Get number of features\n",
    "    num_features = sample.x.size(1)\n",
    "\n",
    "    # Create a small GATv2 model for visualization\n",
    "    class SmallGAT(nn.Module):\n",
    "        \"\"\"GAT model designed for molecular property prediction\"\"\"\n",
    "        def __init__(self, in_channels, hidden_channels, out_channels, heads=4, dropout=0.1):\n",
    "            super(SmallGAT, self).__init__()\n",
    "            # First GAT layer with multi-head attention\n",
    "            self.gat1 = GATLayer(in_channels, hidden_channels, heads=heads, dropout=dropout)\n",
    "            # Second GAT layer, typically we use concat=False at the last layer\n",
    "            self.gat2 = GATLayer(hidden_channels * heads, hidden_channels, heads=1, dropout=dropout)\n",
    "            # Output layer\n",
    "            self.lin = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "        def forward(self, x, edge_index, batch=None):\n",
    "            # Apply first GAT layer with multi-head attention\n",
    "            x = F.elu(self.gat1(x, edge_index))\n",
    "            x = F.dropout(x, p=0.2, training=self.training)\n",
    "\n",
    "            # Apply second GAT layer\n",
    "            x = F.elu(self.gat2(x, edge_index))\n",
    "\n",
    "            # Global pooling (mean of all node features)\n",
    "            if batch is not None:\n",
    "                x = global_mean_pool(x, batch)\n",
    "            else:\n",
    "                x = torch.mean(x, dim=0)\n",
    "\n",
    "            # Final linear layer\n",
    "            x = self.lin(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    # Create the model\n",
    "    viz_model = SmallGAT(num_features, hidden_channels=8, out_channels=1, heads=heads, dropout=0.1)\n",
    "\n",
    "    epochs = 50\n",
    "    lr = 0.001\n",
    "    weight_decay = 5e-4\n",
    "    # Train briefly (just to get some reasonable parameters)\n",
    "    optimizer = torch.optim.Adam(viz_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    result = train_and_evaluate_model(viz_model, optimizer, criterion, train_loader, val_loader, test_loader, epochs, heads=heads)\n",
    "    return result['model']\n",
    "\n",
    "# Visualize feature transformations for one of our test molecules\n",
    "# Get models\n",
    "print(\"Training model with 4 heads..\")\n",
    "four_head_model = train_small_model_for_vis(heads=4)\n",
    "print(\"Training model with 8 heads..\")\n",
    "eight_head_model = train_small_model_for_vis(heads=8)\n",
    "\n",
    "# Example molecules\n",
    "example_molecules = {\n",
    "    \"Aspirin\": \"CC(=O)OC1=CC=CC=C1C(=O)O\",\n",
    "    \"Paracetamol\": \"CC(=O)NC1=CC=C(C=C1)O\",\n",
    "    \"Ibuprofen\": \"CC(C)CC1=CC=C(C=C1)C(C)C(=O)O\"\n",
    "}\n",
    "\n",
    "# Visualize feature evolution\n",
    "aspirin_smiles = example_molecules['Aspirin']\n",
    "aspirin_data = mol_to_graph(aspirin_smiles)\n",
    "print(f\"Aspirin data: {aspirin_data}\")\n",
    "\n",
    "print(\"Four head features: \")\n",
    "four_head_features = visualize_feature_transformation(four_head_model, aspirin_data, aspirin_smiles)\n",
    "\n",
    "print(\"Eight head features: \")\n",
    "eight_head_features = visualize_feature_transformation(eight_head_model, aspirin_data, aspirin_smiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVu33MvatDK2"
   },
   "source": [
    "## 12. Interactive Visualization of Attention Mechanism <a name=\"interactive-visualiztion\"></a>\n",
    "\n",
    "Let's create an interactive demonstration to understand how attention works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "4TCro08MtDtT",
    "outputId": "bf7c3a05-5625-4f0e-cb19-a4c9f7ae3ca7"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import base64\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "def create_attention_animation(smiles):\n",
    "    \"\"\"\n",
    "    Create an animation showing how attention weights propagate information\n",
    "    \n",
    "    Args:\n",
    "        smiles (str): The SMILES string of the molecule to visualize.\n",
    "\n",
    "    Returns:\n",
    "        ani (matplotlib.animation.FuncAnimation): The animation object.\n",
    "    \n",
    "    Raises:\n",
    "        Exception: If the molecule is not found or cannot be visualized.\n",
    "\n",
    "    Note:\n",
    "        This is just a simple example for demonstration purposes.\n",
    "        in reality, each step of the attention mechanism is computed in parallel,\n",
    "        and the animation is just a visualization of the attention weights.\n",
    "    \"\"\"\n",
    "    # Create molecule graph\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    AllChem.Compute2DCoords(mol)\n",
    "    data = mol_to_graph(smiles)\n",
    "\n",
    "    # Create a simplified attention layer\n",
    "    att_layer = SimpleAttentionLayer(data.x.size(1), data.x.size(1))\n",
    "\n",
    "    # Get attention weights\n",
    "    _, (edge_index, weights) = att_layer(data.x, data.edge_index, return_attention_weights=True)\n",
    "\n",
    "    # Create networkx graph\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(range(data.x.size(0)))\n",
    "\n",
    "    # Get positions\n",
    "    pos = {}\n",
    "    for i, atom in enumerate(mol.GetAtoms()):\n",
    "        pos[i] = mol.GetConformer().GetAtomPosition(i)\n",
    "        pos[i] = (pos[i].x, -pos[i].y)\n",
    "\n",
    "    # Get edges with attention weights\n",
    "    edges = [(edge_index[0, i].item(), edge_index[1, i].item()) for i in range(edge_index.size(1))]\n",
    "    edge_weights = weights.detach().numpy().flatten()\n",
    "\n",
    "    # Get atom labels\n",
    "    atom_labels = {i: atom.GetSymbol() for i, atom in enumerate(mol.GetAtoms())}\n",
    "\n",
    "    # Create animation\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    # Initialize the plot\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=500, node_color='lightblue', ax=ax)\n",
    "    nx.draw_networkx_labels(G, pos, labels=atom_labels, font_size=12, ax=ax)\n",
    "\n",
    "    # Animation function\n",
    "    def update(frame):\n",
    "        ax.clear()\n",
    "\n",
    "        # Draw nodes and labels\n",
    "        nx.draw_networkx_nodes(G, pos, node_size=500, node_color='lightblue', ax=ax)\n",
    "        nx.draw_networkx_labels(G, pos, labels=atom_labels, font_size=12, ax=ax)\n",
    "\n",
    "        # Draw edges with attention up to current frame\n",
    "        if frame > 0:\n",
    "            # Scale weights for better visualization\n",
    "            min_width = 1\n",
    "            max_width = 5\n",
    "            norm_weights = [min_width + (w - min(edge_weights)) * (max_width - min_width) /\n",
    "                           (max(edge_weights) - min(edge_weights) + 1e-6) for w in edge_weights[:frame]]\n",
    "\n",
    "            # Draw edges with attention weights\n",
    "            nx.draw_networkx_edges(G, pos,\n",
    "                                 edgelist=edges[:frame],\n",
    "                                 width=norm_weights,\n",
    "                                 edge_color=edge_weights[:frame],\n",
    "                                 edge_cmap=plt.cm.viridis,\n",
    "                                 alpha=0.8,\n",
    "                                 arrowsize=15,\n",
    "                                 ax=ax,\n",
    "                                 node_size=500,\n",
    "                                 connectionstyle='arc3,rad=0.1')\n",
    "\n",
    "        ax.set_title(f\"Attention Propagation (Step {frame}/{len(edges)})\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Create animation\n",
    "    ani = FuncAnimation(fig, update, frames=len(edges)+1, interval=500)\n",
    "\n",
    "    # To display the animation, save it as a GIF and display with HTML\n",
    "    plt.close()\n",
    "    return ani\n",
    "\n",
    "# Create animation for aspirin\n",
    "aspirin_ani = create_attention_animation(example_molecules['Aspirin'])\n",
    "\n",
    "# Save as GIF\n",
    "aspirin_ani.save('aspirin_attention.gif', writer='pillow', fps=2)\n",
    "\n",
    "# Display the animation\n",
    "from IPython.display import Image\n",
    "Image('aspirin_attention.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaH_DVKxtHJ0"
   },
   "source": [
    "## 13. Conclusion and Further Research Directions <a name=\"conclusion\"></a>\n",
    "\n",
    "Graph Attention Networks (GATs) provide powerful tools for molecular modeling and property prediction by allowing the model to focus on the most relevant parts of a molecule. In this tutorial, we've covered:\n",
    "\n",
    "1. The basics of representing molecules as graphs\n",
    "2. How Graph Neural Networks process molecular data\n",
    "3. The importance of attention mechanisms vs. simple averaging\n",
    "4. The benefits of multi-head attention for capturing different aspects of molecular structure\n",
    "5. Implementing and training a GAT model for molecular property prediction\n",
    "\n",
    "For chemists and pharmacists, GATs offer an interpretable deep learning approach that aligns well with chemical intuition about molecular structure. The attention weights can provide insights into which atomic interactions are most relevant for specific properties, potentially guiding the design of new molecules.\n",
    "\n",
    "### Further Research Directions:\n",
    "\n",
    "- Combining GATs with other molecular descriptors\n",
    "- Applying GATs to protein-ligand interactions\n",
    "- Using GATs for de novo molecular design\n",
    "- Exploring edge attention mechanisms for bond-specific features\n",
    "- Hierarchical GATs for modeling molecular substructures\n",
    "\n",
    "### References:\n",
    "\n",
    "1. Veličković, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., & Bengio, Y. (2017). Graph attention networks. arXiv preprint arXiv:1710.10903.\n",
    "2. Yang, K., Swanson, K., Jin, W., Coley, C., Eiden, P., Gao, H., ... & Barzilay, R. (2019). Analyzing learned molecular representations for property prediction. Journal of chemical information and modeling, 59(8), 3370-3388.\n",
    "3. Xiong, Z., Wang, D., Liu, X., Zhong, F., Wan, X., Li, X., ... & Fu, T. (2019). Pushing the boundaries of molecular representation for drug discovery with the graph attention mechanism. Journal of medicinal chemistry, 63(16), 8749-8760."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mq4ecqstKBg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNY5WPwauiqbgaJzfBirE6X",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
