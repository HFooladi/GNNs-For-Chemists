{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HFooladi/GNNs-For-Chemists/blob/main/notebooks/04_GNN_GCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxtQeGH8EV9G"
   },
   "source": [
    "# Graph Convolutional Networks (GCN) Tutorial for Chemists and Pharmacists\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Setup and Installation](#setup-and-installation)\n",
    "3. [Understanding Molecular Graphs](#understand-graph)\n",
    "4. [Graph Convolutional Networks: Basic Concepts](#gcn-basics)\n",
    "5. [Implementing a Basic GCN ](#implement-gcn)\n",
    "6. [Exploring Different Pooling Methods](#explore-pooling)\n",
    "7. [Demonstrating the Differences Between Pooling Methods on Real Molecules](#demonstrate-pooling)\n",
    "8. [Exploring the Effect of Network Depth](#gcn-depth)\n",
    "9. [Understanding and Implementing Skip Connections](#skip-connection)\n",
    "10. [Visualizing Node Feature Learning](#node-feature-learning)\n",
    "11. [Conclusion and Comprehensive Findings](#conclusion)\n",
    "12. [Additional Resources and References](#resources)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4S0nDnd8M8cM"
   },
   "source": [
    "## 1. Introduction  <a name=\"introduction\"></a>\n",
    "\n",
    "Graph Convolutional Networks (GCNs) are a powerful class of neural networks designed to work directly on graph-structured data. In chemistry and drug discovery, molecules are naturally represented as graphs, with atoms as nodes and bonds as edges, making GCNs particularly valuable for tasks like molecular property prediction and drug design.\n",
    "\n",
    "This notebook provides a step-by-step exploration of GCNs with a focus on:\n",
    "1. How different pooling methods (max, sum, mean) affect results\n",
    "2. The impact of network depth (increasing the number of layers)\n",
    "3. The effect of adding skip connections to the architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qzf1O_zmEbFe"
   },
   "source": [
    "## 2. Setup and Installation <a name=\"setup-and-installation\"></a>\n",
    "\n",
    "First, let's install the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nv12bZ74EW4-",
    "outputId": "648ae681-9b6b-4ba2-e7b4-253c7fc05c80"
   },
   "outputs": [],
   "source": [
    "#@title Intstall necessary libraries\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch_geometric\n",
    "!pip install -q rdkit\n",
    "!pip install -q networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CBrxhtgZEgTn",
    "outputId": "8c9d6402-5991-4235-ef88-61f1cf6015d4"
   },
   "outputs": [],
   "source": [
    "#@title Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_add_pool, global_max_pool, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, AllChem\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import io\n",
    "from PIL import Image\n",
    "import random\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtZeuY3yEv_G"
   },
   "source": [
    "## 3. Understanding Molecular Graphs <a name=\"understand-graph\"></a>\n",
    "\n",
    "Let's start by visualizing molecules as graphs, which is essential for understanding how GCNs process molecular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "c8TTL91rEwUV",
    "outputId": "41a0a84a-0a31-474e-909c-4717cd0c8899"
   },
   "outputs": [],
   "source": [
    "def atom_features(atom):\n",
    "    \"\"\"\n",
    "    Extract a feature vector for an RDKit atom.\n",
    "\n",
    "    Features included:\n",
    "        - Atomic number\n",
    "        - Chirality tag (encoded as integer)\n",
    "        - Degree (number of directly-bonded atoms)\n",
    "        - Formal charge\n",
    "        - Total number of hydrogens\n",
    "        - Number of radical electrons\n",
    "        - Hybridization (encoded as integer)\n",
    "        - Aromaticity (0 or 1)\n",
    "        - Ring membership (0 or 1)\n",
    "\n",
    "    Args:\n",
    "        atom (rdkit.Chem.rdchem.Atom): An RDKit Atom object.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Feature tensor of shape (9,) with dtype long.\n",
    "    \"\"\"\n",
    "    return torch.tensor([\n",
    "        atom.GetAtomicNum(),                    # Atomic number\n",
    "        int(atom.GetChiralTag()),               # Chirality\n",
    "        atom.GetDegree(),                       # Degree\n",
    "        atom.GetFormalCharge(),                 # Formal charge\n",
    "        atom.GetTotalNumHs(),                   # Number of hydrogens\n",
    "        atom.GetNumRadicalElectrons(),          # Radical electrons\n",
    "        int(atom.GetHybridization()),           # Hybridization\n",
    "        int(atom.GetIsAromatic()),              # Aromaticity\n",
    "        int(atom.IsInRing())                    # Ring membership\n",
    "    ], dtype=torch.long)\n",
    "\n",
    "\n",
    "def bond_features(bond):\n",
    "    \"\"\"\n",
    "    Extract a feature vector for an RDKit bond.\n",
    "\n",
    "    Features included:\n",
    "        - Bond type as double (e.g., 1.0 for single, 2.0 for double)\n",
    "        - Conjugation (0 or 1)\n",
    "        - Ring membership (0 or 1)\n",
    "\n",
    "    Args:\n",
    "        bond (rdkit.Chem.rdchem.Bond): An RDKit Bond object.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Feature tensor of shape (3,) with dtype long.\n",
    "    \"\"\"\n",
    "    return torch.tensor([\n",
    "        int(bond.GetBondTypeAsDouble()),        # Bond type\n",
    "        int(bond.GetIsConjugated()),            # Conjugation\n",
    "        int(bond.IsInRing())                    # Ring membership\n",
    "    ], dtype=torch.long)\n",
    "\n",
    "def mol_to_graph(smiles):\n",
    "    \"\"\"\n",
    "    Converts a SMILES into a PyTorch Geometric graph data object.\n",
    "\n",
    "    Nodes represent atoms with features, and edges represent bonds with features.\n",
    "    The graph is undirected: each bond adds two directed edges (i->j and j->i).\n",
    "\n",
    "    Args:\n",
    "        smiles (str): SMILES representing the molecule.\n",
    "\n",
    "    Returns:\n",
    "        torch_geometric.data.Data: Graph data object containing:\n",
    "            - x: Node feature matrix [num_nodes, 9]\n",
    "            - edge_index: Edge list [2, num_edges]\n",
    "            - edge_attr: Edge feature matrix [num_edges, 3]\n",
    "            - smiles: Original SMILES\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        raise ValueError(f\"Invalid SMILES string: {smiles}\")\n",
    "\n",
    "    # Node features\n",
    "    x = torch.stack([atom_features(atom) for atom in mol.GetAtoms()], dim=0)\n",
    "\n",
    "    # Edge index and edge features\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "\n",
    "        # Add both directions for undirected graph\n",
    "        edge_index.append((i, j))\n",
    "        edge_index.append((j, i))\n",
    "\n",
    "        edge_attr.append(bond_features(bond))\n",
    "        edge_attr.append(bond_features(bond))\n",
    "\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.stack(edge_attr, dim=0) if edge_attr else None\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, smiles=smiles)\n",
    "    return data\n",
    "\n",
    "\n",
    "def visualize_molecule(smiles, title=\"Molecule\"):\n",
    "    \"\"\"Visualize a molecule using RDKit\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    AllChem.Compute2DCoords(mol)\n",
    "\n",
    "    # Draw molecule\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    drawer = rdMolDraw2D.MolDraw2DCairo(500, 500)\n",
    "    drawer.DrawMolecule(mol)\n",
    "    drawer.FinishDrawing()\n",
    "    img = drawer.GetDrawingText()\n",
    "\n",
    "    # Convert the image data to a PIL Image\n",
    "    pil_image = Image.open(io.BytesIO(img))\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(pil_image)\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def visualize_molecular_graph(smiles, title=\"Molecular Graph\"):\n",
    "    \"\"\"\n",
    "    Visualizes the 2D structure of a molecule using RDKit and networkx and displays it.\n",
    "\n",
    "    Args:\n",
    "        smiles (str): SMILES representing the molecule.\n",
    "        title (str): Plot title.\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    AllChem.Compute2DCoords(mol)\n",
    "\n",
    "    data = mol_to_graph(smiles)\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "    # Get the 2D coordinates from RDKit\n",
    "    pos = {}\n",
    "    for i, atom in enumerate(mol.GetAtoms()):\n",
    "        pos[i] = mol.GetConformer().GetAtomPosition(i)\n",
    "        pos[i] = (pos[i].x, -pos[i].y)  # Flip y for better visualization\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "\n",
    "    # Get atom labels\n",
    "    atom_labels = {i: atom.GetSymbol() for i, atom in enumerate(mol.GetAtoms())}\n",
    "\n",
    "    # Get atom features for node coloring\n",
    "    atom_features = [atom.GetAtomicNum() for atom in mol.GetAtoms()]\n",
    "\n",
    "    # Draw the graph\n",
    "    nx.draw(G, pos,\n",
    "            labels=atom_labels,\n",
    "            with_labels=True,\n",
    "            node_color=atom_features,\n",
    "            cmap=plt.cm.viridis,\n",
    "            node_size=500,\n",
    "            font_size=10,\n",
    "            font_color='white',\n",
    "            edge_color='gray')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example molecules\n",
    "example_molecules = {\n",
    "    \"Aspirin\": \"CC(=O)OC1=CC=CC=C1C(=O)O\",\n",
    "    \"Paracetamol\": \"CC(=O)NC1=CC=C(C=C1)O\",\n",
    "    \"Ibuprofen\": \"CC(C)CC1=CC=C(C=C1)C(C)C(=O)O\"\n",
    "}\n",
    "\n",
    "# Visualize each molecule\n",
    "for name, smiles in example_molecules.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    visualize_molecule(smiles, f\"{name} Structure\")\n",
    "    visualize_molecular_graph(smiles, f\"{name} as Graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76kVLBBsE2nN"
   },
   "source": [
    "## 4. Graph Convolutional Networks: Basic Concepts <a name=\"gcn-basics\"></a>\n",
    "\n",
    "Now that we understand how molecules are represented as graphs, let's explore the basic concepts of Graph Convolutional Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "nNrWMQqNEz_2",
    "outputId": "9ed49417-1703-4b1f-884b-2c2251a9fd95"
   },
   "outputs": [],
   "source": [
    "# Create a simple visualization of GCN message passing\n",
    "def visualize_gcn_message_passing():\n",
    "    \"\"\"\n",
    "    Create a visual, step-by-step explanation of GCN (Graph Convolutional Network) message passing.\n",
    "\n",
    "    The visualization is split into three subplots:\n",
    "        1. Initial molecular graph structure.\n",
    "        2. First GCN layer: Aggregation from immediate neighbors.\n",
    "        3. Second GCN layer: Expanded receptive field including neighbors-of-neighbors.\n",
    "\n",
    "    The example graph structure mimics a benzene ring with an additional side chain.\n",
    "    This is purely illustrative and not derived from chemical accuracy.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Create a simple molecular graph for illustration\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(7))\n",
    "\n",
    "    # Define edges for a simple molecule (like benzene with a side chain)\n",
    "    edges = [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 0), (1, 6)]\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    # Define positions\n",
    "    pos = {\n",
    "        0: (0, 1),\n",
    "        1: (1, 0.5),\n",
    "        2: (1, -0.5),\n",
    "        3: (0, -1),\n",
    "        4: (-1, -0.5),\n",
    "        5: (-1, 0.5),\n",
    "        6: (2, 1)\n",
    "    }\n",
    "\n",
    "    # Initial state - just the graph\n",
    "    ax = axes[0]\n",
    "    nx.draw(G, pos,\n",
    "            node_color='lightblue',\n",
    "            node_size=500,\n",
    "            with_labels=True,\n",
    "            font_size=12,\n",
    "            font_color='black',\n",
    "            ax=ax)\n",
    "    ax.set_title(\"Initial Graph\")\n",
    "\n",
    "    # First GCN layer - gather information from neighbors\n",
    "    ax = axes[1]\n",
    "    nx.draw(G, pos,\n",
    "            node_color='lightblue',\n",
    "            node_size=500,\n",
    "            with_labels=True,\n",
    "            font_size=12,\n",
    "            font_color='black',\n",
    "            ax=ax)\n",
    "\n",
    "    # Add arrows to show information flow\n",
    "    central_node = 1\n",
    "    for neighbor in [0, 2, 6]:\n",
    "        ax.annotate(\"\",\n",
    "                  xy=pos[central_node],\n",
    "                  xytext=pos[neighbor],\n",
    "                  arrowprops=dict(arrowstyle=\"->\", color=\"red\", lw=2))\n",
    "\n",
    "    ax.set_title(\"GCN Layer 1: Neighborhood Aggregation\")\n",
    "\n",
    "    # Second GCN layer - expanded neighborhood\n",
    "    ax = axes[2]\n",
    "    nx.draw(G, pos,\n",
    "            node_color='lightblue',\n",
    "            node_size=500,\n",
    "            with_labels=True,\n",
    "            font_size=12,\n",
    "            font_color='black',\n",
    "            ax=ax)\n",
    "\n",
    "    # Draw circles to show the receptive field\n",
    "    circle1 = plt.Circle(pos[central_node], 0.5, color='red', fill=False, linestyle='-', linewidth=2)\n",
    "    circle2 = plt.Circle(pos[central_node], 1.5, color='green', fill=False, linestyle='--', linewidth=2)\n",
    "    ax.add_patch(circle1)\n",
    "    ax.add_patch(circle2)\n",
    "\n",
    "    # Add a legend\n",
    "    ax.plot([], [], color='red', linestyle='-', linewidth=2, label='Layer 1 receptive field')\n",
    "    ax.plot([], [], color='green', linestyle='--', linewidth=2, label='Layer 2 receptive field')\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    ax.set_title(\"GCN Layer 2: Expanded Receptive Field\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_gcn_message_passing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TimJSOp3E7I9"
   },
   "source": [
    "## 5. Implementing a Basic GCN <a name=\"implement-gcn\"></a>\n",
    "\n",
    "Let's implement a basic GCN for molecular property prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Sbx2-aCE_pG",
    "outputId": "10962654-1078-4d83-9ebd-e4e1dd84251b"
   },
   "outputs": [],
   "source": [
    "class BasicGCN(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic Graph Convolutional Network (GCN) for graph-level prediction.\n",
    "\n",
    "    Architecture:\n",
    "    - Initial GCN layer to process input node features\n",
    "    - Optional multiple hidden GCN layers (default: 1 hidden layer)\n",
    "    - Global mean pooling to get graph-level representation\n",
    "    - Two-layer MLP for final prediction\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input node features\n",
    "        hidden_channels (int): Number of hidden units in GCN and MLP layers\n",
    "        out_channels (int): Output size (e.g., 1 for regression, num_classes for classification)\n",
    "        num_layers (int): Number of GCN layers (default is 2: input + 1 hidden)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2):\n",
    "        super(BasicGCN, self).__init__()\n",
    "\n",
    "        # Input layer\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "\n",
    "        # Hidden layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "\n",
    "        # Output layer\n",
    "        self.lin1 = nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        \"\"\"\n",
    "        Forward pass of the GCN model.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Node features [num_nodes, in_channels]\n",
    "            edge_index (LongTensor): Edge list [2, num_edges]\n",
    "            batch (LongTensor): Batch vector mapping each node to its graph\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Graph-level prediction logits [batch_size, out_channels]\n",
    "        \"\"\"\n",
    "        # First GCN layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Additional GCN layers\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "\n",
    "        # Global pooling (default: mean)\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # MLP for final prediction\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.lin2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Create a simple model to demonstrate\n",
    "sample_data = mol_to_graph(example_molecules[\"Aspirin\"])\n",
    "print(f\"Node features: {sample_data.x.shape}\")\n",
    "model = BasicGCN(sample_data.x.shape[1], hidden_channels=64, out_channels=1, num_layers=2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cmBKyRuFA08"
   },
   "source": [
    "## 6. Exploring Different Pooling Methods <a name=\"explore-pooling\"></a>\n",
    "\n",
    "Now let's compare the effect of different pooling methods: max, sum, and mean pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_du1U0n2FDKE",
    "outputId": "e7066c0a-addb-4e77-b5cf-2a0c5aa30e5e"
   },
   "outputs": [],
   "source": [
    "class GCNWithPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    A GCN model that supports different global pooling strategies.\n",
    "\n",
    "    Architecture:\n",
    "    - Two GCN layers to learn node embeddings.\n",
    "    - One global pooling layer (mean, sum, or max) to get a graph-level representation.\n",
    "    - Two-layer MLP for graph-level prediction.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input node features.\n",
    "        hidden_channels (int): Number of hidden units in GCN and MLP layers.\n",
    "        out_channels (int): Output dimension (e.g., 1 for regression).\n",
    "        pooling_method (str): One of ['mean', 'sum', 'max'].\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, pooling_method='mean'):\n",
    "        super(GCNWithPooling, self).__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.lin1 = nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "        # Set pooling method\n",
    "        self.pooling_method = pooling_method\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        \"\"\"\n",
    "        Forward pass of the GCN.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Node features [num_nodes, in_channels].\n",
    "            edge_index (LongTensor): Edge list [2, num_edges].\n",
    "            batch (LongTensor): Batch vector mapping nodes to graphs.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Graph-level predictions [batch_size, out_channels].\n",
    "        \"\"\"\n",
    "        # GCN layers\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "\n",
    "        # Apply the specified pooling method\n",
    "        if self.pooling_method == 'max':\n",
    "            x = global_max_pool(x, batch)\n",
    "        elif self.pooling_method == 'sum':\n",
    "            x = global_add_pool(x, batch)\n",
    "        else:  # Default to mean\n",
    "            x = global_mean_pool(x, batch)\n",
    "\n",
    "        # Final MLP\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.lin2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def visualize_pooling_operations():\n",
    "    \"\"\"\n",
    "    Visualize the effect of different global pooling methods (mean, max, sum)\n",
    "    on a small batch of synthetic node features grouped into graphs.\n",
    "    \"\"\"\n",
    "    # Create a simple example\n",
    "    node_features = torch.tensor([\n",
    "        [1.0, 0.2],\n",
    "        [0.8, 0.4],\n",
    "        [0.3, 0.9],\n",
    "        [0.5, 0.6],\n",
    "        [0.2, 0.8],\n",
    "        [0.7, 0.3]\n",
    "    ], dtype=torch.float)\n",
    "\n",
    "    # Batch indicator: first 3 nodes belong to graph 0, next 3 to graph 1\n",
    "    batch = torch.tensor([0, 0, 0, 1, 1, 1], dtype=torch.long)\n",
    "\n",
    "    # Apply different pooling operations\n",
    "    mean_pooled = global_mean_pool(node_features, batch)\n",
    "    max_pooled = global_max_pool(node_features, batch)\n",
    "    sum_pooled = global_add_pool(node_features, batch)\n",
    "\n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "    # Original node features\n",
    "    ax = axes[0, 0]\n",
    "    ax.imshow(node_features.numpy(), cmap='viridis')\n",
    "    ax.set_title(\"Original Node Features\")\n",
    "    ax.set_xlabel(\"Feature Dimension\")\n",
    "    ax.set_ylabel(\"Node ID\")\n",
    "\n",
    "    for i in range(len(node_features)):\n",
    "        graph_id = batch[i].item()\n",
    "        for j in range(node_features.size(1)):\n",
    "            ax.text(j, i, f\"{node_features[i, j]:.1f}\",\n",
    "                   ha=\"center\", va=\"center\", color=\"white\")\n",
    "\n",
    "    # Add dividing line between graphs\n",
    "    ax.axhline(y=2.5, color='red', linestyle='-', linewidth=2)\n",
    "    ax.text(node_features.size(1) + 0.8, 1, \"Graph 0\", ha=\"right\", va=\"center\", color=\"black\", fontsize=12)\n",
    "    ax.text(node_features.size(1) + 0.8, 4, \"Graph 1\", ha=\"right\", va=\"center\", color=\"black\", fontsize=12)\n",
    "\n",
    "    # Mean pooling\n",
    "    ax = axes[0, 1]\n",
    "    ax.imshow(mean_pooled.numpy(), cmap='viridis')\n",
    "    ax.set_title(\"After Mean Pooling\")\n",
    "    ax.set_xlabel(\"Feature Dimension\")\n",
    "    ax.set_ylabel(\"Graph ID\")\n",
    "\n",
    "    for i in range(mean_pooled.size(0)):\n",
    "        for j in range(mean_pooled.size(1)):\n",
    "            ax.text(j, i, f\"{mean_pooled[i, j]:.2f}\",\n",
    "                   ha=\"center\", va=\"center\", color=\"white\")\n",
    "\n",
    "    # Max pooling\n",
    "    ax = axes[1, 0]\n",
    "    ax.imshow(max_pooled.numpy(), cmap='viridis')\n",
    "    ax.set_title(\"After Max Pooling\")\n",
    "    ax.set_xlabel(\"Feature Dimension\")\n",
    "    ax.set_ylabel(\"Graph ID\")\n",
    "\n",
    "    for i in range(max_pooled.size(0)):\n",
    "        for j in range(max_pooled.size(1)):\n",
    "            ax.text(j, i, f\"{max_pooled[i, j]:.2f}\",\n",
    "                   ha=\"center\", va=\"center\", color=\"white\")\n",
    "\n",
    "    # Sum pooling\n",
    "    ax = axes[1, 1]\n",
    "    ax.imshow(sum_pooled.numpy(), cmap='viridis')\n",
    "    ax.set_title(\"After Sum Pooling\")\n",
    "    ax.set_xlabel(\"Feature Dimension\")\n",
    "    ax.set_ylabel(\"Graph ID\")\n",
    "\n",
    "    for i in range(sum_pooled.size(0)):\n",
    "        for j in range(sum_pooled.size(1)):\n",
    "            ax.text(j, i, f\"{sum_pooled[i, j]:.2f}\",\n",
    "                   ha=\"center\", va=\"center\", color=\"white\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize how the different pooling operations work\n",
    "visualize_pooling_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJfJj0xoFIuU"
   },
   "source": [
    "## 7. Demonstrating the Differences Between Pooling Methods on Real Molecules <a name=\"demonstrate-pooling\"></a>\n",
    "\n",
    "Let's understand how different pooling methods impact results for molecular property prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_DI99KP4FI_E",
    "outputId": "17ea4af8-9054-4efe-8457-86f2f4ad7b6c"
   },
   "outputs": [],
   "source": [
    "# Load a dataset from MoleculeNet\n",
    "print(\"Loading ESOL dataset (water solubility data)...\")\n",
    "dataset = MoleculeNet(root='data', name='ESOL')\n",
    "print(f\"Dataset loaded: {len(dataset)} molecules\")\n",
    "\n",
    "# Split the dataset\n",
    "torch.manual_seed(42)\n",
    "indices = torch.randperm(len(dataset))\n",
    "train_idx = indices[:int(0.8 * len(dataset))]\n",
    "val_idx = indices[int(0.8 * len(dataset)):int(0.9 * len(dataset))]\n",
    "test_idx = indices[int(0.9 * len(dataset)):]\n",
    "\n",
    "train_dataset = dataset[train_idx]\n",
    "val_dataset = dataset[val_idx]\n",
    "test_dataset = dataset[test_idx]\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Validation: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "# Create data loaders\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Function to train and evaluate models\n",
    "def train_and_evaluate(model, optimizer, criterion, train_loader, val_loader, test_loader, epochs=50, early_stopping=True):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x.float(), data.edge_index, data.batch)\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * data.num_graphs\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                data = data.to(device)\n",
    "                out = model(data.x.float(), data.edge_index, data.batch)\n",
    "                loss = criterion(out, data.y)\n",
    "                val_loss += loss.item() * data.num_graphs\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch:3d}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "        # Check for early stopping\n",
    "        if early_stopping:\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                # Save the best model\n",
    "                torch.save(model.state_dict(), 'best_model.pt')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    # Load the best model if using early stopping\n",
    "    if early_stopping:\n",
    "        model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_preds = []\n",
    "    test_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x.float(), data.edge_index, data.batch)\n",
    "            loss = criterion(out, data.y)\n",
    "            test_loss += loss.item() * data.num_graphs\n",
    "            test_preds.append(out.cpu())\n",
    "            test_targets.append(data.y.cpu())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_preds = torch.cat(test_preds, dim=0)\n",
    "    test_targets = torch.cat(test_targets, dim=0)\n",
    "\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'test_loss': test_loss,\n",
    "        'test_preds': test_preds,\n",
    "        'test_targets': test_targets,\n",
    "        'model': model\n",
    "    }\n",
    "\n",
    "# Train models with different pooling methods\n",
    "def compare_pooling_methods():\n",
    "    # Get input dimension\n",
    "    sample = dataset[0]\n",
    "    in_channels = sample.x.shape[1]\n",
    "\n",
    "    # Common parameters\n",
    "    hidden_channels = 64\n",
    "    out_channels = 1  # Regression task\n",
    "    lr = 0.001\n",
    "    weight_decay = 5e-4\n",
    "    epochs = 50\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Initialize models with different pooling methods\n",
    "    models = {\n",
    "        'Mean Pooling': GCNWithPooling(in_channels, hidden_channels, out_channels, 'mean').to(device),\n",
    "        'Max Pooling': GCNWithPooling(in_channels, hidden_channels, out_channels, 'max').to(device),\n",
    "        'Sum Pooling': GCNWithPooling(in_channels, hidden_channels, out_channels, 'sum').to(device)\n",
    "    }\n",
    "\n",
    "    # Train and evaluate each model\n",
    "    results = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining model with {name}...\")\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        results[name] = train_and_evaluate(model, optimizer, criterion, train_loader, val_loader, test_loader, epochs)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the comparison\n",
    "pooling_results = compare_pooling_methods()\n",
    "\n",
    "# Visualize training curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(1, 2, 1)\n",
    "for name, result in pooling_results.items():\n",
    "    plt.plot(result['train_losses'], label=f\"{name} (Train)\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "for name, result in pooling_results.items():\n",
    "    plt.plot(result['val_losses'], label=f\"{name} (Val)\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Validation Loss Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize test performance\n",
    "names = list(pooling_results.keys())\n",
    "test_losses = [result['test_loss'] for result in pooling_results.values()]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(names, test_losses)\n",
    "plt.xlabel('Pooling Method')\n",
    "plt.ylabel('Test Loss (MSE)')\n",
    "plt.title('Test Loss Comparison')\n",
    "\n",
    "# Add value labels on the bars\n",
    "for bar, val in zip(bars, test_losses):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, bar.get_height() + 0.01, f'{val:.4f}',\n",
    "            ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize predictions vs targets for each model\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, (name, result) in enumerate(pooling_results.items()):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.scatter(result['test_targets'].numpy(), result['test_preds'].numpy(), alpha=0.5)\n",
    "    plt.plot([-10, 2], [-10, 2], 'r--')  # Diagonal line for perfect predictions\n",
    "    plt.xlabel('Actual log(Solubility)')\n",
    "    plt.ylabel('Predicted log(Solubility)')\n",
    "    plt.title(f'{name}')\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bx0QoCiE-jKS"
   },
   "source": [
    "Since solubility heavily depends on the moleular size, sum pooling which preserve the information about molecular size has the best performance on this task. For an exercise, you can check the performance on other MoleculeNet datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I17RC4VFFOGU"
   },
   "source": [
    "## 8. Exploring the Effect of Network Depth <a name=\"gcn-depth\"></a>\n",
    "\n",
    "Now, let's investigate how the number of GCN layers affects model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vrSPTQFFFP1L",
    "outputId": "ab0bc81b-962e-4da6-c28b-bfe247a97ff2"
   },
   "outputs": [],
   "source": [
    "class VariableDepthGCN(nn.Module):\n",
    "    \"\"\"\n",
    "    A Graph Convolutional Network (GCN) with a variable number of layers.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input node features.\n",
    "        hidden_channels (int): Number of hidden units in GCN layers.\n",
    "        out_channels (int): Output dimension (e.g., 1 for regression).\n",
    "        num_layers (int): Number of GCN layers (depth).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers):\n",
    "        super(VariableDepthGCN, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Input layer\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "\n",
    "        # Hidden layers\n",
    "        self.convs = nn.ModuleList([GCNConv(hidden_channels, hidden_channels) for _ in range(num_layers - 1)])\n",
    "\n",
    "        # Output layers\n",
    "        self.lin1 = nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        \"\"\"\n",
    "        Forward pass of the GCN.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Node features [num_nodes, in_channels].\n",
    "            edge_index (Tensor): Edge list [2, num_edges].\n",
    "            batch (Tensor): Batch vector assigning nodes to graphs.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Graph-level predictions.\n",
    "        \"\"\"\n",
    "        # First GCN layer\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "\n",
    "        # Additional GCN layers\n",
    "        for i in range(self.num_layers - 1):\n",
    "            x = F.relu(self.convs[i](x, edge_index))\n",
    "\n",
    "        # Global pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # MLP for final prediction\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.lin2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def visualize_receptive_field(num_layers):\n",
    "    \"\"\"\n",
    "    Visualize how the receptive field of a central node expands with increasing GCN layers.\n",
    "\n",
    "    Args:\n",
    "        num_layers (int): Maximum GCN depth to visualize.\n",
    "    \"\"\"\n",
    "    # Create a simple grid graph for visualization\n",
    "    G = nx.grid_2d_graph(5, 5)\n",
    "\n",
    "    # Convert to 0-indexed integer nodes\n",
    "    G = nx.convert_node_labels_to_integers(G)\n",
    "\n",
    "    # Set positions\n",
    "    pos = {i: (i % 5, i // 5) for i in range(25)}\n",
    "\n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, num_layers + 1, figsize=(4 * (num_layers + 1), 4))\n",
    "\n",
    "    # Central node\n",
    "    central_node = 12  # Middle of the grid\n",
    "\n",
    "    # Drawing the initial graph with the central node highlighted\n",
    "    ax = axes[0]\n",
    "    nodes = nx.draw_networkx_nodes(G, pos, node_color=['red' if n == central_node else 'lightblue' for n in G.nodes()],\n",
    "                                 node_size=500, ax=ax)\n",
    "    edges = nx.draw_networkx_edges(G, pos, ax=ax)\n",
    "    labels = nx.draw_networkx_labels(G, pos, ax=ax)\n",
    "    ax.set_title(\"Initial Node\")\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Draw receptive field for each layer\n",
    "    for layer in range(1, num_layers + 1):\n",
    "        ax = axes[layer]\n",
    "\n",
    "        # Compute nodes in the receptive field after 'layer' hops\n",
    "        receptive_field = {central_node}\n",
    "        current_boundary = {central_node}\n",
    "\n",
    "        for _ in range(layer):\n",
    "            new_boundary = set()\n",
    "            for node in current_boundary:\n",
    "                new_boundary.update(G.neighbors(node))\n",
    "            receptive_field.update(new_boundary)\n",
    "            current_boundary = new_boundary\n",
    "\n",
    "        # Color nodes based on whether they're in the receptive field\n",
    "        node_colors = ['red' if n == central_node else\n",
    "                      'orange' if n in receptive_field and n != central_node else\n",
    "                      'lightblue' for n in G.nodes()]\n",
    "\n",
    "        nodes = nx.draw_networkx_nodes(G, pos, node_color=node_colors,\n",
    "                                     node_size=500, ax=ax)\n",
    "        edges = nx.draw_networkx_edges(G, pos, ax=ax)\n",
    "        labels = nx.draw_networkx_labels(G, pos, ax=ax)\n",
    "        ax.set_title(f\"After {layer} GCN Layers\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize how the receptive field grows with depth\n",
    "visualize_receptive_field(num_layers=3)\n",
    "\n",
    "# Compare models with different depths\n",
    "def compare_network_depths():\n",
    "    \"\"\"\n",
    "    Compare GCN models with different numbers of layers and visualize their performance.\n",
    "\n",
    "    Returns:\n",
    "        dict: Results including train/val losses and test loss for each depth.\n",
    "    \"\"\"\n",
    "    # Get input dimension\n",
    "    sample = dataset[0]\n",
    "    in_channels = sample.x.shape[1]\n",
    "\n",
    "    # Common parameters\n",
    "    hidden_channels = 64\n",
    "    out_channels = 1  # Regression task\n",
    "    lr = 0.001\n",
    "    weight_decay = 5e-4\n",
    "    epochs = 50\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Initialize models with different depths\n",
    "    models = {\n",
    "        '1 Layer': VariableDepthGCN(in_channels, hidden_channels, out_channels, num_layers=1).to(device),\n",
    "        '2 Layers': VariableDepthGCN(in_channels, hidden_channels, out_channels, num_layers=2).to(device),\n",
    "        '3 Layers': VariableDepthGCN(in_channels, hidden_channels, out_channels, num_layers=3).to(device),\n",
    "        '4 Layers': VariableDepthGCN(in_channels, hidden_channels, out_channels, num_layers=4).to(device),\n",
    "        '5 Layers': VariableDepthGCN(in_channels, hidden_channels, out_channels, num_layers=5).to(device),\n",
    "    }\n",
    "\n",
    "    # Train and evaluate each model\n",
    "    results = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining model with {name}...\")\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        results[name] = train_and_evaluate(model, optimizer, criterion, train_loader, val_loader, test_loader, epochs)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the comparison\n",
    "depth_results = compare_network_depths()\n",
    "\n",
    "# Visualize training curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(1, 2, 1)\n",
    "for name, result in depth_results.items():\n",
    "    plt.plot(result['train_losses'], label=f\"{name} (Train)\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training Loss vs. Network Depth')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "for name, result in depth_results.items():\n",
    "    plt.plot(result['val_losses'], label=f\"{name} (Val)\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Validation Loss vs. Network Depth')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize test performance\n",
    "names = list(depth_results.keys())\n",
    "test_losses = [result['test_loss'] for result in depth_results.values()]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(names, test_losses)\n",
    "plt.xlabel('Number of GCN Layers')\n",
    "plt.ylabel('Test Loss (MSE)')\n",
    "plt.title('Test Loss vs. Network Depth')\n",
    "\n",
    "# Add value labels on the bars\n",
    "for bar, val in zip(bars, test_losses):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, bar.get_height() + 0.01, f'{val:.4f}',\n",
    "            ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QuGDIjfG7CP"
   },
   "source": [
    "# The Oversmoothing Problem in Deep GCNs\n",
    "\n",
    "As more GCN layers are added, node features tend to converge to similar values, causing:\n",
    "\n",
    "1. **Loss of Discriminative Power**: Nodes become indistinguishable\n",
    "2. **Gradient Vanishing**: Training becomes more difficult\n",
    "3. **Decreased Performance**: Model accuracy drops after certain depth\n",
    "\n",
    "This phenomenon, called \"oversmoothing,\" explains why GCNs often don't benefit\n",
    "from extra depth like CNNs or Transformers.\n",
    "\n",
    "# Solutions for Oversmoothing:\n",
    "\n",
    "1. **Skip Connections** or **Residual Connections**: Allow information to flow directly from earlier layers\n",
    "2. **Normalization Techniques**: Layer norm, batch norm, etc.\n",
    "3. **Attention Mechanisms**: Focus on important neighbors (like GAT)\n",
    "\n",
    "Next, let's investigate how skip connections can mitigate the oversmoothing problem\n",
    "\n",
    "## 9. Understanding and Implementing Skip Connections <a name=\"skip-connection\"></a>\n",
    "\n",
    "Skip (or residual) connections allow information to flow directly from earlier layers to later layers, bypassing intermediate operations. Let's explore how they can improve GCN performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2VFh6smOG7aH",
    "outputId": "2272da26-ae68-47c3-88fb-3c6e27e37465"
   },
   "outputs": [],
   "source": [
    "class GCNWithSkipConnections(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Convolutional Network (GCN) with optional residual (skip) connections.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Dimension of input node features.\n",
    "        hidden_channels (int): Dimension of hidden representations.\n",
    "        out_channels (int): Output dimension (e.g., regression or classification target size).\n",
    "        num_layers (int): Number of GCN layers.\n",
    "        use_skip (bool): Whether to use residual skip connections.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, use_skip=True):\n",
    "        super(GCNWithSkipConnections, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.use_skip = use_skip\n",
    "\n",
    "        # Input layer\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "\n",
    "        # Hidden layers\n",
    "        self.convs = nn.ModuleList([GCNConv(hidden_channels, hidden_channels) for _ in range(num_layers - 1)])\n",
    "\n",
    "        # Output layers\n",
    "        self.lin1 = nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        \"\"\"\n",
    "        Forward pass with optional residual connections.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Node feature matrix [num_nodes, in_channels].\n",
    "            edge_index (Tensor): Edge indices [2, num_edges].\n",
    "            batch (Tensor): Batch vector assigning each node to a graph.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output predictions per graph.\n",
    "        \"\"\"\n",
    "        # First GCN layer\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "\n",
    "        # Additional GCN layers with potential skip connections\n",
    "        for i in range(self.num_layers - 1):\n",
    "            if self.use_skip:\n",
    "                # Store the input to this layer\n",
    "                identity = x\n",
    "                # Apply GCN and activation\n",
    "                x = F.relu(self.convs[i](x, edge_index))\n",
    "                # Add skip connection (residual)\n",
    "                x = x + identity\n",
    "            else:\n",
    "                # Standard GCN without skip\n",
    "                x = F.relu(self.convs[i](x, edge_index))\n",
    "\n",
    "        # Global pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # MLP for final prediction\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.lin2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def visualize_skip_connections():\n",
    "    \"\"\"\n",
    "    Visualize standard GCN vs. GCN with skip connections.\n",
    "    Highlights the flow of information and benefits of residual links.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Regular GCN Layers\n",
    "    offset = 0\n",
    "    rect_width = 1.5\n",
    "    rect_height = 3\n",
    "    spacing = 2.5\n",
    "\n",
    "    # Draw boxes for standard GCN\n",
    "    y_pos = 2.5\n",
    "    for i in range(4):\n",
    "        rect = plt.Rectangle((offset + i*spacing, y_pos), rect_width, rect_height,\n",
    "                            facecolor='lightblue', edgecolor='black', alpha=0.7)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(offset + i*spacing + rect_width/2, y_pos + rect_height/2, f\"Layer {i+1}\",\n",
    "              ha='center', va='center', fontsize=10)\n",
    "\n",
    "    # Draw arrows connecting layers\n",
    "    for i in range(3):\n",
    "        ax.arrow(offset + i*spacing + rect_width, y_pos + rect_height/2,\n",
    "               spacing - rect_width, 0, head_width=0.2, head_length=0.3,\n",
    "               fc='black', ec='black')\n",
    "\n",
    "    # Label for standard GCN\n",
    "    ax.text(offset + 1.5*spacing, y_pos + rect_height + 0.5, \"Standard GCN\",\n",
    "          ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "    # Draw boxes for GCN with skip connections\n",
    "    y_pos = -2.5\n",
    "    for i in range(4):\n",
    "        rect = plt.Rectangle((offset + i*spacing, y_pos), rect_width, rect_height,\n",
    "                            facecolor='lightgreen', edgecolor='black', alpha=0.7)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(offset + i*spacing + rect_width/2, y_pos + rect_height/2, f\"Layer {i+1}\",\n",
    "              ha='center', va='center', fontsize=10)\n",
    "\n",
    "    # Draw regular arrows connecting layers\n",
    "    for i in range(3):\n",
    "        ax.arrow(offset + i*spacing + rect_width, y_pos + rect_height/2,\n",
    "               spacing - rect_width, 0, head_width=0.2, head_length=0.3,\n",
    "               fc='black', ec='black')\n",
    "\n",
    "    # Draw skip connection arrows\n",
    "    for i in range(2):\n",
    "        ax.arrow(offset + i*spacing + rect_width/2, y_pos + rect_height,\n",
    "               spacing, 0, head_width=0.2, head_length=0.3,\n",
    "               fc='red', ec='red', linestyle='--')\n",
    "\n",
    "    # Label for GCN with skip connections\n",
    "    ax.text(offset + 1.5*spacing, y_pos - 0.5, \"GCN with Skip Connections\",\n",
    "          ha='center', va='top', fontsize=12)\n",
    "\n",
    "    # Add explanatory labels\n",
    "    ax.text(offset + 1*spacing + rect_width/2, y_pos + rect_height + 1.5,\n",
    "          \"Skip connections allow information to\\nbypass intermediate layers\",\n",
    "          ha='center', va='center', fontsize=10)\n",
    "\n",
    "    ax.text(offset + 2*spacing + rect_width/2, 0,\n",
    "          \"Benefits:\\n- Mitigates oversmoothing\\n- Improves gradient flow\\n- Enables deeper architectures\",\n",
    "          ha='center', va='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "    ax.set_xlim(-0.5, 10)\n",
    "    ax.set_ylim(-4, 7)\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize skip connections\n",
    "visualize_skip_connections()\n",
    "\n",
    "# Compare models with and without skip connections at different depths\n",
    "def compare_skip_connections():\n",
    "    \"\"\"\n",
    "    Train and evaluate GCN models with and without skip connections at different depths.\n",
    "\n",
    "    Returns:\n",
    "        dict: A mapping from model description to training/validation/test metrics.\n",
    "    \"\"\"\n",
    "    # Get input dimension\n",
    "    sample = dataset[0]\n",
    "    in_channels = sample.x.shape[1]\n",
    "\n",
    "    # Common parameters\n",
    "    hidden_channels = 64\n",
    "    out_channels = 1  # Regression task\n",
    "    lr = 0.001\n",
    "    weight_decay = 5e-4\n",
    "    epochs = 50\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Initialize models with different depths and skip connection settings\n",
    "    models = {\n",
    "        '2 Layers (No Skip)': GCNWithSkipConnections(in_channels, hidden_channels, out_channels, num_layers=2, use_skip=False).to(device),\n",
    "        '2 Layers (With Skip)': GCNWithSkipConnections(in_channels, hidden_channels, out_channels, num_layers=2, use_skip=True).to(device),\n",
    "        '4 Layers (No Skip)': GCNWithSkipConnections(in_channels, hidden_channels, out_channels, num_layers=4, use_skip=False).to(device),\n",
    "        '4 Layers (With Skip)': GCNWithSkipConnections(in_channels, hidden_channels, out_channels, num_layers=4, use_skip=True).to(device),\n",
    "        '6 Layers (No Skip)': GCNWithSkipConnections(in_channels, hidden_channels, out_channels, num_layers=6, use_skip=False).to(device),\n",
    "        '6 Layers (With Skip)': GCNWithSkipConnections(in_channels, hidden_channels, out_channels, num_layers=6, use_skip=True).to(device),\n",
    "    }\n",
    "\n",
    "    # Train and evaluate each model\n",
    "    results = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining model: {name}...\")\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        results[name] = train_and_evaluate(model, optimizer, criterion, train_loader, val_loader, test_loader, epochs)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the comparison\n",
    "skip_results = compare_skip_connections()\n",
    "\n",
    "# Visualize training curves\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(1, 2, 1)\n",
    "colors = ['blue', 'orange', 'green']\n",
    "linestyles = ['-', '--']\n",
    "\n",
    "for i, depth in enumerate([2, 4, 6]):\n",
    "    for j, skip in enumerate([False, True]):\n",
    "        name = f\"{depth} Layers ({'With' if skip else 'No'} Skip)\"\n",
    "        result = skip_results[name]\n",
    "        plt.plot(result['train_losses'], color=colors[i], linestyle=linestyles[j],\n",
    "                label=name)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training Loss: Skip Connections vs. Network Depth')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "for i, depth in enumerate([2, 4, 6]):\n",
    "    for j, skip in enumerate([False, True]):\n",
    "        name = f\"{depth} Layers ({'With' if skip else 'No'} Skip)\"\n",
    "        result = skip_results[name]\n",
    "        plt.plot(result['val_losses'], color=colors[i], linestyle=linestyles[j],\n",
    "                label=name)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Validation Loss: Skip Connections vs. Network Depth')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize test performance\n",
    "names = list(skip_results.keys())\n",
    "test_losses = [result['test_loss'] for result in skip_results.values()]\n",
    "\n",
    "# Group data for better visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Extract data in structured form\n",
    "depths = [2, 4, 6]\n",
    "no_skip_losses = [skip_results[f\"{d} Layers (No Skip)\"][\"test_loss\"] for d in depths]\n",
    "with_skip_losses = [skip_results[f\"{d} Layers (With Skip)\"][\"test_loss\"] for d in depths]\n",
    "\n",
    "# Set up bar positions\n",
    "bar_width = 0.35\n",
    "x = np.arange(len(depths))\n",
    "\n",
    "# Create grouped bars\n",
    "plt.bar(x - bar_width/2, no_skip_losses, bar_width, label='No Skip Connections', color='lightcoral')\n",
    "plt.bar(x + bar_width/2, with_skip_losses, bar_width, label='With Skip Connections', color='lightgreen')\n",
    "\n",
    "# Add labels and customize\n",
    "plt.xlabel('Number of Layers')\n",
    "plt.ylabel('Test Loss (MSE)')\n",
    "plt.title('Effect of Skip Connections at Different Network Depths')\n",
    "plt.xticks(x, depths)\n",
    "plt.legend()\n",
    "plt.grid(True, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(no_skip_losses):\n",
    "    plt.text(i - bar_width/2, v + 0.005, f'{v:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "for i, v in enumerate(with_skip_losses):\n",
    "    plt.text(i + bar_width/2, v + 0.005, f'{v:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze the performance improvement from skip connections\n",
    "improvement = [(no_skip - with_skip) / no_skip * 100 for no_skip, with_skip in zip(no_skip_losses, with_skip_losses)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(depths, improvement, color='lightblue')\n",
    "plt.xlabel('Number of Layers')\n",
    "plt.ylabel('Improvement (%)')\n",
    "plt.title('Performance Improvement from Skip Connections')\n",
    "plt.grid(True, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(improvement):\n",
    "    plt.text(depths[i], v + 0.5, f'{v:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9kjQeKqHCt-"
   },
   "source": [
    "## 10. Visualizing Node Feature Learning <a name=\"node-feature-learning\"></a>\n",
    "\n",
    "Let's now visualize how node features evolve through the GCN layers, with and without skip connections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TxR42aELHE6m",
    "outputId": "12ff9c9f-0e9c-4f4a-a65c-294504471e8b"
   },
   "outputs": [],
   "source": [
    "def visualize_feature_evolution(model, data, smiles, use_skip=False):\n",
    "    \"\"\"\n",
    "    Visualize how node features transform across GCN layers using t-SNE.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained GCN model.\n",
    "        data (Data): Graph data object (PyG format).\n",
    "        smiles (str): SMILES string of the molecule for labeling.\n",
    "        use_skip (bool): Whether the model includes skip connections.\n",
    "\n",
    "    Returns:\n",
    "        List[np.ndarray]: List of feature representations from input through each layer.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Create hooks to get intermediate activations\n",
    "    activations = {}\n",
    "\n",
    "    def get_activation(name):\n",
    "        def hook(module, input, output):\n",
    "            activations[name] = output.detach()\n",
    "        return hook\n",
    "\n",
    "    # Register hooks for each layer\n",
    "    hooks = []\n",
    "    hooks.append(model.conv1.register_forward_hook(get_activation('conv1')))\n",
    "    for i, conv in enumerate(model.convs):\n",
    "        hooks.append(conv.register_forward_hook(get_activation(f'conv{i+2}')))\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        # Move data to the correct device\n",
    "        data = data.to(device)\n",
    "\n",
    "        # Create a fake batch index (all zeros since we have only one graph)\n",
    "        batch = torch.zeros(data.x.size(0), dtype=torch.long, device=device)\n",
    "\n",
    "        # Forward pass\n",
    "        _ = model(data.x.float(), data.edge_index, batch)\n",
    "\n",
    "    # Remove the hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    # Get the input features\n",
    "    input_features = data.x.cpu().numpy()\n",
    "\n",
    "    # Get activations for each layer\n",
    "    all_features = [input_features]\n",
    "    for i in range(len(activations)):\n",
    "        all_features.append(activations[f'conv{i+1}'].cpu().numpy())\n",
    "\n",
    "    # Use t-SNE to visualize high-dimensional features\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(10, all_features[i].shape[0]-1))\n",
    "\n",
    "    # Create molecule for reference\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    atom_symbols = [atom.GetSymbol() for atom in mol.GetAtoms()]\n",
    "\n",
    "    # Determine how many layers to plot\n",
    "    num_layers = min(4, len(all_features))\n",
    "\n",
    "    # Plot the feature evolution\n",
    "    fig, axes = plt.subplots(1, num_layers, figsize=(4 * num_layers, 4))\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        ax = axes[i]\n",
    "\n",
    "        # Apply t-SNE to reduce dimensions\n",
    "        if all_features[i].shape[1] > 2:\n",
    "            features_2d = tsne.fit_transform(all_features[i])\n",
    "        else:\n",
    "            features_2d = all_features[i]\n",
    "\n",
    "        # Plot the features\n",
    "        scatter = ax.scatter(features_2d[:, 0], features_2d[:, 1], s=100, alpha=0.8)\n",
    "\n",
    "        # Add atom labels\n",
    "        for j, symbol in enumerate(atom_symbols):\n",
    "            ax.text(features_2d[j, 0], features_2d[j, 1], symbol, ha='center', va='center')\n",
    "\n",
    "        # Set title\n",
    "        if i == 0:\n",
    "            ax.set_title('Input Features')\n",
    "        else:\n",
    "            ax.set_title(f'After Layer {i}')\n",
    "\n",
    "        ax.set_xlabel('t-SNE Dimension 1')\n",
    "        ax.set_ylabel('t-SNE Dimension 2')\n",
    "\n",
    "    plt.suptitle(f'Feature Evolution in GCN {\"with\" if use_skip else \"without\"} Skip Connections', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return all_features\n",
    "\n",
    "# Create and train two small models for visualization\n",
    "def train_small_model_for_vis(use_skip=False):\n",
    "    \"\"\"\n",
    "    Train a lightweight GCN model for visualization purposes.\n",
    "\n",
    "    Args:\n",
    "        use_skip (bool): Whether to use skip connections.\n",
    "\n",
    "    Returns:\n",
    "        Trained GCN model.\n",
    "    \"\"\"\n",
    "    # Get input dimension\n",
    "    sample = dataset[0]\n",
    "    in_channels = sample.x.shape[1]\n",
    "\n",
    "    # Common parameters\n",
    "    hidden_channels = 64\n",
    "    out_channels = 1  # Regression task\n",
    "    lr = 0.001\n",
    "    weight_decay = 5e-4\n",
    "    epochs = 50\n",
    "    criterion = nn.MSELoss()\n",
    "    num_layers = 4\n",
    "\n",
    "    model = GCNWithSkipConnections(\n",
    "        in_channels,\n",
    "        hidden_channels,\n",
    "        out_channels,\n",
    "        num_layers=num_layers,\n",
    "        use_skip=use_skip\n",
    "    ).to(device)\n",
    "\n",
    "    # Train briefly (just to get some reasonable parameters)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    result = train_and_evaluate(model, optimizer, criterion, train_loader, val_loader, test_loader, epochs)\n",
    "    return result['model']\n",
    "\n",
    "# Get models\n",
    "print(\"Training model without skip connections...\")\n",
    "no_skip_model = train_small_model_for_vis(use_skip=False)\n",
    "print(\"Training model with skip connections...\")\n",
    "skip_model = train_small_model_for_vis(use_skip=True)\n",
    "\n",
    "\n",
    "# Visualize feature evolution\n",
    "aspirin_smiles = example_molecules['Aspirin']\n",
    "aspirin_data = mol_to_graph(aspirin_smiles)\n",
    "no_skip_features = visualize_feature_evolution(no_skip_model, aspirin_data, aspirin_smiles, use_skip=False)\n",
    "skip_features = visualize_feature_evolution(skip_model, aspirin_data, aspirin_smiles, use_skip=True)\n",
    "\n",
    "# Calculate feature diversity through layers\n",
    "def feature_diversity(features_list):\n",
    "    \"\"\"\n",
    "    Compute average variance across feature dimensions for each layer.\n",
    "\n",
    "    Args:\n",
    "        features_list (List[np.ndarray]): Feature representations from each layer.\n",
    "\n",
    "    Returns:\n",
    "        List[float]: Variance (diversity) per layer.\n",
    "    \"\"\"\n",
    "    \"\"\"Calculate how diverse the features are at each layer\"\"\"\n",
    "    return [np.var(features, axis=0).mean() for features in features_list]\n",
    "\n",
    "no_skip_diversity = feature_diversity(no_skip_features)\n",
    "skip_diversity = feature_diversity(skip_features)\n",
    "\n",
    "# Plot feature diversity\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(len(no_skip_diversity)), no_skip_diversity, 'o-', label='Without Skip Connections')\n",
    "plt.plot(range(len(skip_diversity)), skip_diversity, 's-', label='With Skip Connections')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Feature Diversity (Variance)')\n",
    "plt.title('Feature Diversity Through GCN Layers')\n",
    "plt.xticks(range(len(no_skip_diversity)))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_emMuDEHVG1"
   },
   "source": [
    "## 11. Conclusion and Comprehensive Findings <a name=\"conclusion\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 944
    },
    "id": "CxqL9wShHXk1",
    "outputId": "198342c9-0e64-4f5c-dff4-bea1398389fe"
   },
   "outputs": [],
   "source": [
    "def summarize_findings():\n",
    "    \"\"\"Create a visual summary of our findings about GCNs\"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    text = \"\"\"\n",
    "    # Graph Convolutional Networks for Molecular Property Prediction\n",
    "\n",
    "    ## Key Findings\n",
    "\n",
    "    ### 1. Effect of Pooling Methods\n",
    "    - **Mean Pooling**: Provides balanced representation, generally performs well for diverse molecules.\n",
    "    - **Max Pooling**: Captures important features but ignores global structure. Good for identifying key substructures.\n",
    "    - **Sum Pooling**: Better for capturing overall molecular properties, but can be sensitive to molecule size.\n",
    "\n",
    "    ### 2. Effect of Network Depth\n",
    "    - **Shallow Networks (1-2 layers)**: Capture local molecular features but miss long-range interactions.\n",
    "    - **Medium Networks (3-4 layers)**: Good balance between local and global features.\n",
    "    - **Deep Networks (5+ layers)**: Suffer from oversmoothing without skip connections.\n",
    "\n",
    "    ### 3. Skip Connections\n",
    "    - More important as network depth increases\n",
    "    - Preserve feature diversity by preventing oversmoothing\n",
    "    - Allow effective information flow in deep networks\n",
    "    - Particularly beneficial for large, complex molecules\n",
    "\n",
    "    ## Best Practices for Molecular GCNs\n",
    "\n",
    "    1. Use skip connections for networks deeper than 3-4 layers\n",
    "    2. Choose pooling method based on the property you're predicting:\n",
    "       - Mean pooling for general properties (e.g., solubility, logP)\n",
    "       - Max pooling for properties dependent on specific substructures\n",
    "       - Sum pooling for properties that scale with molecule size\n",
    "    3. Add batch normalization between GCN layers for better training\n",
    "    4. Moderate depth (3-4 layers) usually provides the best balance\n",
    "\n",
    "    ## Future Directions\n",
    "\n",
    "    1. Explore edge features for better bond representation\n",
    "    2. Combine GCNs with attention mechanisms (like GATs)\n",
    "    3. Incorporate 3D molecular information\n",
    "    4. Use multi-task learning for related molecular properties\n",
    "    \"\"\"\n",
    "    plt.text(0.05, 0.5, text, fontsize=14, fontfamily='monospace',\n",
    "            verticalalignment='center', horizontalalignment='left')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Summarize our findings\n",
    "summarize_findings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTg-ys8BHbLl"
   },
   "source": [
    "## 12. Additional Resources and References <a name=\"resources\"></a>\n",
    "\n",
    "Here are some important papers and resources for further exploration of GCNs in chemistry:\n",
    "\n",
    "1. **Original GCN paper**: Kipf & Welling, \"Semi-Supervised Classification with Graph Convolutional Networks\" (2017)\n",
    "2. **Chemical GCNs**: Yang et al., \"Analyzing Learned Molecular Representations for Property Prediction\" (2019)\n",
    "3. **Oversmoothing**: Li et al., \"Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning\" (2018)\n",
    "4. **Skip Connections**: Xu et al., \"Representation Learning on Graphs with Jumping Knowledge Networks\" (2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 824
    },
    "id": "B-Mc-KgAHdP9",
    "outputId": "e8554550-5755-464d-c2a3-7598773a9263"
   },
   "outputs": [],
   "source": [
    "def create_resources_visualization():\n",
    "    \"\"\"Create a visual guide to GCN resources\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Create a mind map style visualization\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes for different categories\n",
    "    G.add_node(\"GCNs in Chemistry\", pos=(0, 0))\n",
    "\n",
    "    # Add resource categories\n",
    "    categories = [\"Foundational Papers\", \"Applications\", \"Advanced Architectures\", \"Software Tools\"]\n",
    "    for i, category in enumerate(categories):\n",
    "        angle = 2 * np.pi * i / len(categories)\n",
    "        x = 3 * np.cos(angle)\n",
    "        y = 3 * np.sin(angle)\n",
    "        G.add_node(category, pos=(x, y))\n",
    "        G.add_edge(\"GCNs in Chemistry\", category)\n",
    "\n",
    "    # Add specific resources for each category\n",
    "    resources = {\n",
    "        \"Foundational Papers\": [\n",
    "            \"Kipf & Welling (2017)\\nGCN\",\n",
    "            \"Hamilton et al. (2017)\\nGraphSAGE\",\n",
    "            \"Li et al. (2018)\\nGCN Oversmoothing\"\n",
    "        ],\n",
    "        \"Applications\": [\n",
    "            \"Drug Discovery\",\n",
    "            \"QSAR Modeling\",\n",
    "            \"Reaction Prediction\",\n",
    "            \"Retrosynthesis\"\n",
    "        ],\n",
    "        \"Advanced Architectures\": [\n",
    "            \"GAT\",\n",
    "            \"GIN\",\n",
    "            \"SchNet (3D)\",\n",
    "            \"DimeNet++\"\n",
    "        ],\n",
    "        \"Software Tools\": [\n",
    "            \"PyTorch Geometric\",\n",
    "            \"DGL\",\n",
    "            \"RDKit\",\n",
    "            \"DeepChem\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Add resource nodes\n",
    "    for category, res_list in resources.items():\n",
    "        cat_idx = categories.index(category)\n",
    "        cat_angle = 2 * np.pi * cat_idx / len(categories)\n",
    "        cat_x = 3 * np.cos(cat_angle)\n",
    "        cat_y = 3 * np.sin(cat_angle)\n",
    "\n",
    "        for i, resource in enumerate(res_list):\n",
    "            # Calculate position in a small arc around the category\n",
    "            angle_offset = (i - (len(res_list) - 1) / 2) * 0.6\n",
    "            dist = 2.5\n",
    "            x = cat_x + dist * np.cos(cat_angle + angle_offset)\n",
    "            y = cat_y + dist * np.sin(cat_angle + angle_offset)\n",
    "            G.add_node(resource, pos=(x, y))\n",
    "            G.add_edge(category, resource)\n",
    "\n",
    "    # Get positions\n",
    "    pos = nx.get_node_attributes(G, 'pos')\n",
    "\n",
    "    # Draw the graph\n",
    "    nx.draw_networkx_nodes(G, pos,\n",
    "                         nodelist=[\"GCNs in Chemistry\"],\n",
    "                         node_size=5000,\n",
    "                         node_color='lightblue',\n",
    "                         alpha=0.8)\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos,\n",
    "                         nodelist=categories,\n",
    "                         node_size=3000,\n",
    "                         node_color='lightgreen',\n",
    "                         alpha=0.8)\n",
    "\n",
    "    # Create a flattened list of all resources\n",
    "    all_resources = [res for res_list in resources.values() for res in res_list]\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos,\n",
    "                         nodelist=all_resources,\n",
    "                         node_size=2000,\n",
    "                         node_color='lightsalmon',\n",
    "                         alpha=0.8)\n",
    "\n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(G, pos, width=1.5, alpha=0.5)\n",
    "\n",
    "    # Draw labels\n",
    "    nx.draw_networkx_labels(G, pos,\n",
    "                          font_size=10,\n",
    "                          font_family='sans-serif')\n",
    "\n",
    "    plt.title(\"Resources for Graph Convolutional Networks in Chemistry\", fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show resources\n",
    "create_resources_visualization()\n",
    "\n",
    "print(\"This concludes our tutorial on Graph Convolutional Networks for molecular property prediction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcKT1HBHHrK8"
   },
   "source": [
    "This completes our comprehensive tutorial on Graph Convolutional Networks for chemists and pharmacists, with special focus on pooling methods, network depth, and skip connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jN-KedJfHsEN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMLPvzdV347gwnYC9k6PGSf",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
